{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04e93e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports loaded successfully\n",
      "Models will be saved to: D:\\docs\\MADS\\696-Milestone 2\\unsupervised\n",
      "  pca_optimal: D:\\docs\\MADS\\696-Milestone 2\\unsupervised\\pca_optimal_20251012T183501Z.joblib\n",
      "  kmeans_best: D:\\docs\\MADS\\696-Milestone 2\\unsupervised\\kmeans_best_20251012T181155Z.joblib\n",
      "  gmm_best: D:\\docs\\MADS\\696-Milestone 2\\unsupervised\\gmm_best_20251012T181155Z.joblib\n",
      "\n",
      "Auto-load status:\n",
      "  pca_optimal: âœ“ pca_optimal_20251012T183501Z.joblib\n",
      "  kmeans_best: âœ“ kmeans_best_20251012T181155Z.joblib\n",
      "  gmm_best: âœ“ gmm_best_20251012T181155Z.joblib\n",
      "\n",
      "FORCE_RETRAIN = False\n",
      "Database connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_11680\\1517958113.py:170: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden data loaded into DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23038 entries, 0 to 23037\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   school_name             23038 non-null  object \n",
      " 1   school_type             23038 non-null  object \n",
      " 2   teachers_fte            22550 non-null  float64\n",
      " 3   enrollment              22863 non-null  float64\n",
      " 4   grade_eight_enrollment  21613 non-null  float64\n",
      " 5   math_counts             22507 non-null  float64\n",
      " 6   math_high_pct           22507 non-null  float64\n",
      " 7   read_counts             22386 non-null  float64\n",
      " 8   read_high_pct           22386 non-null  float64\n",
      " 9   pct_hhi_150k_200k       23038 non-null  float64\n",
      " 10  pct_hhi_220k_plus       23038 non-null  float64\n",
      " 11  avg_natwalkind          23038 non-null  float64\n",
      " 12  total_10_14             23038 non-null  int64  \n",
      " 13  pct_10_14               23038 non-null  int64  \n",
      " 14  pct_female_10_14        22937 non-null  float64\n",
      " 15  total_pop               23038 non-null  int64  \n",
      " 16  schools_in_zip          23038 non-null  int64  \n",
      " 17  dup_rank                23038 non-null  int64  \n",
      "dtypes: float64(11), int64(5), object(2)\n",
      "memory usage: 3.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "## Environment\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Core Scientific Stack\n",
    "import numpy as np\n",
    "\n",
    "## Machine Learning / Preprocessing (scikit-learn)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "## Optimization & Persistence\n",
    "import optuna\n",
    "import joblib\n",
    "import contextlib\n",
    "import sys\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "\n",
    "print(\"All imports loaded successfully\")\n",
    "\n",
    "# Configuration flags\n",
    "FORCE_RETRAIN = False  # Set to False to load pretrained models when available\n",
    "# Connect to database and load data\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Setup model directory (handle notebook environment where __file__ is undefined)\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback: assume notebook is inside src; go up one directory if so\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'supervised.ipynb').exists() or (cwd / 'unsupervised.ipynb').exists():\n",
    "        PROJECT_ROOT = cwd\n",
    "    else:\n",
    "        for parent in cwd.parents:\n",
    "            if (parent / 'requirements.txt').exists() or (parent / 'README.md').exists():\n",
    "                PROJECT_ROOT = parent / 'src'\n",
    "                break\n",
    "        else:\n",
    "            PROJECT_ROOT = cwd  # final fallback\n",
    "\n",
    "MODEL_DIR = (PROJECT_ROOT / '..' / 'unsupervised').resolve()\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODEL_DIR}\")\n",
    "\n",
    "def save_model(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"Utility to persist models and optional metadata alongside them.\n",
    "    Saves model as joblib plus a companion JSON with metadata/hyperparams.\"\"\"\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    base_name = f\"{name}_{timestamp}\"\n",
    "    model_path = MODEL_DIR / f\"{base_name}.joblib\"\n",
    "    meta_path = MODEL_DIR / f\"{base_name}.json\"\n",
    "    joblib.dump(model, model_path)\n",
    "    meta = {'model_name': name, 'saved_utc': timestamp}\n",
    "    if extra:\n",
    "        meta.update(extra)\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Saved model -> {model_path.name}; metadata -> {meta_path.name}\")\n",
    "\n",
    "\n",
    "def load_pretrained_model(name: str):\n",
    "    \"\"\"Load a pretrained model if it exists and FORCE_RETRAIN is False.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, metadata_dict) if found, (None, None) if not found or FORCE_RETRAIN=True\n",
    "    \"\"\"\n",
    "    if FORCE_RETRAIN:\n",
    "        print(f\"FORCE_RETRAIN=True, skipping pretrained {name} model loading\")\n",
    "        return None, None\n",
    "    \n",
    "    # Find the most recent model file for this name\n",
    "    model_files = list(MODEL_DIR.glob(f\"{name}_*.joblib\"))\n",
    "    if not model_files:\n",
    "        print(f\"No pretrained {name} model found\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get the most recent model (by timestamp in filename)\n",
    "    model_files.sort(key=lambda x: x.stem.split('_')[-1], reverse=True)\n",
    "    latest_model_path = model_files[0]\n",
    "    meta_path = latest_model_path.with_suffix('.json')\n",
    "    \n",
    "    if not meta_path.exists():\n",
    "        print(f\"Metadata file missing for {name} model, skipping load\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        model = joblib.load(latest_model_path)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"Loaded pretrained {name} model from {latest_model_path.name}\")\n",
    "        return model, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pretrained {name} model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Initialize model variables (only set to None if not already defined)\n",
    "if 'pca_optimal' not in globals():\n",
    "    pca_optimal = None\n",
    "if 'kmeans_best' not in globals():\n",
    "    kmeans_best = None\n",
    "if 'gmm_best' not in globals():\n",
    "    gmm_best = None\n",
    "\n",
    "# Model loading patterns\n",
    "MODEL_GLOB_PATTERNS = {\n",
    "    'pca_optimal': 'pca_optimal_*.joblib',\n",
    "    'kmeans_best': 'kmeans_best_*.joblib',\n",
    "    'gmm_best': 'gmm_best_*.joblib'\n",
    "}\n",
    "\n",
    "# Attempt to load pretrained models\n",
    "loaded_flags = {}\n",
    "for var, pattern in MODEL_GLOB_PATTERNS.items():\n",
    "    if globals().get(var) is not None:\n",
    "        loaded_flags[var] = 'pre-existing'\n",
    "        continue\n",
    "    matches = list(MODEL_DIR.glob(pattern))\n",
    "    if not matches:\n",
    "        loaded_flags[var] = 'not found'\n",
    "        continue\n",
    "    # Sort by modification time to get the truly latest file\n",
    "    latest = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "    try:\n",
    "        globals()[var] = joblib.load(latest)\n",
    "        meta_file = latest.with_suffix('.json')\n",
    "        if meta_file.exists():\n",
    "            with open(meta_file) as f:\n",
    "                globals()[f\"{var}_meta\"] = json.load(f)\n",
    "        # Show the actual file that was loaded\n",
    "        loaded_flags[var] = f\"âœ“ {latest.name}\"\n",
    "        print(f\"  {var}: {latest}\")\n",
    "    except Exception as e:\n",
    "        loaded_flags[var] = f\"failed: {e}\"\n",
    "        globals()[var] = None\n",
    "\n",
    "print(\"\\nAuto-load status:\")\n",
    "for k, v in loaded_flags.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\nFORCE_RETRAIN = {FORCE_RETRAIN}\")\n",
    "\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": os.getenv(\"LOCAL_HOST\"),\n",
    "    \"user\": os.getenv(\"LOCAL_USER\"),\n",
    "    \"password\": os.getenv(\"LOCAL_PW\"),\n",
    "    \"port\": os.getenv(\"LOCAL_PORT\"),\n",
    "    \"dbname\": os.getenv(\"LOCAL_DB\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    print(\"Database connection successful\")\n",
    "    sql_query = \"SELECT * FROM dev.base_data;\"\n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    conn.close()\n",
    "    print(\"Golden data loaded into DataFrame:\")\n",
    "    print(df.info())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf8e01",
   "metadata": {},
   "source": [
    "## 1. Data Overview & Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b6a229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (23038, 18)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "school_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "school_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "teachers_fte",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "enrollment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "grade_eight_enrollment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "math_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "math_high_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "read_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "read_high_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_hhi_150k_200k",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_hhi_220k_plus",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_natwalkind",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_10_14",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pct_10_14",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pct_female_10_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_pop",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "schools_in_zip",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dup_rank",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "bd4a555d-10a4-4aee-af3b-6918115644b2",
       "rows": [
        [
         "0",
         "1 LT Charles W. Whitcomb School",
         "1",
         "93.0",
         "1077.0",
         "370.0",
         "71.0",
         "9.0",
         "71.0",
         "5.0",
         "4.57",
         "5.03",
         "12.317520901322792",
         "2081",
         "0",
         "0.0",
         "41505",
         "2",
         "1"
        ],
        [
         "1",
         "100 Academy of Engineering and Technology MS",
         "1",
         null,
         "147.0",
         "47.0",
         "5.0",
         "49.0",
         "7.0",
         "49.0",
         "2.01",
         "0.75",
         "12.120378210516511",
         "3841",
         "0",
         "0.0",
         "47881",
         "3",
         "1"
        ],
        [
         "2",
         "1R ELEMENTARY",
         "1",
         "12.0",
         "191.0",
         "25.0",
         "9.0",
         "79.0",
         "9.0",
         "59.0",
         "2.33",
         "1.34",
         "8.28723404261702",
         "2008",
         "0",
         "0.0",
         "25966",
         "4",
         "1"
        ],
        [
         "3",
         "21st Century Charter Sch of Gary",
         "1",
         "96.0",
         "1329.0",
         "102.0",
         "98.0",
         "5.0",
         "96.0",
         "49.0",
         "0.54",
         "0.15",
         "8.16719576731746",
         "528",
         "0",
         "0.0",
         "6105",
         "1",
         "1"
        ],
        [
         "4",
         "21st Century Cyber CS",
         "1",
         "72.0",
         "1536.0",
         "202.0",
         "95.0",
         "49.0",
         "96.0",
         "69.0",
         "5.35",
         "8.93",
         "8.938297872387235",
         "2838",
         "0",
         "0.0",
         "50510",
         "4",
         "1"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_name</th>\n",
       "      <th>school_type</th>\n",
       "      <th>teachers_fte</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>grade_eight_enrollment</th>\n",
       "      <th>math_counts</th>\n",
       "      <th>math_high_pct</th>\n",
       "      <th>read_counts</th>\n",
       "      <th>read_high_pct</th>\n",
       "      <th>pct_hhi_150k_200k</th>\n",
       "      <th>pct_hhi_220k_plus</th>\n",
       "      <th>avg_natwalkind</th>\n",
       "      <th>total_10_14</th>\n",
       "      <th>pct_10_14</th>\n",
       "      <th>pct_female_10_14</th>\n",
       "      <th>total_pop</th>\n",
       "      <th>schools_in_zip</th>\n",
       "      <th>dup_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 LT Charles W. Whitcomb School</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.57</td>\n",
       "      <td>5.03</td>\n",
       "      <td>12.317521</td>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41505</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 Academy of Engineering and Technology MS</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12.120378</td>\n",
       "      <td>3841</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47881</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1R ELEMENTARY</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8.287234</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25966</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21st Century Charter Sch of Gary</td>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>8.167196</td>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6105</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21st Century Cyber CS</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>8.93</td>\n",
       "      <td>8.938298</td>\n",
       "      <td>2838</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50510</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    school_name school_type  teachers_fte  \\\n",
       "0               1 LT Charles W. Whitcomb School           1          93.0   \n",
       "1  100 Academy of Engineering and Technology MS           1           NaN   \n",
       "2                                 1R ELEMENTARY           1          12.0   \n",
       "3              21st Century Charter Sch of Gary           1          96.0   \n",
       "4                         21st Century Cyber CS           1          72.0   \n",
       "\n",
       "   enrollment  grade_eight_enrollment  math_counts  math_high_pct  \\\n",
       "0      1077.0                   370.0         71.0            9.0   \n",
       "1       147.0                    47.0          5.0           49.0   \n",
       "2       191.0                    25.0          9.0           79.0   \n",
       "3      1329.0                   102.0         98.0            5.0   \n",
       "4      1536.0                   202.0         95.0           49.0   \n",
       "\n",
       "   read_counts  read_high_pct  pct_hhi_150k_200k  pct_hhi_220k_plus  \\\n",
       "0         71.0            5.0               4.57               5.03   \n",
       "1          7.0           49.0               2.01               0.75   \n",
       "2          9.0           59.0               2.33               1.34   \n",
       "3         96.0           49.0               0.54               0.15   \n",
       "4         96.0           69.0               5.35               8.93   \n",
       "\n",
       "   avg_natwalkind  total_10_14  pct_10_14  pct_female_10_14  total_pop  \\\n",
       "0       12.317521         2081          0               0.0      41505   \n",
       "1       12.120378         3841          0               0.0      47881   \n",
       "2        8.287234         2008          0               0.0      25966   \n",
       "3        8.167196          528          0               0.0       6105   \n",
       "4        8.938298         2838          0               0.0      50510   \n",
       "\n",
       "   schools_in_zip  dup_rank  \n",
       "0               2         1  \n",
       "1               3         1  \n",
       "2               4         1  \n",
       "3               1         1  \n",
       "4               4         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric subset shape: (23038, 16)\n",
      "Missing value percentage (top 15):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e72d5d2e-63a0-4cfd-99f3-dc3b22805d04",
       "rows": [
        [
         "grade_eight_enrollment",
         "0.061854327632606995"
        ],
        [
         "read_counts",
         "0.028301067801024395"
        ],
        [
         "read_high_pct",
         "0.028301067801024395"
        ],
        [
         "math_counts",
         "0.023048875770466187"
        ],
        [
         "math_high_pct",
         "0.023048875770466187"
        ],
        [
         "teachers_fte",
         "0.021182394305061202"
        ],
        [
         "enrollment",
         "0.00759614549874121"
        ],
        [
         "pct_female_10_14",
         "0.004384061116416356"
        ],
        [
         "pct_hhi_150k_200k",
         "0.0"
        ],
        [
         "pct_hhi_220k_plus",
         "0.0"
        ],
        [
         "avg_natwalkind",
         "0.0"
        ],
        [
         "total_10_14",
         "0.0"
        ],
        [
         "pct_10_14",
         "0.0"
        ],
        [
         "total_pop",
         "0.0"
        ],
        [
         "schools_in_zip",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 15
       }
      },
      "text/plain": [
       "grade_eight_enrollment    0.061854\n",
       "read_counts               0.028301\n",
       "read_high_pct             0.028301\n",
       "math_counts               0.023049\n",
       "math_high_pct             0.023049\n",
       "teachers_fte              0.021182\n",
       "enrollment                0.007596\n",
       "pct_female_10_14          0.004384\n",
       "pct_hhi_150k_200k         0.000000\n",
       "pct_hhi_220k_plus         0.000000\n",
       "avg_natwalkind            0.000000\n",
       "total_10_14               0.000000\n",
       "pct_10_14                 0.000000\n",
       "total_pop                 0.000000\n",
       "schools_in_zip            0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic shape & preview\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "display(df.head())\n",
    "\n",
    "df_numeric = df.select_dtypes(include=['int64','float64']).copy()\n",
    "print(f\"Numeric subset shape: {df_numeric.shape}\")\n",
    "missing_pct = df_numeric.isna().mean().sort_values(ascending=False)\n",
    "print(\"Missing value percentage (top 15):\")\n",
    "display(missing_pct.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf8ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix ready. Shape: (23038, 16)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values: simple strategy (median). Could be enhanced later.\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "scaled_features = imputer.fit_transform(df_numeric)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(scaled_features)\n",
    "print(f\"Feature matrix ready. Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fb2fd",
   "metadata": {},
   "source": [
    "## 2. Optimization Helpers (Optuna)\n",
    "We define metric computation and a utility to optionally apply PCA inside each trial to reduce dimensionality (tuned as a hyperparameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a508a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions and metrics ready.\n"
     ]
    }
   ],
   "source": [
    "# Ensure optuna is available (if running in an environment where pip install is allowed)\n",
    "\n",
    "\n",
    "# Cache original data for reuse\n",
    "X_full = X  # already scaled\n",
    "\n",
    "\n",
    "def prepare_features(trial, X_input):\n",
    "    \"\"\"Optionally apply PCA controlled by trial hyperparameters.\"\"\"\n",
    "    use_pca = trial.suggest_categorical('use_pca', [True, False])\n",
    "    if use_pca:\n",
    "        # limit components between 2 and min(50, n_features)\n",
    "        max_comp = min(50, X_input.shape[1])\n",
    "        n_components = trial.suggest_int('pca_components', 2, max_comp)\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_red = pca.fit_transform(X_input)\n",
    "        return X_red, pca\n",
    "    return X_input, None\n",
    "\n",
    "\n",
    "def compute_cluster_metrics(X_data, labels):\n",
    "    # Guard for metrics requiring >1 cluster and fewer than n_samples clusters\n",
    "    unique_labels = set(labels)\n",
    "    if len(unique_labels) <= 1 or len(unique_labels) >= len(labels):\n",
    "        return {\n",
    "            'silhouette': float('nan'),\n",
    "            'calinski_harabasz': float('nan'),\n",
    "            'davies_bouldin': float('nan')\n",
    "        }\n",
    "    return {\n",
    "        'silhouette': silhouette_score(X_data, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_data, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(X_data, labels)\n",
    "    }\n",
    "\n",
    "\n",
    "def objective_wrapper(build_model_fn):\n",
    "    def objective(trial):\n",
    "        X_trial, pca_obj = prepare_features(trial, X_full)\n",
    "        model = build_model_fn(trial)\n",
    "        labels = model.fit_predict(X_trial)\n",
    "        metrics = compute_cluster_metrics(X_trial, labels)\n",
    "        # We'll optimize on silhouette (maximize)\n",
    "        trial.set_user_attr('metrics', metrics)\n",
    "        if pca_obj is not None:\n",
    "            trial.set_user_attr('pca_components_actual', getattr(pca_obj, 'n_components_', None))\n",
    "        return metrics['silhouette']\n",
    "    return objective\n",
    "\n",
    "print(\"Helper functions and metrics ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babe8f7",
   "metadata": {},
   "source": [
    "## 3. KMeans Optimization\n",
    "We search hyperparameters: n_clusters, init method, algorithm, optional PCA usage & components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edbd27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna logging suppression active: True\n"
     ]
    }
   ],
   "source": [
    "SUPPRESS_TRIAL_OUTPUT = True  # toggle this to see full trial logs\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def silent_stdout(enabled=True):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    new_target = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    try:\n",
    "        sys.stdout = new_target\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "# Reduce Optuna logging level (shows only WARNING+)\n",
    "if SUPPRESS_TRIAL_OUTPUT:\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "else:\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "print(\"Optuna logging suppression active:\" , SUPPRESS_TRIAL_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5532530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KMeans] Running optimization (silence trials=True) ...\n"
     ]
    }
   ],
   "source": [
    "# --- KMeans Hyperparameter Optimization (enhanced) ---\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "N_KMEANS_TRIALS = 40  # adjust if you want a faster/slower search\n",
    "REUSE_EXISTING_KMEANS_STUDY = False  # set True to skip re-optimizing if study_kmeans already present\n",
    "KMEANS_STUDY_NAME = 'kmeans_clustering'\n",
    "\n",
    "if REUSE_EXISTING_KMEANS_STUDY and 'study_kmeans' in globals():\n",
    "    print('[KMeans] Reusing existing Optuna study; skipping optimization.')\n",
    "else:\n",
    "    def build_kmeans(trial):\n",
    "        n_clusters = trial.suggest_int('kmeans_n_clusters', 2, 15)\n",
    "        init = trial.suggest_categorical('kmeans_init', ['k-means++', 'random'])\n",
    "        algorithm = trial.suggest_categorical('kmeans_algorithm', ['lloyd', 'elkan'])\n",
    "        # Tune n_init (sklearn >=1.4 supports int or 'auto')\n",
    "        n_init = trial.suggest_categorical('kmeans_n_init', [10, 20, 30, 'auto'])\n",
    "        return KMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            init=init,\n",
    "            algorithm=algorithm,\n",
    "            n_init=n_init,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    print(f\"[KMeans] Running optimization (silence trials={SUPPRESS_TRIAL_OUTPUT}) ...\")\n",
    "    with silent_stdout(SUPPRESS_TRIAL_OUTPUT):\n",
    "        study_kmeans = optuna.create_study(direction='maximize', study_name=KMEANS_STUDY_NAME)\n",
    "\n",
    "        def objective_wrapper_with_runtime(build_model_fn):\n",
    "            base_objective = objective_wrapper(build_model_fn)\n",
    "            def _inner(trial):\n",
    "                start = time.time()\n",
    "                val = base_objective(trial)\n",
    "                trial.set_user_attr('runtime_sec', time.time() - start)\n",
    "                return val\n",
    "            return _inner\n",
    "\n",
    "        study_kmeans.optimize(\n",
    "            objective_wrapper_with_runtime(build_kmeans),\n",
    "            n_trials=N_KMEANS_TRIALS,\n",
    "            show_progress_bar=not SUPPRESS_TRIAL_OUTPUT\n",
    "        )\n",
    "\n",
    "best_k_params = study_kmeans.best_trial.params\n",
    "best_k_metrics = study_kmeans.best_trial.user_attrs.get('metrics', {})\n",
    "print(\"[KMeans] Best Params:\")\n",
    "print(best_k_params)\n",
    "print(\"[KMeans] Best Metrics:\")\n",
    "print(best_k_metrics)\n",
    "print(f\"[KMeans] Best silhouette: {study_kmeans.best_value:.4f}\")\n",
    "\n",
    "# Build results DataFrame (include inertia if available)\n",
    "kmeans_results = []\n",
    "for t in study_kmeans.trials:\n",
    "    row = {**t.params}\n",
    "    metrics = t.user_attrs.get('metrics', {})\n",
    "    row.update(metrics)\n",
    "    row['runtime_sec'] = t.user_attrs.get('runtime_sec')\n",
    "    # inertia: recompute quickly if silhouette is valid and clusters >1\n",
    "    try:\n",
    "        if not np.isnan(metrics.get('silhouette', np.nan)) and 'kmeans_n_clusters' in t.params:\n",
    "            # Refit minimal model (no PCA) ONLY for inertia if clusters moderate\n",
    "            km_tmp = KMeans(\n",
    "                n_clusters=t.params['kmeans_n_clusters'],\n",
    "                init=t.params['kmeans_init'],\n",
    "                algorithm=t.params['kmeans_algorithm'],\n",
    "                n_init=t.params.get('kmeans_n_init','auto'),\n",
    "                random_state=42\n",
    "            ).fit(X_full)\n",
    "            row['inertia'] = km_tmp.inertia_\n",
    "        else:\n",
    "            row['inertia'] = np.nan\n",
    "    except Exception:\n",
    "        row['inertia'] = np.nan\n",
    "    kmeans_results.append(row)\n",
    "\n",
    "kmeans_results_df = pd.DataFrame(kmeans_results)\n",
    "if not kmeans_results_df.empty:\n",
    "    # Rank by silhouette then inertia (lower inertia better)\n",
    "    kmeans_results_df['inertia_rank'] = kmeans_results_df['inertia'].rank(method='min')\n",
    "    display(kmeans_results_df.sort_values(['silhouette','inertia'], ascending=[False, True]).head(10))\n",
    "    print('[KMeans] Summary:')\n",
    "    print(kmeans_results_df[['silhouette','inertia','runtime_sec']].describe().round(3))\n",
    "else:\n",
    "    print('[KMeans] No trials recorded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58316419",
   "metadata": {},
   "source": [
    "## 4. Gaussian Mixture (GMM) Optimization\n",
    "We tune: n_components, covariance_type, reg_covar, and optional PCA usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gaussian Mixture (GMM) Hyperparameter Optimization (enhanced) ---\n",
    "\n",
    "N_GMM_TRIALS = 40  # adjust if you want a faster/slower search\n",
    "REUSE_EXISTING_GMM_STUDY = False  # set True to skip re-optimizing if study_gmm already present\n",
    "GMM_STUDY_NAME = 'gmm_clustering'\n",
    "\n",
    "if REUSE_EXISTING_GMM_STUDY and 'study_gmm' in globals():\n",
    "    print('[GMM] Reusing existing Optuna study; skipping optimization.')\n",
    "else:\n",
    "    def build_gmm(trial):\n",
    "        n_components = trial.suggest_int('gmm_n_components', 2, 15)\n",
    "        covariance_type = trial.suggest_categorical('gmm_covariance_type', ['full', 'tied', 'diag', 'spherical'])\n",
    "        reg_covar = trial.suggest_float('gmm_reg_covar', 1e-6, 1e-2, log=True)\n",
    "        return GaussianMixture(\n",
    "            n_components=n_components,\n",
    "            covariance_type=covariance_type,\n",
    "            reg_covar=reg_covar,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    print(f\"[GMM] Running optimization (silence trials={SUPPRESS_TRIAL_OUTPUT}) ...\")\n",
    "    with silent_stdout(SUPPRESS_TRIAL_OUTPUT):\n",
    "        study_gmm = optuna.create_study(direction='maximize', study_name=GMM_STUDY_NAME)\n",
    "        study_gmm.optimize(\n",
    "            objective_wrapper_with_runtime(build_gmm),\n",
    "            n_trials=N_GMM_TRIALS,\n",
    "            show_progress_bar=not SUPPRESS_TRIAL_OUTPUT\n",
    "        )\n",
    "\n",
    "best_g_params = study_gmm.best_trial.params\n",
    "best_g_metrics = study_gmm.best_trial.user_attrs.get('metrics', {})\n",
    "print(\"[GMM] Best Params:\")\n",
    "print(best_g_params)\n",
    "print(\"[GMM] Best Metrics:\")\n",
    "print(best_g_metrics)\n",
    "print(f\"[GMM] Best silhouette: {study_gmm.best_value:.4f}\")\n",
    "\n",
    "# Build results DataFrame (include runtime)\n",
    "gmm_results = []\n",
    "for t in study_gmm.trials:\n",
    "    row = {**t.params}\n",
    "    metrics = t.user_attrs.get('metrics', {})\n",
    "    row.update(metrics)\n",
    "    row['runtime_sec'] = t.user_attrs.get('runtime_sec')\n",
    "    gmm_results.append(row)\n",
    "\n",
    "gmm_results_df = pd.DataFrame(gmm_results)\n",
    "if not gmm_results_df.empty:\n",
    "    display(gmm_results_df.sort_values('silhouette', ascending=False).head(10))\n",
    "    print('[GMM] Summary:')\n",
    "    print(gmm_results_df[['silhouette','calinski_harabasz','davies_bouldin','runtime_sec']].describe().round(3))\n",
    "else:\n",
    "    print('[GMM] No trials recorded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a0a76",
   "metadata": {},
   "source": [
    "## 4.5. PCA - Dimensionality Reduction Model\n",
    "**Objective:** Reduce feature space while retaining maximum variance for efficient data representation.\n",
    "\n",
    "**Evaluation Metrics (per framework):**\n",
    "- **Reconstruction error** (MSE between original and reconstructed data)\n",
    "- **Variance retained** (cumulative explained variance ratio)\n",
    "- **Visualization quality** (2D/3D projections interpretability)\n",
    "- **Downstream performance** (clustering quality on reduced features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA Dimensionality Reduction: Analysis ---\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"PCA DIMENSIONALITY REDUCTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Fit PCA with all components to analyze variance distribution\n",
    "print(\"\\n[PCA] Step 1: Analyzing variance across all components...\")\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_full)\n",
    "\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Key thresholds\n",
    "n_80 = np.argmax(cumsum_variance >= 0.80) + 1\n",
    "n_90 = np.argmax(cumsum_variance >= 0.90) + 1\n",
    "n_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "n_99 = np.argmax(cumsum_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nComponents needed for variance thresholds:\")\n",
    "print(f\"   80% variance: {n_80} components\")\n",
    "print(f\"   90% variance: {n_90} components\")\n",
    "print(f\"   95% variance: {n_95} components\")\n",
    "print(f\"   99% variance: {n_99} components\")\n",
    "print(f\"   Total available: {X_full.shape[1]} features\")\n",
    "\n",
    "# 2. Elbow method - evaluate reconstruction error vs n_components\n",
    "print(\"\\n[PCA] Step 2: Computing reconstruction errors for elbow analysis...\")\n",
    "n_components_range = list(range(2, min(51, X_full.shape[1]), 2))\n",
    "reconstruction_errors = []\n",
    "variance_retained_list = []\n",
    "\n",
    "for n in n_components_range:\n",
    "    pca_temp = PCA(n_components=n, random_state=42)\n",
    "    X_transformed = pca_temp.fit_transform(X_full)\n",
    "    X_reconstructed = pca_temp.inverse_transform(X_transformed)\n",
    "    mse = mean_squared_error(X_full, X_reconstructed)\n",
    "    reconstruction_errors.append(mse)\n",
    "    variance_retained_list.append(pca_temp.explained_variance_ratio_.sum())\n",
    "    \n",
    "    if n % 10 == 0:\n",
    "        print(f\"   Evaluated {n} components: MSE={mse:.6f}, Variance={pca_temp.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Find elbow using second derivative\n",
    "reconstruction_errors_arr = np.array(reconstruction_errors)\n",
    "if len(reconstruction_errors_arr) > 2:\n",
    "    second_derivative = np.diff(reconstruction_errors_arr, n=2)\n",
    "    elbow_idx = np.argmax(second_derivative > 0) + 2\n",
    "    elbow_n = n_components_range[min(elbow_idx, len(n_components_range)-1)]\n",
    "else:\n",
    "    elbow_n = n_95\n",
    "\n",
    "print(f\"\\n[PCA] Elbow method suggests: {elbow_n} components\")\n",
    "print(f\"[PCA] 95% variance threshold: {n_95} components\")\n",
    "\n",
    "# Choose final n_components (balance between compression and information retention)\n",
    "final_n_components = n_95\n",
    "print(f\"\\n[PCA] Selected: {final_n_components} components (95% variance threshold)\")\n",
    "\n",
    "# 3. Fit optimal PCA model\n",
    "print(f\"\\n[PCA] Step 3: Fitting final PCA model with {final_n_components} components...\")\n",
    "pca_optimal = PCA(n_components=final_n_components, random_state=42)\n",
    "X_reduced = pca_optimal.fit_transform(X_full)\n",
    "X_reconstructed_final = pca_optimal.inverse_transform(X_reduced)\n",
    "\n",
    "# 4. Calculate comprehensive metrics\n",
    "print(\"\\n[PCA] Step 4: Computing performance metrics...\")\n",
    "\n",
    "# Reconstruction error\n",
    "final_reconstruction_error = mean_squared_error(X_full, X_reconstructed_final)\n",
    "final_rmse = np.sqrt(final_reconstruction_error)\n",
    "\n",
    "# Variance metrics\n",
    "final_variance_retained = pca_optimal.explained_variance_ratio_.sum()\n",
    "final_variance_lost = 1 - final_variance_retained\n",
    "\n",
    "# Dimensionality metrics\n",
    "dimensionality_reduction_pct = (1 - final_n_components / X_full.shape[1]) * 100\n",
    "compression_ratio = X_full.shape[1] / final_n_components\n",
    "\n",
    "# Per-feature reconstruction error\n",
    "feature_reconstruction_errors = np.mean((X_full - X_reconstructed_final) ** 2, axis=0)\n",
    "\n",
    "pca_metrics = {\n",
    "    'n_components': int(final_n_components),\n",
    "    'original_dimensions': int(X_full.shape[1]),\n",
    "    'reduced_dimensions': int(final_n_components),\n",
    "    'dimensionality_reduction_pct': float(dimensionality_reduction_pct),\n",
    "    'reconstruction_error_mse': float(final_reconstruction_error),\n",
    "    'reconstruction_error_rmse': float(final_rmse),\n",
    "    'variance_retained': float(final_variance_retained),\n",
    "    'variance_lost': float(final_variance_lost),\n",
    "    'compression_ratio': float(compression_ratio),\n",
    "    'explained_variance_per_component': pca_optimal.explained_variance_ratio_.tolist(),\n",
    "    'cumulative_variance': np.cumsum(pca_optimal.explained_variance_ratio_).tolist(),\n",
    "    'singular_values': pca_optimal.singular_values_.tolist(),\n",
    "    'mean_feature_reconstruction_error': float(np.mean(feature_reconstruction_errors)),\n",
    "    'max_feature_reconstruction_error': float(np.max(feature_reconstruction_errors))\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" \" * 25 + \"PCA PERFORMANCE METRICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š DIMENSIONALITY REDUCTION:\")\n",
    "print(f\"   Original dimensions:     {pca_metrics['original_dimensions']}\")\n",
    "print(f\"   Reduced dimensions:      {pca_metrics['reduced_dimensions']}\")\n",
    "print(f\"   Reduction percentage:    {pca_metrics['dimensionality_reduction_pct']:.1f}%\")\n",
    "print(f\"   Compression ratio:       {pca_metrics['compression_ratio']:.2f}x\")\n",
    "\n",
    "print(f\"\\nðŸ“Š RECONSTRUCTION QUALITY:\")\n",
    "print(f\"   Mean Squared Error:      {pca_metrics['reconstruction_error_mse']:.6f}\")\n",
    "print(f\"   Root Mean Squared Error: {pca_metrics['reconstruction_error_rmse']:.6f}\")\n",
    "print(f\"   Mean per-feature error:  {pca_metrics['mean_feature_reconstruction_error']:.6f}\")\n",
    "print(f\"   Max per-feature error:   {pca_metrics['max_feature_reconstruction_error']:.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š VARIANCE RETENTION:\")\n",
    "print(f\"   Variance retained:       {pca_metrics['variance_retained']:.4f} ({pca_metrics['variance_retained']*100:.2f}%)\")\n",
    "print(f\"   Variance lost:           {pca_metrics['variance_lost']:.4f} ({pca_metrics['variance_lost']*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š TOP 10 PRINCIPAL COMPONENTS (Individual Variance):\")\n",
    "for i in range(min(10, final_n_components)):\n",
    "    var = pca_optimal.explained_variance_ratio_[i]\n",
    "    cumvar = np.sum(pca_optimal.explained_variance_ratio_[:i+1])\n",
    "    print(f\"   PC{i+1:2d}: {var:.4f} ({var*100:5.2f}%) | Cumulative: {cumvar:.4f} ({cumvar*100:5.2f}%)\")\n",
    "\n",
    "# 5. Component analysis - feature loadings\n",
    "print(f\"\\n[PCA] Step 5: Analyzing feature contributions to principal components...\")\n",
    "\n",
    "feature_names = df_numeric.columns.tolist()\n",
    "loadings = pca_optimal.components_  # Shape: (n_components, n_features)\n",
    "\n",
    "# Create component importance DataFrame\n",
    "component_importance = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(final_n_components)],\n",
    "    'Variance_Explained': pca_optimal.explained_variance_ratio_,\n",
    "    'Cumulative_Variance': np.cumsum(pca_optimal.explained_variance_ratio_),\n",
    "    'Singular_Value': pca_optimal.singular_values_\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸ“Š Component Summary Table:\")\n",
    "display(component_importance.head(20))\n",
    "\n",
    "# Top features per PC\n",
    "print(f\"\\nðŸ“Š TOP 5 CONTRIBUTING FEATURES PER PRINCIPAL COMPONENT:\")\n",
    "for pc_idx in range(min(5, final_n_components)):\n",
    "    pc_name = f'PC{pc_idx+1}'\n",
    "    abs_loadings = np.abs(loadings[pc_idx])\n",
    "    top_indices = np.argsort(abs_loadings)[-5:][::-1]\n",
    "    var_explained = pca_optimal.explained_variance_ratio_[pc_idx]\n",
    "    \n",
    "    print(f\"\\n   {pc_name} (Variance: {var_explained:.4f} = {var_explained*100:.2f}%):\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        loading = loadings[pc_idx][idx]\n",
    "        print(f\"      {rank}. {feature_names[idx]:<40s}: {loading:+.4f}\")\n",
    "\n",
    "# Find features with highest overall contribution (sum of absolute loadings)\n",
    "feature_importance_pca = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Total_Abs_Loading': np.sum(np.abs(loadings), axis=0),\n",
    "    'Max_Abs_Loading': np.max(np.abs(loadings), axis=0),\n",
    "    'Reconstruction_Error': feature_reconstruction_errors\n",
    "})\n",
    "feature_importance_pca = feature_importance_pca.sort_values('Total_Abs_Loading', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š TOP 10 MOST IMPORTANT FEATURES (Overall Contribution):\")\n",
    "display(feature_importance_pca.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA Visualizations ---\n",
    "\n",
    "# 6. Visualizations\n",
    "print(f\"\\n[PCA] Step 6: Generating visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# 6.1 Scree plot\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "n_plot = min(50, len(cumsum_variance))\n",
    "# Bars for individual variance\n",
    "ax1.bar(range(1, n_plot+1), pca_full.explained_variance_ratio_[:n_plot], \n",
    "        color='steelblue', alpha=0.7, edgecolor='black', label='Individual Variance')\n",
    "# Line for cumulative variance\n",
    "ax1.plot(range(1, n_plot+1), cumsum_variance[:n_plot], 'ro-', linewidth=2, markersize=4, label='Cumulative Variance')\n",
    "ax1.axhline(y=0.95, color='r', linestyle='--', label='95% threshold', linewidth=2)\n",
    "ax1.axhline(y=0.90, color='orange', linestyle='--', label='90% threshold', linewidth=1.5)\n",
    "ax1.axvline(x=final_n_components, color='g', linestyle='--', label=f'Selected: {final_n_components}', linewidth=2)\n",
    "ax1.set_xlabel('Number of Components', fontsize=11)\n",
    "ax1.set_ylabel('Variance Explained', fontsize=11)\n",
    "ax1.set_title('PCA Scree Plot', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 6.2 Individual variance per component\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "n_bars = min(20, final_n_components)\n",
    "bars = ax2.bar(range(1, n_bars+1), pca_optimal.explained_variance_ratio_[:n_bars], \n",
    "        color='steelblue', alpha=0.7, edgecolor='black', label='Individual Variance')\n",
    "ax2.set_xlabel('Principal Component', fontsize=11)\n",
    "ax2.set_ylabel('Individual Variance Explained', fontsize=11)\n",
    "ax2.set_title(f'Variance per Component (Top {n_bars})', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Secondary axis for cumulative variance\n",
    "ax2_twin = ax2.twinx()\n",
    "cum_var_top = np.cumsum(pca_optimal.explained_variance_ratio_[:n_bars])\n",
    "line, = ax2_twin.plot(range(1, n_bars+1), cum_var_top, 'ro-', linewidth=2, markersize=4, label='Cumulative Variance')\n",
    "ax2_twin.set_ylabel('Cumulative Variance Explained', fontsize=11, color='red')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='red')\n",
    "ax2_twin.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on the cumulative line\n",
    "for i, (x, y) in enumerate(zip(range(1, n_bars+1), cum_var_top)):\n",
    "    if i % 2 == 0 or i == n_bars - 1:  # Label every other point and the last one\n",
    "        ax2_twin.text(x, y + 0.02, f'{y:.2f}', ha='center', va='bottom', fontsize=8, color='red', fontweight='bold')\n",
    "\n",
    "# Combined legend\n",
    "lines = [bars, line]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax2.legend(lines, labels, fontsize=9, loc='upper left')\n",
    "\n",
    "# 6.3 Reconstruction error vs n_components\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(n_components_range, reconstruction_errors, 'ro-', linewidth=2, markersize=4)\n",
    "ax3.axvline(x=final_n_components, color='g', linestyle='--', \n",
    "           label=f'Selected: {final_n_components}', linewidth=2)\n",
    "ax3.set_xlabel('Number of Components', fontsize=11)\n",
    "ax3.set_ylabel('Reconstruction Error (MSE)', fontsize=11)\n",
    "ax3.set_title('Reconstruction Error vs Dimensionality', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 6.4 Variance retained vs n_components\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(n_components_range, variance_retained_list, 'go-', linewidth=2, markersize=4)\n",
    "ax4.axhline(y=0.95, color='r', linestyle='--', label='95% target', linewidth=2)\n",
    "ax4.axvline(x=final_n_components, color='b', linestyle='--', \n",
    "           label=f'Selected: {final_n_components}', linewidth=2)\n",
    "ax4.set_xlabel('Number of Components', fontsize=11)\n",
    "ax4.set_ylabel('Variance Retained', fontsize=11)\n",
    "ax4.set_title('Variance Retained vs Dimensionality', fontsize=13, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 6.5 Feature loading heatmap (top 5 PCs, top 20 features)\n",
    "ax5 = fig.add_subplot(gs[1, 1:])\n",
    "n_pcs_heatmap = min(5, final_n_components)\n",
    "top_features_idx = feature_importance_pca.head(20).index\n",
    "loadings_subset = loadings[:n_pcs_heatmap, top_features_idx]\n",
    "feature_names_subset = [feature_names[i] for i in top_features_idx]\n",
    "\n",
    "im = ax5.imshow(loadings_subset, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax5.set_yticks(range(n_pcs_heatmap))\n",
    "ax5.set_yticklabels([f'PC{i+1}' for i in range(n_pcs_heatmap)], fontsize=9)\n",
    "ax5.set_xticks(range(len(feature_names_subset)))\n",
    "ax5.set_xticklabels(feature_names_subset, rotation=45, ha='right', fontsize=8)\n",
    "ax5.set_title('Feature Loadings (Top 5 PCs, Top 20 Features)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax5, label='Loading Value')\n",
    "\n",
    "# 6.6 2D PCA visualization\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "sample_indices = np.random.choice(X_reduced.shape[0], size=min(5000, X_reduced.shape[0]), replace=False)\n",
    "ax6.scatter(X_reduced[sample_indices, 0], X_reduced[sample_indices, 1], \n",
    "           alpha=0.3, s=10, c='steelblue', edgecolors='none')\n",
    "ax6.set_xlabel(f'PC1 ({pca_optimal.explained_variance_ratio_[0]*100:.1f}%)', fontsize=11)\n",
    "ax6.set_ylabel(f'PC2 ({pca_optimal.explained_variance_ratio_[1]*100:.1f}%)', fontsize=11)\n",
    "ax6.set_title('2D PCA Projection (Sample)', fontsize=13, fontweight='bold')\n",
    "ax6.grid(alpha=0.3)\n",
    "\n",
    "# 6.7 3D PCA visualization\n",
    "if final_n_components >= 3:\n",
    "    ax7 = fig.add_subplot(gs[2, 1], projection='3d')\n",
    "    ax7.scatter(X_reduced[sample_indices, 0], X_reduced[sample_indices, 1], X_reduced[sample_indices, 2],\n",
    "               alpha=0.3, s=5, c='steelblue', edgecolors='none')\n",
    "    ax7.set_xlabel(f'PC1 ({pca_optimal.explained_variance_ratio_[0]*100:.1f}%)', fontsize=9)\n",
    "    ax7.set_ylabel(f'PC2 ({pca_optimal.explained_variance_ratio_[1]*100:.1f}%)', fontsize=9)\n",
    "    ax7.set_zlabel(f'PC3 ({pca_optimal.explained_variance_ratio_[2]*100:.1f}%)', fontsize=9)\n",
    "    ax7.set_title('3D PCA Projection (Sample)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# 6.8 Per-feature reconstruction error\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "sorted_errors = np.sort(feature_reconstruction_errors)[::-1]\n",
    "ax8.bar(range(len(sorted_errors[:30])), sorted_errors[:30], color='coral', alpha=0.7, edgecolor='black')\n",
    "ax8.set_xlabel('Feature Rank (by error)', fontsize=11)\n",
    "ax8.set_ylabel('Reconstruction Error', fontsize=11)\n",
    "ax8.set_title('Top 30 Features by Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig(MODEL_DIR / 'pca_comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… PCA visualization saved to: {MODEL_DIR / 'pca_comprehensive_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA Model Saving ---\n",
    "\n",
    "# 7. Save PCA model and metadata\n",
    "print(f\"\\n[PCA] Step 7: Saving PCA model and metadata...\")\n",
    "\n",
    "pca_extra = {\n",
    "    'metrics': pca_metrics,\n",
    "    'optimization': {\n",
    "        'method': 'variance_threshold',\n",
    "        'target_variance': 0.95,\n",
    "        'elbow_method_suggestion': int(elbow_n),\n",
    "        'final_choice': int(final_n_components),\n",
    "        'thresholds': {\n",
    "            '80pct': int(n_80),\n",
    "            '90pct': int(n_90),\n",
    "            '95pct': int(n_95),\n",
    "            '99pct': int(n_99)\n",
    "        }\n",
    "    },\n",
    "    'data_shape': {\n",
    "        'original': list(X_full.shape),\n",
    "        'reduced': list(X_reduced.shape)\n",
    "    },\n",
    "    'top_features': feature_importance_pca.head(20).to_dict('records')\n",
    "}\n",
    "\n",
    "save_model(pca_optimal, 'pca_optimal', pca_extra)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_pca.to_csv(MODEL_DIR / 'pca_feature_importance.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" \" * 25 + \"PCA ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nâœ… PCA model saved: {final_n_components} components\")\n",
    "print(f\"âœ… Dimensionality reduced by {dimensionality_reduction_pct:.1f}%\")\n",
    "print(f\"âœ… Retained {final_variance_retained*100:.2f}% of variance\")\n",
    "print(f\"âœ… Reconstruction RMSE: {final_rmse:.6f}\")\n",
    "print(f\"âœ… Feature importance saved to: {MODEL_DIR / 'pca_feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d9a75",
   "metadata": {},
   "source": [
    "## 5. Visualization (2D PCA Projections)\n",
    "We project the full standardized feature matrix to 2 principal components (outside of optimization) for consistent side-by-side cluster plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca62644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Refit best models on (possibly PCA-transformed) feature space chosen by best trial\n",
    "best_kmeans_params = study_kmeans.best_trial.params\n",
    "best_gmm_params = study_gmm.best_trial.params\n",
    "\n",
    "# Build best models again without PCA reduction (for consistent plotting base); PCA only for 2D viz\n",
    "# Check for pretrained models first\n",
    "if kmeans_best is None or FORCE_RETRAIN:\n",
    "    print(\"Building new KMeans model...\")\n",
    "    kmeans_best = KMeans(\n",
    "        n_clusters=best_kmeans_params['kmeans_n_clusters'],\n",
    "        init=best_kmeans_params['kmeans_init'],\n",
    "        algorithm=best_kmeans_params['kmeans_algorithm'],\n",
    "        n_init='auto',\n",
    "        random_state=42\n",
    "    ).fit(X_full)\n",
    "    # Save the model\n",
    "    kmeans_extra = {\n",
    "        'n_trials': len(study_kmeans.trials),\n",
    "        'best_silhouette': float(study_kmeans.best_value),\n",
    "        'hyperparameters': best_kmeans_params\n",
    "    }\n",
    "    save_model(kmeans_best, 'kmeans_best', kmeans_extra)\n",
    "\n",
    "if gmm_best is None or FORCE_RETRAIN:\n",
    "    print(\"Building new GMM model...\")\n",
    "    gmm_best = GaussianMixture(\n",
    "        n_components=best_gmm_params['gmm_n_components'],\n",
    "        covariance_type=best_gmm_params['gmm_covariance_type'],\n",
    "        reg_covar=best_gmm_params['gmm_reg_covar'],\n",
    "        random_state=42\n",
    "    ).fit(X_full)\n",
    "    # Save the model\n",
    "    gmm_extra = {\n",
    "        'n_trials': len(study_gmm.trials),\n",
    "        'best_silhouette': float(study_gmm.best_value),\n",
    "        'hyperparameters': best_gmm_params\n",
    "    }\n",
    "    save_model(gmm_best, 'gmm_best', gmm_extra)\n",
    "\n",
    "labels_kmeans = kmeans_best.predict(X_full)\n",
    "labels_gmm = gmm_best.predict(X_full)\n",
    "\n",
    "# PCA for viz only\n",
    "pca_viz = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca_viz.fit_transform(X_full)\n",
    "plot_df = pd.DataFrame({\n",
    "    'PC1': X_2d[:,0],\n",
    "    'PC2': X_2d[:,1],\n",
    "    'KMeans_Cluster': labels_kmeans.astype(str),\n",
    "    'GMM_Cluster': labels_gmm.astype(str)\n",
    "})\n",
    "\n",
    "fig1 = px.scatter(plot_df, x='PC1', y='PC2', color='KMeans_Cluster', title='KMeans Clusters (PCA 2D)')\n",
    "fig1.show()\n",
    "fig2 = px.scatter(plot_df, x='PC1', y='PC2', color='GMM_Cluster', title='GMM Clusters (PCA 2D)')\n",
    "fig2.show()\n",
    "\n",
    "print(\"Visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a661",
   "metadata": {},
   "source": [
    "## 6. Cluster Profiling & Comparison\n",
    "Generate aggregate statistics per cluster for both algorithms and compare metrics side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21513850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Model Comparison & Analysis ---\n",
    "\n",
    "# Generate predictions from best models\n",
    "labels_kmeans = kmeans_best.predict(X_full)\n",
    "labels_gmm = gmm_best.predict(X_full)\n",
    "\n",
    "# Recompute metrics for final comparison\n",
    "kmeans_final_metrics = compute_cluster_metrics(X_full, labels_kmeans)\n",
    "gmm_final_metrics = compute_cluster_metrics(X_full, labels_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19bc08",
   "metadata": {},
   "source": [
    "## 7. Notes & Next Steps\n",
    "Potential enhancements:\n",
    "- Add DBSCAN / HDBSCAN for density-based perspective.\n",
    "- Use feature selection or domain-driven grouping before clustering.\n",
    "- Evaluate stability across bootstrap samples.\n",
    "- Store cluster assignments back to database for downstream analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee0c97",
   "metadata": {},
   "source": [
    "## 8. Persist Best Models\n",
    "Save best KMeans and GMM models, hyperparameters, and metrics into `src/unsupervised/` for reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30262e1d",
   "metadata": {},
   "source": [
    "Metrics to compute later:\n",
    "- Silhouette Score (higher better)\n",
    "- Calinski-Harabasz Index (higher better)\n",
    "- Davies-Bouldin Index (lower better)\n",
    "\n",
    "We'll define reusable helper functions so both model families share logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7849a",
   "metadata": {},
   "source": [
    "## 11. Advanced Metric Performance Analysis\n",
    "Deep dive into clustering quality metrics with statistical tests and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metric Stability Analysis: Bootstrap Resampling ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "N_BOOTSTRAP = 30  # Number of bootstrap samples\n",
    "SAMPLE_SIZE = min(5000, len(X_full))  # Sample size for each bootstrap iteration\n",
    "\n",
    "print(f\"=== BOOTSTRAP STABILITY ANALYSIS ===\")\n",
    "print(f\"Running {N_BOOTSTRAP} bootstrap iterations with sample size {SAMPLE_SIZE}...\")\n",
    "\n",
    "kmeans_sil_scores = []\n",
    "gmm_sil_scores = []\n",
    "kmeans_ch_scores = []\n",
    "gmm_ch_scores = []\n",
    "kmeans_db_scores = []\n",
    "gmm_db_scores = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(N_BOOTSTRAP):\n",
    "    # Sample with replacement\n",
    "    indices = np.random.choice(len(X_full), size=SAMPLE_SIZE, replace=True)\n",
    "    X_boot = X_full[indices]\n",
    "    \n",
    "    # KMeans predictions\n",
    "    labels_k = kmeans_best.predict(X_boot)\n",
    "    metrics_k = compute_cluster_metrics(X_boot, labels_k)\n",
    "    kmeans_sil_scores.append(metrics_k['silhouette'])\n",
    "    kmeans_ch_scores.append(metrics_k['calinski_harabasz'])\n",
    "    kmeans_db_scores.append(metrics_k['davies_bouldin'])\n",
    "    \n",
    "    # GMM predictions\n",
    "    labels_g = gmm_best.predict(X_boot)\n",
    "    metrics_g = compute_cluster_metrics(X_boot, labels_g)\n",
    "    gmm_sil_scores.append(metrics_g['silhouette'])\n",
    "    gmm_ch_scores.append(metrics_g['calinski_harabasz'])\n",
    "    gmm_db_scores.append(metrics_g['davies_bouldin'])\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Completed {i + 1}/{N_BOOTSTRAP} iterations...\")\n",
    "\n",
    "# Calculate statistics\n",
    "bootstrap_stats = pd.DataFrame({\n",
    "    'Metric': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin'] * 2,\n",
    "    'Model': ['KMeans']*3 + ['GMM']*3,\n",
    "    'Mean': [\n",
    "        np.mean(kmeans_sil_scores), np.mean(kmeans_ch_scores), np.mean(kmeans_db_scores),\n",
    "        np.mean(gmm_sil_scores), np.mean(gmm_ch_scores), np.mean(gmm_db_scores)\n",
    "    ],\n",
    "    'Std': [\n",
    "        np.std(kmeans_sil_scores), np.std(kmeans_ch_scores), np.std(kmeans_db_scores),\n",
    "        np.std(gmm_sil_scores), np.std(gmm_ch_scores), np.std(gmm_db_scores)\n",
    "    ],\n",
    "    'Min': [\n",
    "        np.min(kmeans_sil_scores), np.min(kmeans_ch_scores), np.min(kmeans_db_scores),\n",
    "        np.min(gmm_sil_scores), np.min(gmm_ch_scores), np.min(gmm_db_scores)\n",
    "    ],\n",
    "    'Max': [\n",
    "        np.max(kmeans_sil_scores), np.max(kmeans_ch_scores), np.max(kmeans_db_scores),\n",
    "        np.max(gmm_sil_scores), np.max(gmm_ch_scores), np.max(gmm_db_scores)\n",
    "    ],\n",
    "    'CV': [\n",
    "        np.std(kmeans_sil_scores)/np.mean(kmeans_sil_scores),\n",
    "        np.std(kmeans_ch_scores)/np.mean(kmeans_ch_scores),\n",
    "        np.std(kmeans_db_scores)/np.mean(kmeans_db_scores),\n",
    "        np.std(gmm_sil_scores)/np.mean(gmm_sil_scores),\n",
    "        np.std(gmm_ch_scores)/np.mean(gmm_ch_scores),\n",
    "        np.std(gmm_db_scores)/np.mean(gmm_db_scores)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== BOOTSTRAP METRIC STABILITY ===\")\n",
    "display(bootstrap_stats.round(4))\n",
    "\n",
    "# Statistical significance tests\n",
    "print(\"\\n=== STATISTICAL SIGNIFICANCE TESTS (Mann-Whitney U) ===\")\n",
    "sil_stat, sil_pval = stats.mannwhitneyu(kmeans_sil_scores, gmm_sil_scores, alternative='two-sided')\n",
    "ch_stat, ch_pval = stats.mannwhitneyu(kmeans_ch_scores, gmm_ch_scores, alternative='two-sided')\n",
    "db_stat, db_pval = stats.mannwhitneyu(kmeans_db_scores, gmm_db_scores, alternative='two-sided')\n",
    "\n",
    "print(f\"Silhouette Score: U={sil_stat:.2f}, p-value={sil_pval:.4f} {'***' if sil_pval < 0.001 else '**' if sil_pval < 0.01 else '*' if sil_pval < 0.05 else 'ns'}\")\n",
    "print(f\"Calinski-Harabasz: U={ch_stat:.2f}, p-value={ch_pval:.4f} {'***' if ch_pval < 0.001 else '**' if ch_pval < 0.01 else '*' if ch_pval < 0.05 else 'ns'}\")\n",
    "print(f\"Davies-Bouldin: U={db_stat:.2f}, p-value={db_pval:.4f} {'***' if db_pval < 0.001 else '**' if db_pval < 0.01 else '*' if db_pval < 0.05 else 'ns'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bootstrap Visualizations ---\n",
    "\n",
    "# Visualize bootstrap distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].hist(kmeans_sil_scores, alpha=0.5, bins=15, label='KMeans', color='blue')\n",
    "axes[0].hist(gmm_sil_scores, alpha=0.5, bins=15, label='GMM', color='orange')\n",
    "axes[0].set_xlabel('Silhouette Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Silhouette Score Distribution (Bootstrap)', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].axvline(np.mean(kmeans_sil_scores), color='blue', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(np.mean(gmm_sil_scores), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "axes[1].hist(kmeans_ch_scores, alpha=0.5, bins=15, label='KMeans', color='blue')\n",
    "axes[1].hist(gmm_ch_scores, alpha=0.5, bins=15, label='GMM', color='orange')\n",
    "axes[1].set_xlabel('Calinski-Harabasz Index', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Calinski-Harabasz Distribution (Bootstrap)', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].axvline(np.mean(kmeans_ch_scores), color='blue', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(np.mean(gmm_ch_scores), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "axes[2].hist(kmeans_db_scores, alpha=0.5, bins=15, label='KMeans', color='blue')\n",
    "axes[2].hist(gmm_db_scores, alpha=0.5, bins=15, label='GMM', color='orange')\n",
    "axes[2].set_xlabel('Davies-Bouldin Index', fontsize=12)\n",
    "axes[2].set_ylabel('Frequency', fontsize=12)\n",
    "axes[2].set_title('Davies-Bouldin Distribution (Bootstrap)', fontsize=14)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].axvline(np.mean(kmeans_db_scores), color='blue', linestyle='--', linewidth=2)\n",
    "axes[2].axvline(np.mean(gmm_db_scores), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'bootstrap_metric_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Bootstrap distribution plot saved to: {MODEL_DIR / 'bootstrap_metric_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801e99f",
   "metadata": {},
   "source": [
    "## 12. Cluster Quality Deep Dive\n",
    "Analyze within-cluster cohesion, between-cluster separation, and silhouette analysis per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14389de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-Cluster Silhouette Analysis ---\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"=== PER-CLUSTER SILHOUETTE ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate silhouette scores for each sample\n",
    "silhouette_kmeans = silhouette_samples(X_full, labels_kmeans)\n",
    "silhouette_gmm = silhouette_samples(X_full, labels_gmm)\n",
    "\n",
    "# KMeans per-cluster analysis\n",
    "kmeans_cluster_silhouettes = []\n",
    "for i in range(best_kmeans_params['kmeans_n_clusters']):\n",
    "    cluster_mask = labels_kmeans == i\n",
    "    cluster_sil = silhouette_kmeans[cluster_mask]\n",
    "    kmeans_cluster_silhouettes.append({\n",
    "        'Cluster': i,\n",
    "        'Size': cluster_mask.sum(),\n",
    "        'Mean_Silhouette': cluster_sil.mean(),\n",
    "        'Std_Silhouette': cluster_sil.std(),\n",
    "        'Min_Silhouette': cluster_sil.min(),\n",
    "        'Max_Silhouette': cluster_sil.max(),\n",
    "        'Pct_Negative': (cluster_sil < 0).sum() / len(cluster_sil) * 100\n",
    "    })\n",
    "\n",
    "kmeans_sil_df = pd.DataFrame(kmeans_cluster_silhouettes)\n",
    "print(\"KMeans - Per-Cluster Silhouette Scores:\")\n",
    "display(kmeans_sil_df.round(4))\n",
    "\n",
    "# GMM per-cluster analysis\n",
    "gmm_cluster_silhouettes = []\n",
    "for i in range(best_gmm_params['gmm_n_components']):\n",
    "    cluster_mask = labels_gmm == i\n",
    "    cluster_sil = silhouette_gmm[cluster_mask]\n",
    "    gmm_cluster_silhouettes.append({\n",
    "        'Cluster': i,\n",
    "        'Size': cluster_mask.sum(),\n",
    "        'Mean_Silhouette': cluster_sil.mean(),\n",
    "        'Std_Silhouette': cluster_sil.std(),\n",
    "        'Min_Silhouette': cluster_sil.min(),\n",
    "        'Max_Silhouette': cluster_sil.max(),\n",
    "        'Pct_Negative': (cluster_sil < 0).sum() / len(cluster_sil) * 100\n",
    "    })\n",
    "\n",
    "gmm_sil_df = pd.DataFrame(gmm_cluster_silhouettes)\n",
    "print(\"\\nGMM - Per-Cluster Silhouette Scores:\")\n",
    "display(gmm_sil_df.round(4))\n",
    "\n",
    "# Visualize silhouette plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# KMeans silhouette plot\n",
    "y_lower = 10\n",
    "for i in range(best_kmeans_params['kmeans_n_clusters']):\n",
    "    cluster_sil_values = silhouette_kmeans[labels_kmeans == i]\n",
    "    cluster_sil_values.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_sil_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.nipy_spectral(float(i) / best_kmeans_params['kmeans_n_clusters'])\n",
    "    axes[0].fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    axes[0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "axes[0].set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "axes[0].set_ylabel('Cluster Label', fontsize=12)\n",
    "axes[0].set_title(f'KMeans Silhouette Plot (n={best_kmeans_params[\"kmeans_n_clusters\"]} clusters)', fontsize=14)\n",
    "axes[0].axvline(x=silhouette_kmeans.mean(), color=\"red\", linestyle=\"--\", label=f'Mean: {silhouette_kmeans.mean():.3f}')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# GMM silhouette plot\n",
    "y_lower = 10\n",
    "for i in range(best_gmm_params['gmm_n_components']):\n",
    "    cluster_sil_values = silhouette_gmm[labels_gmm == i]\n",
    "    cluster_sil_values.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_sil_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.nipy_spectral(float(i) / best_gmm_params['gmm_n_components'])\n",
    "    axes[1].fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    axes[1].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "axes[1].set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "axes[1].set_ylabel('Cluster Label', fontsize=12)\n",
    "axes[1].set_title(f'GMM Silhouette Plot (n={best_gmm_params[\"gmm_n_components\"]} components)', fontsize=14)\n",
    "axes[1].axvline(x=silhouette_gmm.mean(), color=\"red\", linestyle=\"--\", label=f'Mean: {silhouette_gmm.mean():.3f}')\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'silhouette_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Silhouette plots saved to: {MODEL_DIR / 'silhouette_plots.png'}\")\n",
    "\n",
    "# Identify poorly clustered samples\n",
    "kmeans_poorly_clustered = (silhouette_kmeans < 0).sum()\n",
    "gmm_poorly_clustered = (silhouette_gmm < 0).sum()\n",
    "\n",
    "print(f\"\\n=== POORLY CLUSTERED SAMPLES (Negative Silhouette) ===\")\n",
    "print(f\"KMeans: {kmeans_poorly_clustered} samples ({kmeans_poorly_clustered/len(X_full)*100:.2f}%)\")\n",
    "print(f\"GMM: {gmm_poorly_clustered} samples ({gmm_poorly_clustered/len(X_full)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Intra-cluster & Inter-cluster Distance Analysis ---\n",
    "\n",
    "print(\"=== INTRA-CLUSTER AND INTER-CLUSTER DISTANCE ANALYSIS ===\\n\")\n",
    "\n",
    "# KMeans analysis\n",
    "kmeans_centers = kmeans_best.cluster_centers_\n",
    "kmeans_intra_distances = []\n",
    "kmeans_cluster_compactness = []\n",
    "\n",
    "for i in range(best_kmeans_params['kmeans_n_clusters']):\n",
    "    cluster_points = X_full[labels_kmeans == i]\n",
    "    center = kmeans_centers[i]\n",
    "    # Calculate distances to cluster center\n",
    "    distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "    kmeans_intra_distances.extend(distances)\n",
    "    kmeans_cluster_compactness.append({\n",
    "        'Cluster': i,\n",
    "        'Mean_Distance_to_Center': distances.mean(),\n",
    "        'Std_Distance': distances.std(),\n",
    "        'Max_Distance': distances.max(),\n",
    "        'Radius_95pct': np.percentile(distances, 95)\n",
    "    })\n",
    "\n",
    "# Inter-cluster distances (center-to-center)\n",
    "kmeans_inter_distances = cdist(kmeans_centers, kmeans_centers, metric='euclidean')\n",
    "np.fill_diagonal(kmeans_inter_distances, np.nan)  # Ignore self-distances\n",
    "\n",
    "kmeans_compactness_df = pd.DataFrame(kmeans_cluster_compactness)\n",
    "print(\"KMeans - Cluster Compactness (Distance to Center):\")\n",
    "display(kmeans_compactness_df.round(4))\n",
    "\n",
    "print(f\"\\nKMeans - Inter-cluster distances (center-to-center):\")\n",
    "print(f\"  Mean: {np.nanmean(kmeans_inter_distances):.4f}\")\n",
    "print(f\"  Min: {np.nanmin(kmeans_inter_distances):.4f}\")\n",
    "print(f\"  Max: {np.nanmax(kmeans_inter_distances):.4f}\")\n",
    "print(f\"  Separation Ratio (inter/intra): {np.nanmean(kmeans_inter_distances) / np.mean(kmeans_intra_distances):.4f}\")\n",
    "\n",
    "# GMM analysis - use predicted cluster centers (mean of each component)\n",
    "gmm_cluster_compactness = []\n",
    "gmm_intra_distances = []\n",
    "\n",
    "for i in range(best_gmm_params['gmm_n_components']):\n",
    "    cluster_points = X_full[labels_gmm == i]\n",
    "    center = cluster_points.mean(axis=0)  # Empirical center\n",
    "    distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "    gmm_intra_distances.extend(distances)\n",
    "    gmm_cluster_compactness.append({\n",
    "        'Cluster': i,\n",
    "        'Mean_Distance_to_Center': distances.mean(),\n",
    "        'Std_Distance': distances.std(),\n",
    "        'Max_Distance': distances.max(),\n",
    "        'Radius_95pct': np.percentile(distances, 95)\n",
    "    })\n",
    "\n",
    "# GMM inter-cluster distances\n",
    "gmm_centers = np.array([X_full[labels_gmm == i].mean(axis=0) for i in range(best_gmm_params['gmm_n_components'])])\n",
    "gmm_inter_distances = cdist(gmm_centers, gmm_centers, metric='euclidean')\n",
    "np.fill_diagonal(gmm_inter_distances, np.nan)\n",
    "\n",
    "gmm_compactness_df = pd.DataFrame(gmm_cluster_compactness)\n",
    "print(\"\\nGMM - Cluster Compactness (Distance to Empirical Center):\")\n",
    "display(gmm_compactness_df.round(4))\n",
    "\n",
    "print(f\"\\nGMM - Inter-cluster distances (center-to-center):\")\n",
    "print(f\"  Mean: {np.nanmean(gmm_inter_distances):.4f}\")\n",
    "print(f\"  Min: {np.nanmin(gmm_inter_distances):.4f}\")\n",
    "print(f\"  Max: {np.nanmax(gmm_inter_distances):.4f}\")\n",
    "print(f\"  Separation Ratio (inter/intra): {np.nanmean(gmm_inter_distances) / np.mean(gmm_intra_distances):.4f}\")\n",
    "\n",
    "# Visualize distance distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "axes[0].hist(kmeans_intra_distances, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_xlabel('Distance to Cluster Center', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('KMeans - Intra-cluster Distance Distribution', fontsize=14)\n",
    "axes[0].axvline(np.mean(kmeans_intra_distances), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(kmeans_intra_distances):.3f}')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "axes[1].hist(gmm_intra_distances, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Distance to Cluster Center', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('GMM - Intra-cluster Distance Distribution', fontsize=14)\n",
    "axes[1].axvline(np.mean(gmm_intra_distances), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(gmm_intra_distances):.3f}')\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'distance_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Distance distribution plots saved to: {MODEL_DIR / 'distance_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a074a04",
   "metadata": {},
   "source": [
    "## 13. Feature Importance & Cluster Characterization\n",
    "Identify which features drive cluster separation and characterize each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance via ANOVA F-statistic ---\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE FOR CLUSTER SEPARATION ===\\n\")\n",
    "\n",
    "# Calculate F-statistics for each feature across clusters\n",
    "f_stats_kmeans, p_values_kmeans = f_classif(X_full, labels_kmeans)\n",
    "f_stats_gmm, p_values_gmm = f_classif(X_full, labels_gmm)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_names = df_numeric.columns.tolist()\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'KMeans_F_Stat': f_stats_kmeans,\n",
    "    'KMeans_P_Value': p_values_kmeans,\n",
    "    'GMM_F_Stat': f_stats_gmm,\n",
    "    'GMM_P_Value': p_values_gmm\n",
    "})\n",
    "\n",
    "# Rank features\n",
    "feature_importance['KMeans_Rank'] = feature_importance['KMeans_F_Stat'].rank(ascending=False)\n",
    "feature_importance['GMM_Rank'] = feature_importance['GMM_F_Stat'].rank(ascending=False)\n",
    "feature_importance['Avg_Rank'] = (feature_importance['KMeans_Rank'] + feature_importance['GMM_Rank']) / 2\n",
    "\n",
    "# Sort by average rank\n",
    "feature_importance = feature_importance.sort_values('Avg_Rank')\n",
    "\n",
    "print(\"Top 20 Most Important Features for Cluster Separation:\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "print(\"\\nBottom 10 Least Important Features:\")\n",
    "display(feature_importance.tail(10))\n",
    "\n",
    "# Visualize top features\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# KMeans feature importance\n",
    "axes[0].barh(range(top_n), top_features['KMeans_F_Stat'].values)\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(top_features['Feature'].values, fontsize=9)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('F-Statistic', fontsize=12)\n",
    "axes[0].set_title('Top 15 Features - KMeans Cluster Separation', fontsize=14)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# GMM feature importance\n",
    "axes[1].barh(range(top_n), top_features['GMM_F_Stat'].values, color='orange')\n",
    "axes[1].set_yticks(range(top_n))\n",
    "axes[1].set_yticklabels(top_features['Feature'].values, fontsize=9)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('F-Statistic', fontsize=12)\n",
    "axes[1].set_title('Top 15 Features - GMM Cluster Separation', fontsize=14)\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature importance plot saved to: {MODEL_DIR / 'feature_importance.png'}\")\n",
    "\n",
    "# Save feature importance to CSV\n",
    "feature_importance.to_csv(MODEL_DIR / 'feature_importance.csv', index=False)\n",
    "print(f\"ðŸ’¾ Feature importance saved to: {MODEL_DIR / 'feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cluster Characterization: Statistical Profiles ---\n",
    "\n",
    "print(\"=== CLUSTER CHARACTERIZATION ===\\n\")\n",
    "\n",
    "# Get top 10 most important features for detailed profiling\n",
    "top_10_features = feature_importance.head(10)['Feature'].tolist()\n",
    "\n",
    "# Add cluster labels to original numeric dataframe\n",
    "profile_df_kmeans = df_numeric[top_10_features].copy()\n",
    "profile_df_kmeans['cluster'] = labels_kmeans\n",
    "\n",
    "profile_df_gmm = df_numeric[top_10_features].copy()\n",
    "profile_df_gmm['cluster'] = labels_gmm\n",
    "\n",
    "# KMeans cluster profiles\n",
    "print(\"KMeans - Cluster Profiles (Top 10 Features):\")\n",
    "kmeans_profiles = profile_df_kmeans.groupby('cluster').agg(['mean', 'std', 'median', 'min', 'max'])\n",
    "display(kmeans_profiles.round(3))\n",
    "\n",
    "# GMM cluster profiles\n",
    "print(\"\\nGMM - Cluster Profiles (Top 10 Features):\")\n",
    "gmm_profiles = profile_df_gmm.groupby('cluster').agg(['mean', 'std', 'median', 'min', 'max'])\n",
    "display(gmm_profiles.round(3))\n",
    "\n",
    "# Create heatmap of cluster centers (standardized features)\n",
    "print(\"\\n=== CLUSTER CENTER HEATMAPS ===\")\n",
    "\n",
    "# KMeans centers for top features\n",
    "top_feature_indices = [df_numeric.columns.get_loc(f) for f in top_10_features]\n",
    "kmeans_centers_top = kmeans_centers[:, top_feature_indices]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# KMeans heatmap\n",
    "im1 = axes[0].imshow(kmeans_centers_top, cmap='RdBu_r', aspect='auto', interpolation='nearest')\n",
    "axes[0].set_xticks(range(len(top_10_features)))\n",
    "axes[0].set_xticklabels(top_10_features, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_yticks(range(best_kmeans_params['kmeans_n_clusters']))\n",
    "axes[0].set_yticklabels([f\"Cluster {i}\" for i in range(best_kmeans_params['kmeans_n_clusters'])], fontsize=10)\n",
    "axes[0].set_title('KMeans Cluster Centers (Standardized)', fontsize=14)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# GMM centers (empirical) for top features\n",
    "gmm_centers_top = gmm_centers[:, top_feature_indices]\n",
    "im2 = axes[1].imshow(gmm_centers_top, cmap='RdBu_r', aspect='auto', interpolation='nearest')\n",
    "axes[1].set_xticks(range(len(top_10_features)))\n",
    "axes[1].set_xticklabels(top_10_features, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_yticks(range(best_gmm_params['gmm_n_components']))\n",
    "axes[1].set_yticklabels([f\"Cluster {i}\" for i in range(best_gmm_params['gmm_n_components'])], fontsize=10)\n",
    "axes[1].set_title('GMM Cluster Centers (Standardized)', fontsize=14)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'cluster_center_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Cluster center heatmaps saved to: {MODEL_DIR / 'cluster_center_heatmaps.png'}\")\n",
    "\n",
    "# Identify most distinctive clusters (highest variance in features)\n",
    "kmeans_cluster_distinctiveness = []\n",
    "for i in range(best_kmeans_params['kmeans_n_clusters']):\n",
    "    center_distances = np.linalg.norm(kmeans_centers - kmeans_centers[i], axis=1)\n",
    "    center_distances[i] = np.inf  # Exclude self\n",
    "    kmeans_cluster_distinctiveness.append({\n",
    "        'Cluster': i,\n",
    "        'Min_Distance_to_Other': center_distances.min(),\n",
    "        'Mean_Distance_to_Others': center_distances[center_distances != np.inf].mean()\n",
    "    })\n",
    "\n",
    "kmeans_distinct_df = pd.DataFrame(kmeans_cluster_distinctiveness)\n",
    "print(\"\\nKMeans - Cluster Distinctiveness (Distance from other clusters):\")\n",
    "display(kmeans_distinct_df.round(4))\n",
    "\n",
    "gmm_cluster_distinctiveness = []\n",
    "for i in range(best_gmm_params['gmm_n_components']):\n",
    "    center_distances = np.linalg.norm(gmm_centers - gmm_centers[i], axis=1)\n",
    "    center_distances[i] = np.inf  # Exclude self\n",
    "    gmm_cluster_distinctiveness.append({\n",
    "        'Cluster': i,\n",
    "        'Min_Distance_to_Other': center_distances.min(),\n",
    "        'Mean_Distance_to_Others': center_distances[center_distances != np.inf].mean()\n",
    "    })\n",
    "\n",
    "gmm_distinct_df = pd.DataFrame(gmm_cluster_distinctiveness)\n",
    "print(\"\\nGMM - Cluster Distinctiveness (Distance from other clusters):\")\n",
    "display(gmm_distinct_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab5a5b",
   "metadata": {},
   "source": [
    "## 14. Cluster Agreement & Overlap Analysis\n",
    "Compare cluster assignments between KMeans and GMM to understand model agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb098f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cluster Agreement Analysis ---\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "print(\"=== CLUSTER AGREEMENT BETWEEN KMEANS AND GMM ===\\n\")\n",
    "\n",
    "# Calculate agreement metrics\n",
    "ari = adjusted_rand_score(labels_kmeans, labels_gmm)\n",
    "nmi = normalized_mutual_info_score(labels_kmeans, labels_gmm)\n",
    "fmi = fowlkes_mallows_score(labels_kmeans, labels_gmm)\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"  Interpretation: {ari:.4f} {'(Perfect agreement)' if ari > 0.9 else '(High agreement)' if ari > 0.7 else '(Moderate agreement)' if ari > 0.4 else '(Low agreement)'}\")\n",
    "print(f\"\\nNormalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(f\"  Interpretation: {nmi:.4f} {'(Perfect agreement)' if nmi > 0.9 else '(High agreement)' if nmi > 0.7 else '(Moderate agreement)' if nmi > 0.4 else '(Low agreement)'}\")\n",
    "print(f\"\\nFowlkes-Mallows Index (FMI): {fmi:.4f}\")\n",
    "print(f\"  Interpretation: {fmi:.4f} {'(Perfect agreement)' if fmi > 0.9 else '(High agreement)' if fmi > 0.7 else '(Moderate agreement)' if fmi > 0.4 else '(Low agreement)'}\")\n",
    "\n",
    "# Create contingency matrix\n",
    "contingency = contingency_matrix(labels_kmeans, labels_gmm)\n",
    "contingency_df = pd.DataFrame(\n",
    "    contingency,\n",
    "    index=[f'KMeans_{i}' for i in range(best_kmeans_params['kmeans_n_clusters'])],\n",
    "    columns=[f'GMM_{i}' for i in range(best_gmm_params['gmm_n_components'])]\n",
    ")\n",
    "\n",
    "print(\"\\n=== CONTINGENCY TABLE (Sample Overlap) ===\")\n",
    "print(\"Rows: KMeans clusters, Columns: GMM clusters\")\n",
    "display(contingency_df)\n",
    "\n",
    "# Visualize contingency matrix as heatmap\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(contingency, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(contingency.shape[0]):\n",
    "    for j in range(contingency.shape[1]):\n",
    "        text = ax.text(j, i, contingency[i, j],\n",
    "                      ha=\"center\", va=\"center\", color=\"black\" if contingency[i, j] < contingency.max()/2 else \"white\")\n",
    "\n",
    "ax.set_xticks(range(best_gmm_params['gmm_n_components']))\n",
    "ax.set_yticks(range(best_kmeans_params['kmeans_n_clusters']))\n",
    "ax.set_xticklabels([f'GMM {i}' for i in range(best_gmm_params['gmm_n_components'])], fontsize=10)\n",
    "ax.set_yticklabels([f'KMeans {i}' for i in range(best_kmeans_params['kmeans_n_clusters'])], fontsize=10)\n",
    "ax.set_xlabel('GMM Clusters', fontsize=12)\n",
    "ax.set_ylabel('KMeans Clusters', fontsize=12)\n",
    "ax.set_title('Cluster Overlap: KMeans vs GMM\\n(Number of shared samples)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Sample Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'cluster_agreement_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Cluster agreement heatmap saved to: {MODEL_DIR / 'cluster_agreement_heatmap.png'}\")\n",
    "\n",
    "# Identify samples with different cluster assignments\n",
    "disagreement_mask = labels_kmeans != labels_gmm\n",
    "n_disagreements = disagreement_mask.sum()\n",
    "pct_disagreement = (n_disagreements / len(labels_kmeans)) * 100\n",
    "\n",
    "print(f\"\\n=== CLUSTER ASSIGNMENT DISAGREEMENT ===\")\n",
    "print(f\"Samples with different cluster assignments: {n_disagreements} ({pct_disagreement:.2f}%)\")\n",
    "print(f\"Samples with same cluster assignments: {len(labels_kmeans) - n_disagreements} ({100 - pct_disagreement:.2f}%)\")\n",
    "\n",
    "# For disagreeing samples, analyze their characteristics\n",
    "if n_disagreements > 0:\n",
    "    disagreement_analysis = pd.DataFrame({\n",
    "        'KMeans_Cluster': labels_kmeans[disagreement_mask],\n",
    "        'GMM_Cluster': labels_gmm[disagreement_mask],\n",
    "        'KMeans_Silhouette': silhouette_kmeans[disagreement_mask],\n",
    "        'GMM_Silhouette': silhouette_gmm[disagreement_mask]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDisagreement Sample Statistics:\")\n",
    "    print(f\"  Mean KMeans Silhouette: {disagreement_analysis['KMeans_Silhouette'].mean():.4f}\")\n",
    "    print(f\"  Mean GMM Silhouette: {disagreement_analysis['GMM_Silhouette'].mean():.4f}\")\n",
    "    print(f\"  Samples with negative silhouette in both: {((disagreement_analysis['KMeans_Silhouette'] < 0) & (disagreement_analysis['GMM_Silhouette'] < 0)).sum()}\")\n",
    "    \n",
    "    print(\"\\nTop 10 most common disagreement patterns:\")\n",
    "    disagreement_patterns = disagreement_analysis.groupby(['KMeans_Cluster', 'GMM_Cluster']).size().sort_values(ascending=False)\n",
    "    display(disagreement_patterns.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9063a7",
   "metadata": {},
   "source": [
    "## 15. Comprehensive Performance Report\n",
    "Generate final summary report with all models (Clustering: KMeans, GMM | Dimensionality Reduction: PCA) and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Performance Report: Generation ---\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 15 + \"UNSUPERVISED LEARNING - COMPREHENSIVE FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build final report dictionary with all three models\n",
    "final_report = {\n",
    "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(X_full),\n",
    "        'n_features': X_full.shape[1],\n",
    "        'n_numeric_features': len(df_numeric.columns)\n",
    "    },\n",
    "    'models_summary': {\n",
    "        'clustering': ['KMeans', 'GMM'],\n",
    "        'dimensionality_reduction': ['PCA'],\n",
    "        'total_models': 3\n",
    "    },\n",
    "    'optimization_summary': {\n",
    "        'kmeans': {\n",
    "            'n_trials': len(study_kmeans.trials),\n",
    "            'best_n_clusters': best_kmeans_params['kmeans_n_clusters'],\n",
    "            'best_silhouette': float(study_kmeans.best_value),\n",
    "            'hyperparameters': best_kmeans_params\n",
    "        },\n",
    "        'gmm': {\n",
    "            'n_trials': len(study_gmm.trials),\n",
    "            'best_n_components': best_gmm_params['gmm_n_components'],\n",
    "            'best_silhouette': float(study_gmm.best_value),\n",
    "            'hyperparameters': best_gmm_params\n",
    "        },\n",
    "        'pca': {\n",
    "            'method': 'variance_threshold',\n",
    "            'target_variance': 0.95,\n",
    "            'n_components': pca_metrics['n_components'],\n",
    "            'optimization_criterion': 'cumulative_variance_95pct'\n",
    "        }\n",
    "    },\n",
    "    'final_metrics': {\n",
    "        'kmeans': {\n",
    "            'silhouette': float(kmeans_final_metrics['silhouette']),\n",
    "            'calinski_harabasz': float(kmeans_final_metrics['calinski_harabasz']),\n",
    "            'davies_bouldin': float(kmeans_final_metrics['davies_bouldin']),\n",
    "            'inertia': float(kmeans_best.inertia_)\n",
    "        },\n",
    "        'gmm': {\n",
    "            'silhouette': float(gmm_final_metrics['silhouette']),\n",
    "            'calinski_harabasz': float(gmm_final_metrics['calinski_harabasz']),\n",
    "            'davies_bouldin': float(gmm_final_metrics['davies_bouldin']),\n",
    "            'aic': float(gmm_best.aic(X_full)),\n",
    "            'bic': float(gmm_best.bic(X_full))\n",
    "        }\n",
    "    },\n",
    "    'bootstrap_stability': {\n",
    "        'kmeans': {\n",
    "            'silhouette_mean': float(np.mean(kmeans_sil_scores)),\n",
    "            'silhouette_std': float(np.std(kmeans_sil_scores)),\n",
    "            'silhouette_cv': float(np.std(kmeans_sil_scores) / np.mean(kmeans_sil_scores))\n",
    "        },\n",
    "        'gmm': {\n",
    "            'silhouette_mean': float(np.mean(gmm_sil_scores)),\n",
    "            'silhouette_std': float(np.std(gmm_sil_scores)),\n",
    "            'silhouette_cv': float(np.std(gmm_sil_scores) / np.mean(gmm_sil_scores))\n",
    "        },\n",
    "        'statistical_tests': {\n",
    "            'mann_whitney_u': {\n",
    "                'silhouette_pvalue': float(sil_pval),\n",
    "                'significant': bool(sil_pval < 0.05)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'cluster_quality': {\n",
    "        'kmeans': {\n",
    "            'poorly_clustered_pct': float(kmeans_poorly_clustered / len(X_full) * 100),\n",
    "            'mean_intra_distance': float(np.mean(kmeans_intra_distances)),\n",
    "            'mean_inter_distance': float(np.nanmean(kmeans_inter_distances)),\n",
    "            'separation_ratio': float(np.nanmean(kmeans_inter_distances) / np.mean(kmeans_intra_distances)),\n",
    "            'cluster_balance_cv': float(kmeans_cv)\n",
    "        },\n",
    "        'gmm': {\n",
    "            'poorly_clustered_pct': float(gmm_poorly_clustered / len(X_full) * 100),\n",
    "            'mean_intra_distance': float(np.mean(gmm_intra_distances)),\n",
    "            'mean_inter_distance': float(np.nanmean(gmm_inter_distances)),\n",
    "            'separation_ratio': float(np.nanmean(gmm_inter_distances) / np.mean(gmm_intra_distances)),\n",
    "            'cluster_balance_cv': float(gmm_cv)\n",
    "        }\n",
    "    },\n",
    "    'dimensionality_reduction_metrics': {\n",
    "        'pca': pca_metrics\n",
    "    },\n",
    "    'model_agreement': {\n",
    "        'adjusted_rand_index': float(ari),\n",
    "        'normalized_mutual_info': float(nmi),\n",
    "        'fowlkes_mallows_index': float(fmi),\n",
    "        'disagreement_pct': float(pct_disagreement)\n",
    "    },\n",
    "    'top_features': feature_importance.head(10)[['Feature', 'KMeans_F_Stat', 'GMM_F_Stat']].to_dict('records')\n",
    "}\n",
    "\n",
    "# Print key findings\n",
    "print(\"\\nðŸ“Š KEY FINDINGS\\n\")\n",
    "print(f\"1. Dataset: {final_report['dataset_info']['total_samples']:,} samples, {final_report['dataset_info']['n_features']} features\")\n",
    "\n",
    "print(f\"\\n2. Model Summary:\")\n",
    "print(f\"   Clustering Models: KMeans, GMM\")\n",
    "print(f\"   Dimensionality Reduction: PCA\")\n",
    "print(f\"   Total Models Analyzed: 3\")\n",
    "\n",
    "print(f\"\\n3. Best Configurations:\")\n",
    "print(f\"   KMeans: {final_report['optimization_summary']['kmeans']['best_n_clusters']} clusters\")\n",
    "print(f\"   GMM: {final_report['optimization_summary']['gmm']['best_n_components']} components\")\n",
    "print(f\"   PCA: {final_report['optimization_summary']['pca']['n_components']} components ({final_report['optimization_summary']['pca']['target_variance']*100:.0f}% variance)\")\n",
    "\n",
    "print(f\"\\n4. Clustering Performance Metrics:\")\n",
    "print(f\"   {'Metric':<25} {'KMeans':<15} {'GMM':<15}\")\n",
    "print(f\"   {'-'*55}\")\n",
    "for metric in ['silhouette', 'calinski_harabasz', 'davies_bouldin']:\n",
    "    k_val = final_report['final_metrics']['kmeans'][metric]\n",
    "    g_val = final_report['final_metrics']['gmm'][metric]\n",
    "    print(f\"   {metric.replace('_', ' ').title():<25} {k_val:<15.4f} {g_val:<15.4f}\")\n",
    "\n",
    "print(f\"\\n5. Dimensionality Reduction Metrics (PCA):\")\n",
    "pca_m = final_report['dimensionality_reduction_metrics']['pca']\n",
    "print(f\"   Original dimensions: {pca_m['original_dimensions']}\")\n",
    "print(f\"   Reduced dimensions: {pca_m['reduced_dimensions']}\")\n",
    "print(f\"   Dimensionality reduction: {pca_m['dimensionality_reduction_pct']:.1f}%\")\n",
    "print(f\"   Variance retained: {pca_m['variance_retained']:.4f} ({pca_m['variance_retained']*100:.2f}%)\")\n",
    "print(f\"   Reconstruction RMSE: {pca_m['reconstruction_error_rmse']:.6f}\")\n",
    "print(f\"   Compression ratio: {pca_m['compression_ratio']:.2f}x\")\n",
    "\n",
    "print(f\"\\n6. Stability (Bootstrap Analysis - Clustering):\")\n",
    "print(f\"   KMeans Silhouette CV: {final_report['bootstrap_stability']['kmeans']['silhouette_cv']:.4f}\")\n",
    "print(f\"   GMM Silhouette CV: {final_report['bootstrap_stability']['gmm']['silhouette_cv']:.4f}\")\n",
    "print(f\"   Statistical Significance: {'Yes' if final_report['bootstrap_stability']['statistical_tests']['mann_whitney_u']['significant'] else 'No'} (p={final_report['bootstrap_stability']['statistical_tests']['mann_whitney_u']['silhouette_pvalue']:.4f})\")\n",
    "\n",
    "print(f\"\\n7. Cluster Quality:\")\n",
    "for model in ['kmeans', 'gmm']:\n",
    "    pct = final_report['cluster_quality'][model]['poorly_clustered_pct']\n",
    "    sep = final_report['cluster_quality'][model]['separation_ratio']\n",
    "    print(f\"   {model.upper()} - Poorly clustered: {pct:.2f}%, Separation Ratio: {sep:.4f}\")\n",
    "\n",
    "print(f\"\\n8. Model Agreement (Clustering):\")\n",
    "print(f\"   Adjusted Rand Index: {final_report['model_agreement']['adjusted_rand_index']:.4f}\")\n",
    "print(f\"   Normalized Mutual Info: {final_report['model_agreement']['normalized_mutual_info']:.4f}\")\n",
    "print(f\"   Disagreement Rate: {final_report['model_agreement']['disagreement_pct']:.2f}%\")\n",
    "\n",
    "print(f\"\\n9. Top 5 Most Important Features (Clustering):\")\n",
    "for i, feat in enumerate(final_report['top_features'][:5], 1):\n",
    "    print(f\"   {i}. {feat['Feature']} (KMeans F={feat['KMeans_F_Stat']:.2f}, GMM F={feat['GMM_F_Stat']:.2f})\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\\nðŸŽ¯ RECOMMENDATIONS\\n\")\n",
    "recs = []\n",
    "k_sil, g_sil = final_report['final_metrics']['kmeans']['silhouette'], final_report['final_metrics']['gmm']['silhouette']\n",
    "recs.append(f\"âœ“ {'KMeans' if k_sil > g_sil else 'GMM'} shows higher silhouette score - prefer for {'simpler, spherical' if k_sil > g_sil else 'complex, elliptical'} clusters\")\n",
    "\n",
    "k_sep, g_sep = final_report['cluster_quality']['kmeans']['separation_ratio'], final_report['cluster_quality']['gmm']['separation_ratio']\n",
    "recs.append(f\"âœ“ {'KMeans' if k_sep > g_sep else 'GMM'} has better cluster separation\")\n",
    "\n",
    "k_cv, g_cv = final_report['bootstrap_stability']['kmeans']['silhouette_cv'], final_report['bootstrap_stability']['gmm']['silhouette_cv']\n",
    "recs.append(f\"âœ“ {'KMeans' if k_cv < g_cv else 'GMM'} shows more stable clustering across resamples\")\n",
    "\n",
    "ari = final_report['model_agreement']['adjusted_rand_index']\n",
    "if ari > 0.7:\n",
    "    recs.append(\"âœ“ High agreement between models - robust clustering structure\")\n",
    "elif ari < 0.4:\n",
    "    recs.append(\"âš  Low agreement between models - consider data characteristics or try other algorithms\")\n",
    "\n",
    "poor_k, poor_g = final_report['cluster_quality']['kmeans']['poorly_clustered_pct'], final_report['cluster_quality']['gmm']['poorly_clustered_pct']\n",
    "if max(poor_k, poor_g) > 10:\n",
    "    recs.append(\"âš  High percentage of poorly clustered samples - consider feature engineering or outlier removal\")\n",
    "\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# Save comprehensive report\n",
    "report_path = MODEL_DIR / 'comprehensive_performance_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(f\"\\n\\nðŸ’¾ Full report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Report Visualization ---\n",
    "\n",
    "# Create summary visualization\n",
    "fig = plt.figure(figsize=(12, 7.5))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "models = ['KMeans', 'GMM']\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "# Helper for bar plots\n",
    "def add_bar(ax, data, title, ylabel, ylim_factor=1.2):\n",
    "    ax.bar(models, data, color=colors)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim([0, max(data) * ylim_factor])\n",
    "    for i, v in enumerate(data):\n",
    "        ax.text(i, v + (max(data) * 0.01 if ylim_factor == 1.2 else 0.02), f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Silhouette comparison\n",
    "add_bar(fig.add_subplot(gs[0, 0]),\n",
    "        [final_report['final_metrics']['kmeans']['silhouette'], final_report['final_metrics']['gmm']['silhouette']],\n",
    "        'Silhouette Score Comparison', 'Silhouette Score')\n",
    "\n",
    "# Separation ratio comparison\n",
    "add_bar(fig.add_subplot(gs[0, 1]),\n",
    "        [final_report['cluster_quality']['kmeans']['separation_ratio'], final_report['cluster_quality']['gmm']['separation_ratio']],\n",
    "        'Cluster Separation (higher is better)', 'Separation Ratio')\n",
    "\n",
    "# Stability (CV) comparison\n",
    "add_bar(fig.add_subplot(gs[0, 2]),\n",
    "        [final_report['bootstrap_stability']['kmeans']['silhouette_cv'], final_report['bootstrap_stability']['gmm']['silhouette_cv']],\n",
    "        'Stability (lower is better)', 'Coefficient of Variation', ylim_factor=1.1)\n",
    "\n",
    "# Cluster size distribution\n",
    "ax4 = fig.add_subplot(gs[1, :2])\n",
    "cluster_sizes_k = pd.Series(labels_kmeans).value_counts().sort_index()\n",
    "cluster_sizes_g = pd.Series(labels_gmm).value_counts().sort_index()\n",
    "x_k = np.arange(len(cluster_sizes_k))\n",
    "x_g = np.arange(len(cluster_sizes_g))\n",
    "width = 0.35\n",
    "ax4.bar(x_k - width/2, cluster_sizes_k, width, label='KMeans', color='blue', alpha=0.7)\n",
    "ax4.bar(x_g + width/2, cluster_sizes_g, width, label='GMM', color='orange', alpha=0.7)\n",
    "ax4.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax4.set_ylabel('Sample Count', fontsize=12)\n",
    "ax4.set_title('Cluster Size Distribution', fontsize=14)\n",
    "ax4.legend(fontsize=10)\n",
    "\n",
    "# Model agreement metrics\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "agreement_metrics = ['ARI', 'NMI', 'FMI']\n",
    "agreement_values = [final_report['model_agreement']['adjusted_rand_index'],\n",
    "                    final_report['model_agreement']['normalized_mutual_info'],\n",
    "                    final_report['model_agreement']['fowlkes_mallows_index']]\n",
    "ax5.bar(agreement_metrics, agreement_values, color='green', alpha=0.7)\n",
    "ax5.set_ylabel('Score', fontsize=12)\n",
    "ax5.set_title('Model Agreement Metrics', fontsize=14)\n",
    "ax5.set_ylim([0, 1])\n",
    "for i, v in enumerate(agreement_values):\n",
    "    ax5.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# All metrics comparison (normalized)\n",
    "ax6 = fig.add_subplot(gs[2, :])\n",
    "metrics_names = ['Silhouette', 'CH Index\\n(scaled)', 'DB Index\\n(inverted)', 'Separation\\nRatio', 'Stability\\n(inv CV)']\n",
    "# Normalize metrics to 0-1 range for comparison\n",
    "kmeans_normalized = [\n",
    "    final_report['final_metrics']['kmeans']['silhouette'],\n",
    "    final_report['final_metrics']['kmeans']['calinski_harabasz'] / 10000,  # Scale down\n",
    "    1 / (1 + final_report['final_metrics']['kmeans']['davies_bouldin']),  # Invert (lower is better)\n",
    "    final_report['cluster_quality']['kmeans']['separation_ratio'] / 10,\n",
    "    1 / (1 + final_report['bootstrap_stability']['kmeans']['silhouette_cv'] * 10)  # Invert\n",
    "]\n",
    "gmm_normalized = [\n",
    "    final_report['final_metrics']['gmm']['silhouette'],\n",
    "    final_report['final_metrics']['gmm']['calinski_harabasz'] / 10000,\n",
    "    1 / (1 + final_report['final_metrics']['gmm']['davies_bouldin']),\n",
    "    final_report['cluster_quality']['gmm']['separation_ratio'] / 10,\n",
    "    1 / (1 + final_report['bootstrap_stability']['gmm']['silhouette_cv'] * 10)\n",
    "]\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "ax6.bar(x - width/2, kmeans_normalized, width, label='KMeans', color='blue', alpha=0.7)\n",
    "ax6.bar(x + width/2, gmm_normalized, width, label='GMM', color='orange', alpha=0.7)\n",
    "ax6.set_ylabel('Normalized Score', fontsize=12)\n",
    "ax6.set_title('Comprehensive Metric Comparison (normalized, higher is better for all)', fontsize=14)\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(metrics_names, fontsize=10)\n",
    "ax6.legend(fontsize=10)\n",
    "ax6.set_ylim([0, 1])\n",
    "\n",
    "plt.savefig(MODEL_DIR / 'comprehensive_performance_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Performance summary visualization saved to: {MODEL_DIR / 'comprehensive_performance_summary.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \" * 25 + \"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da98b45",
   "metadata": {},
   "source": [
    "## Summary: Model Evaluation Framework\n",
    "\n",
    "This notebook implements **3 unsupervised learning models** with appropriate evaluation metrics:\n",
    "\n",
    "### ðŸŽ¯ Models Implemented:\n",
    "\n",
    "#### 1. **KMeans Clustering**\n",
    "- **Type:** Clustering (Partitioning)\n",
    "- **Metrics:** Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index, ARI/NMI\n",
    "- **Qualitative:** Cluster interpretability, balance, separation\n",
    "\n",
    "#### 2. **Gaussian Mixture Model (GMM)**\n",
    "- **Type:** Clustering (Probabilistic)\n",
    "- **Metrics:** Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index, AIC/BIC, ARI/NMI\n",
    "- **Qualitative:** Cluster interpretability, probabilistic assignments\n",
    "\n",
    "#### 3. **PCA (Principal Component Analysis)**\n",
    "- **Type:** Dimensionality Reduction (Linear)\n",
    "- **Metrics:** Reconstruction Error (MSE/RMSE), Variance Retained, Compression Ratio\n",
    "- **Qualitative:** Visualization quality, downstream clustering performance\n",
    "\n",
    "### ðŸ“Š Evaluation Framework Applied:\n",
    "\n",
    "| Model Type | Quantitative Metrics | Qualitative / Task-based |\n",
    "|------------|---------------------|-------------------------|\n",
    "| **Clustering** | Silhouette, DB, CH, ARI, NMI | Cluster interpretability, balance |\n",
    "| **Dim. Reduction** | Reconstruction error, Variance retained | Visualization, downstream performance |\n",
    "\n",
    "All models are optimized, evaluated, saved, and compared comprehensively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
