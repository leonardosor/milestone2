{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731d278a",
   "metadata": {},
   "source": [
    "# Advanced Regression Pipeline\n",
    "\n",
    "\n",
    "This notebook builds on the LightGBM pipeline to compare three regression algorithms using data from the local database. It includes data loading, preprocessing, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55439ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: D:\\docs\\MADS\\696-Milestone 2\\supervised\n",
      "LightGBM version: 4.6.0\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "## Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Core Scientific Stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Database\n",
    "import psycopg2\n",
    "\n",
    "## Machine Learning / Preprocessing (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, Pipeline as SkPipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "## Gradient Boosting Libraries\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "## Deep Learning / Tabular\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "## Optimization & Persistence\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Setup model directory (handle notebook environment where __file__ is undefined)\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback: assume notebook is inside src; go up one directory if so\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'supervised.ipynb').exists() or (cwd / 'unsupervised.ipynb').exists():\n",
    "        PROJECT_ROOT = cwd\n",
    "    else:\n",
    "        for parent in cwd.parents:\n",
    "            if (parent / 'requirements.txt').exists() or (parent / 'README.md').exists():\n",
    "                PROJECT_ROOT = parent / 'src'\n",
    "                break\n",
    "        else:\n",
    "            PROJECT_ROOT = cwd  # final fallback\n",
    "\n",
    "MODEL_DIR = (PROJECT_ROOT / '..' / 'supervised').resolve()\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODEL_DIR}\")\n",
    "\n",
    "def save_model(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"Utility to persist models and optional metadata alongside them.\n",
    "    Saves model as joblib plus a companion JSON with metadata/hyperparams.\"\"\"\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    base_name = f\"{name}_{timestamp}\"\n",
    "    model_path = MODEL_DIR / f\"{base_name}.joblib\"\n",
    "    meta_path = MODEL_DIR / f\"{base_name}.json\"\n",
    "    joblib.dump(model, model_path)\n",
    "    meta = {'model_name': name, 'saved_utc': timestamp}\n",
    "    if extra:\n",
    "        meta.update(extra)\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Saved model -> {model_path.name}; metadata -> {meta_path.name}\")\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": os.getenv(\"LOCAL_HOST\"),\n",
    "    \"user\": os.getenv(\"LOCAL_USER\"),\n",
    "    \"password\": os.getenv(\"LOCAL_PW\"),\n",
    "    \"port\": os.getenv(\"LOCAL_PORT\"),\n",
    "    \"dbname\": os.getenv(\"LOCAL_DB\")\n",
    "}\n",
    "\n",
    "# Display versions\n",
    "print('LightGBM version:', lgb.__version__)\n",
    "print('XGBoost version:', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e688798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available - PyTorch will use CPU only\")\n",
    "    print(\"To enable GPU training, install PyTorch with CUDA support:\")\n",
    "    print(\"  conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\")\n",
    "    print(\"  or\")\n",
    "    print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcae04",
   "metadata": {},
   "source": [
    "### Auto Load / Conditional Training\n",
    "If a previously saved optimized model exists in `src/supervised`, the notebook will load the most recent artifact (by timestamp in filename) and skip retraining unless `FORCE_RETRAIN=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe517ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-load status:\n",
      "  xgb_model: pre-existing\n",
      "  rf_model: pre-existing\n",
      "  mlp_model: pre-existing\n",
      "  svr_model: pre-existing\n",
      "  lr_model: pre-existing\n",
      "  tabnet_model: deferred (class not yet defined)\n",
      "FORCE_RETRAIN= True\n"
     ]
    }
   ],
   "source": [
    "# Auto-load previously saved optimized models (XGBoost / RandomForest / SVR / LinearRegression / Polynomial / MLP / TabNet)\n",
    "from pathlib import Path as _Path\n",
    "import json as _json\n",
    "\n",
    "# Initialize placeholders if not already present\n",
    "globals().setdefault('FORCE_RETRAIN', True)\n",
    "\n",
    "# Only set to None if not defined to avoid clobbering models loaded earlier in session\n",
    "if 'xgb_model' not in globals():\n",
    "    xgb_model = None\n",
    "if 'rf_model' not in globals():\n",
    "    rf_model = None\n",
    "if 'mlp_model' not in globals():\n",
    "    mlp_model = None\n",
    "if 'tabnet_model' not in globals():\n",
    "    tabnet_model = None  # Will delay loading until wrapper class defined\n",
    "if 'svr_model' not in globals():\n",
    "    svr_model = None\n",
    "if 'lr_model' not in globals():\n",
    "    lr_model = None\n",
    "\n",
    "MODEL_GLOB_PATTERNS = {\n",
    "    'xgb_model': 'xgboost_opt_*.joblib',\n",
    "    'rf_model': 'random_forest_opt_*.joblib', \n",
    "    'mlp_model': 'mlp_opt_*.pt',  # Changed to .pt for PyTorch models\n",
    "    # 'tabnet_model': DEFERRED - skip here, load after wrapper class defined\n",
    "    'svr_model': 'svr_opt_*.joblib',\n",
    "    'lr_model': 'linear_regression_opt_*.joblib',\n",
    "}\n",
    "\n",
    "loaded_flags = {}\n",
    "for var, pattern in MODEL_GLOB_PATTERNS.items():\n",
    "    if globals().get(var) is not None:\n",
    "        loaded_flags[var] = 'pre-existing'\n",
    "        continue\n",
    "    matches = sorted(MODEL_DIR.glob(pattern))\n",
    "    if not matches:\n",
    "        loaded_flags[var] = 'not found'\n",
    "        continue\n",
    "    latest = matches[-1]\n",
    "    try:\n",
    "        # Special handling for PyTorch models\n",
    "        if pattern.endswith('.pt'):\n",
    "            # Load PyTorch model state\n",
    "            import torch\n",
    "            checkpoint = torch.load(latest, map_location='cpu')\n",
    "            model_params = checkpoint['model_params']\n",
    "            \n",
    "            # Recreate the model (assumes TorchMLPRegressor class is defined)\n",
    "            if 'mlp' in var:\n",
    "                # Model will be recreated when needed - just store params for now\n",
    "                globals()[f\"{var}_params\"] = model_params\n",
    "                globals()[f\"{var}_checkpoint\"] = checkpoint\n",
    "                \n",
    "                # Load metadata JSON for PyTorch models\n",
    "                meta_file = latest.with_suffix('.json')\n",
    "                if meta_file.exists():\n",
    "                    with open(meta_file) as f:\n",
    "                        globals()[f\"{var}_meta\"] = _json.load(f)\n",
    "                \n",
    "                loaded_flags[var] = f\"✓ {latest.name} (PyTorch checkpoint)\"\n",
    "                print(f\"  {var}: {latest} (will be recreated on use)\")\n",
    "                globals()[var] = None  # Will recreate later\n",
    "                continue\n",
    "        \n",
    "        # Standard joblib loading for non-PyTorch models\n",
    "        globals()[var] = joblib.load(latest)\n",
    "        meta_file = latest.with_suffix('.json')\n",
    "        if meta_file.exists():\n",
    "            with open(meta_file) as f:\n",
    "                globals()[f\"{var}_meta\"] = _json.load(f)\n",
    "        loaded_flags[var] = f\"✓ {latest.name}\"\n",
    "        print(f\"  {var}: {latest}\")\n",
    "    except Exception as e:\n",
    "        loaded_flags[var] = f\"failed: {e}\";\n",
    "        globals()[var] = None\n",
    "\n",
    "loaded_flags['tabnet_model'] = 'deferred (class not yet defined)'\n",
    "\n",
    "print(\"Auto-load status:\")\n",
    "for k,v in loaded_flags.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"FORCE_RETRAIN=\", FORCE_RETRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6177eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_26256\\312996059.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden data loaded into DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23038 entries, 0 to 23037\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   school_name             23038 non-null  object \n",
      " 1   school_type             23038 non-null  object \n",
      " 2   teachers_fte            22550 non-null  float64\n",
      " 3   enrollment              22863 non-null  float64\n",
      " 4   grade_eight_enrollment  21613 non-null  float64\n",
      " 5   math_counts             22507 non-null  float64\n",
      " 6   math_high_pct           22507 non-null  float64\n",
      " 7   read_counts             22386 non-null  float64\n",
      " 8   read_high_pct           22386 non-null  float64\n",
      " 9   pct_hhi_150k_200k       23038 non-null  float64\n",
      " 10  pct_hhi_220k_plus       23038 non-null  float64\n",
      " 11  avg_natwalkind          23038 non-null  float64\n",
      " 12  total_10_14             23038 non-null  int64  \n",
      " 13  pct_10_14               23038 non-null  int64  \n",
      " 14  pct_female_10_14        22937 non-null  float64\n",
      " 15  total_pop               23038 non-null  int64  \n",
      " 16  schools_in_zip          23038 non-null  int64  \n",
      " 17  dup_rank                23038 non-null  int64  \n",
      "dtypes: float64(11), int64(5), object(2)\n",
      "memory usage: 3.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load data\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    print(\"Database connection successful\")\n",
    "    sql_query = \"SELECT * FROM dev.base_data;\"\n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    conn.close()\n",
    "    print(\"Golden data loaded into DataFrame:\")\n",
    "    print(df.info())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6bcca",
   "metadata": {},
   "source": [
    "## 3. Data Splitting: Train, Validation, Test\n",
    "Split the dataset into train, validation, and test sets, ensuring proper handling of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1bb52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_name', 'school_type', 'teachers_fte', 'enrollment',\n",
       "       'grade_eight_enrollment', 'math_counts', 'math_high_pct', 'read_counts',\n",
       "       'read_high_pct', 'pct_hhi_150k_200k', 'pct_hhi_220k_plus',\n",
       "       'avg_natwalkind', 'total_10_14', 'pct_10_14', 'pct_female_10_14',\n",
       "       'total_pop', 'schools_in_zip', 'dup_rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13df7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (12378, 15), Validation shape: (4126, 15), Test shape: (4126, 15)\n"
     ]
    }
   ],
   "source": [
    "# Define target and drop missing\n",
    "TARGET = 'math_high_pct' if 'math_high_pct' in df.columns else 'target'\n",
    "data = df.dropna().reset_index(drop=True)\n",
    "data = data.set_index('school_name')\n",
    "\n",
    "# Split features and target\n",
    "feature_cols = [c for c in data.columns if c != TARGET and c != 'dup_rank' and c != 'math_low_pct']\n",
    "X = data[feature_cols]\n",
    "y = data[TARGET]\n",
    "\n",
    "# Train/validation/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "print(f'Train shape: {X_train.shape}, Validation shape: {X_valid.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52296",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Preprocessing Pipeline\n",
    "Identify numeric and categorical features, set up StandardScaler and OneHotEncoder, and build a ColumnTransformer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c071ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['teachers_fte', 'enrollment', 'grade_eight_enrollment', 'math_counts', 'read_counts', 'read_high_pct', 'pct_hhi_150k_200k', 'pct_hhi_220k_plus', 'avg_natwalkind', 'total_10_14', 'pct_10_14', 'pct_female_10_14', 'total_pop', 'schools_in_zip']\n",
      "Categorical features: ['school_type']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_valid_enc = preprocessor.transform(X_valid)\n",
    "X_test_enc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6187ca5",
   "metadata": {},
   "source": [
    "## Model Optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bc1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Optuna-based optimizer for cross-validated hyperparameter tuning\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "def optimize_model_with_optuna(\n",
    "                                model_name: str,\n",
    "                                estimator_builder: Callable[[Dict], object],\n",
    "                                param_space_fn: Callable[[optuna.trial.Trial], Dict],\n",
    "                                X,\n",
    "                                y,\n",
    "                                scoring: str = 'neg_root_mean_squared_error',\n",
    "                                cv: int = 3,\n",
    "                                n_trials: int = 5,\n",
    "                                direction: str = 'minimize',\n",
    "                                random_state: int = 42,\n",
    "                                n_jobs: int = -1,\n",
    "                            ) -> Tuple[optuna.study.Study, Dict]:\n",
    "    \"\"\"Optimize a model's hyperparameters using Optuna and cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name used to label the Optuna study\n",
    "        estimator_builder: Callable that receives a params dict and returns an unfitted estimator\n",
    "        param_space_fn: Callable that maps an Optuna trial to a hyperparameter dictionary\n",
    "        X, y: Training features and targets used for cross-validation\n",
    "        scoring: scikit-learn scoring string guiding optimization\n",
    "        cv: Number of cross-validation folds\n",
    "        n_trials: Number of Optuna trials to run\n",
    "        direction: 'minimize' or 'maximize' depending on the objective\n",
    "        random_state: Seed for the Optuna sampler\n",
    "        n_jobs: Parallelism for cross_val_score\n",
    "\n",
    "    Returns:\n",
    "        The completed Optuna study and the best hyperparameters discovered.\n",
    "    \"\"\"\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(study_name=f\"{model_name}_opt\", direction=direction, sampler=sampler)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        params = param_space_fn(trial)\n",
    "        estimator = estimator_builder(params)\n",
    "        scores = cross_val_score(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "        mean_score = np.mean(scores)\n",
    "        normalized_score = -mean_score if scoring.startswith('neg') else mean_score\n",
    "        return normalized_score if direction == 'minimize' else -normalized_score\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    return study, study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6bcf7",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533b035",
   "metadata": {},
   "source": [
    "## LINEAR MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c2c6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:20:32,815] A new study created in memory with name: SVR_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVR] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274d86e77b9b49f6865f65c7046df2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:20:39,953] Trial 0 finished with value: 19.380419730474674 and parameters: {'kernel': 'poly', 'C': 3.0049873591901566, 'epsilon': 0.08900466011060913, 'degree': 2, 'gamma': 0.1143098387631322}. Best is trial 0 with value: 19.380419730474674.\n",
      "[I 2025-10-12 18:20:46,877] Trial 1 finished with value: 21.00995200439677 and parameters: {'kernel': 'rbf', 'C': 0.5318033256270142, 'epsilon': 0.2924774630404986, 'gamma': 0.3818145165896869}. Best is trial 0 with value: 19.380419730474674.\n",
      "[I 2025-10-12 18:20:52,736] Trial 2 finished with value: 19.135056005123072 and parameters: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}. Best is trial 2 with value: 19.135056005123072.\n",
      "Best SVR params: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:21:06,087] A new study created in memory with name: PolynomialRegression_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model -> svr_opt_20251012T222106Z.joblib; metadata -> svr_opt_20251012T222106Z.json\n",
      "Saved model -> linear_regression_opt_20251012T222106Z.joblib; metadata -> linear_regression_opt_20251012T222106Z.json\n",
      "[PolynomialRegression] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e32bee26784a33a13b268b4317284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:21:10,036] Trial 0 finished with value: 5991224411.412984 and parameters: {'degree': 3, 'interaction_only': False}. Best is trial 0 with value: 5991224411.412984.\n",
      "[I 2025-10-12 18:27:34,980] Trial 1 finished with value: 917618917.5113349 and parameters: {'degree': 4, 'interaction_only': False}. Best is trial 1 with value: 917618917.5113349.\n",
      "[I 2025-10-12 18:27:35,489] Trial 2 finished with value: 224636540636.54575 and parameters: {'degree': 2, 'interaction_only': False}. Best is trial 1 with value: 917618917.5113349.\n",
      "Best Polynomial Regression params: {'degree': 4, 'interaction_only': False}\n",
      "Saved model -> poly_reg_opt_20251012T222954Z.joblib; metadata -> poly_reg_opt_20251012T222954Z.json\n"
     ]
    }
   ],
   "source": [
    "# Linear & Kernel-based Models: SVR + Polynomial Regression (with Optuna tuning + CV)\n",
    "def build_svr_estimator(params: Dict) -> SVR:\n",
    "    base = {'kernel': params.get('kernel', 'rbf')}\n",
    "    # Map params safely\n",
    "    for k in ['C','epsilon','gamma','degree']:\n",
    "        if k in params:\n",
    "            base[k] = params[k]\n",
    "    return SVR(**base)\n",
    "\n",
    "def svr_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf','poly','sigmoid'])\n",
    "    params = {\n",
    "        'kernel': kernel,\n",
    "        'C': trial.suggest_float('C', 0.5, 10, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.05, 0.3),\n",
    "    }\n",
    "    if kernel in ['rbf','sigmoid']:\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 0.5, log=True)\n",
    "    if kernel == 'poly':\n",
    "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 1, log=True)\n",
    "    return params\n",
    "\n",
    "def build_poly_estimator(params: Dict):\n",
    "    degree = params.get('degree', 2)\n",
    "    include_bias = params.get('include_bias', False)\n",
    "    interaction_only = params.get('interaction_only', False)\n",
    "    return SkPipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias, interaction_only=interaction_only)),\n",
    "        ('lr', LinearRegression())\n",
    "    ])\n",
    "\n",
    "def poly_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'degree': trial.suggest_int('degree', 2, 5),\n",
    "        'include_bias': False,\n",
    "        'interaction_only': trial.suggest_categorical('interaction_only', [False, True])\n",
    "    }\n",
    "\n",
    "# Run / reuse SVR optimization\n",
    "if svr_model is None or FORCE_RETRAIN:\n",
    "    print('[SVR] Starting Optuna optimization...')\n",
    "    svr_study, svr_best_params = optimize_model_with_optuna(\n",
    "        model_name='SVR',\n",
    "        estimator_builder=build_svr_estimator,\n",
    "        param_space_fn=svr_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best SVR params:', svr_best_params)\n",
    "    svr_model = build_svr_estimator(svr_best_params)\n",
    "    svr_model.fit(X_train_enc, y_train)\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "    save_model(svr_model, 'svr_opt', {'best_params': svr_best_params})\n",
    "else:\n",
    "    print('[SVR] Using preloaded optimized model; generating predictions.')\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "\n",
    "# Baseline Linear Regression (also optionally re-optimized via polynomial)\n",
    "if lr_model is None or FORCE_RETRAIN:\n",
    "    # Keep a simple baseline linear regression for reference\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_enc, y_train)\n",
    "    save_model(lr_model, 'linear_regression_opt', {'params': lr_model.get_params(), 'baseline': True})\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "elif lr_model is not None:\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "\n",
    "# Polynomial Regression optimization\n",
    "if poly_model is None or FORCE_RETRAIN:\n",
    "    print('[PolynomialRegression] Starting Optuna optimization...')\n",
    "    poly_study, poly_best_params = optimize_model_with_optuna(\n",
    "        model_name='PolynomialRegression',\n",
    "        estimator_builder=build_poly_estimator,\n",
    "        param_space_fn=poly_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Polynomial Regression params:', poly_best_params)\n",
    "    poly_model = build_poly_estimator(poly_best_params)\n",
    "    poly_model.fit(X_train_enc, y_train)\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)\n",
    "    save_model(poly_model, 'poly_reg_opt', {'best_params': poly_best_params})\n",
    "else:\n",
    "    print('[PolynomialRegression] Using preloaded optimized model; generating predictions.')\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf134e",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58546626",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745b1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:29:54,625] A new study created in memory with name: TorchMLPRegressor_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Starting optimization on CUDA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc720046bd78445aa2ceeef03fa2a5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:30:17,084] Trial 0 finished with value: 17.610844195036623 and parameters: {'hl1': 128, 'hl2': 192, 'learning_rate_init': 0.0029106359131330704, 'alpha': 0.0002481040974867811, 'batch_size': 64, 'max_iter': 600, 'n_iter_no_change': 20, 'activation': 'relu'}. Best is trial 0 with value: 17.610844195036623.\n",
      "[I 2025-10-12 18:30:49,130] Trial 1 finished with value: 17.574408454538453 and parameters: {'hl1': 256, 'hl2': 160, 'learning_rate_init': 0.00026587543983272726, 'alpha': 5.337032762603957e-06, 'batch_size': 256, 'max_iter': 375, 'n_iter_no_change': 10, 'activation': 'relu'}. Best is trial 1 with value: 17.574408454538453.\n",
      "[I 2025-10-12 18:30:49,130] Trial 1 finished with value: 17.574408454538453 and parameters: {'hl1': 256, 'hl2': 160, 'learning_rate_init': 0.00026587543983272726, 'alpha': 5.337032762603957e-06, 'batch_size': 256, 'max_iter': 375, 'n_iter_no_change': 10, 'activation': 'relu'}. Best is trial 1 with value: 17.574408454538453.\n",
      "[I 2025-10-12 18:31:14,497] Trial 2 finished with value: 17.582691842114127 and parameters: {'hl1': 128, 'hl2': 96, 'learning_rate_init': 0.000816845589476017, 'alpha': 0.0013826232179369874, 'batch_size': 256, 'max_iter': 150, 'n_iter_no_change': 20, 'activation': 'relu'}. Best is trial 1 with value: 17.574408454538453.\n",
      "[I 2025-10-12 18:31:14,497] Trial 2 finished with value: 17.582691842114127 and parameters: {'hl1': 128, 'hl2': 96, 'learning_rate_init': 0.000816845589476017, 'alpha': 0.0013826232179369874, 'batch_size': 256, 'max_iter': 150, 'n_iter_no_change': 20, 'activation': 'relu'}. Best is trial 1 with value: 17.574408454538453.\n",
      "[I 2025-10-12 18:31:30,362] Trial 3 finished with value: 17.52160237515248 and parameters: {'hl1': 256, 'hl2': 192, 'learning_rate_init': 0.004138040112561018, 'alpha': 1.6536937182824424e-05, 'batch_size': 128, 'max_iter': 150, 'n_iter_no_change': 15, 'activation': 'tanh'}. Best is trial 3 with value: 17.52160237515248.\n",
      "[I 2025-10-12 18:31:30,362] Trial 3 finished with value: 17.52160237515248 and parameters: {'hl1': 256, 'hl2': 192, 'learning_rate_init': 0.004138040112561018, 'alpha': 1.6536937182824424e-05, 'batch_size': 128, 'max_iter': 150, 'n_iter_no_change': 15, 'activation': 'tanh'}. Best is trial 3 with value: 17.52160237515248.\n",
      "[I 2025-10-12 18:32:07,496] Trial 4 finished with value: 17.594799501360573 and parameters: {'hl1': 96, 'hl2': 128, 'learning_rate_init': 0.0004201672054372534, 'alpha': 0.00012030178871154674, 'batch_size': 256, 'max_iter': 525, 'n_iter_no_change': 25, 'activation': 'relu'}. Best is trial 3 with value: 17.52160237515248.\n",
      "[MLP] Best params: {'hl1': 256, 'hl2': 192, 'learning_rate_init': 0.004138040112561018, 'alpha': 1.6536937182824424e-05, 'batch_size': 128, 'max_iter': 150, 'n_iter_no_change': 15, 'activation': 'tanh'}\n",
      "[I 2025-10-12 18:32:07,496] Trial 4 finished with value: 17.594799501360573 and parameters: {'hl1': 96, 'hl2': 128, 'learning_rate_init': 0.0004201672054372534, 'alpha': 0.00012030178871154674, 'batch_size': 256, 'max_iter': 525, 'n_iter_no_change': 25, 'activation': 'relu'}. Best is trial 3 with value: 17.52160237515248.\n",
      "[MLP] Best params: {'hl1': 256, 'hl2': 192, 'learning_rate_init': 0.004138040112561018, 'alpha': 1.6536937182824424e-05, 'batch_size': 128, 'max_iter': 150, 'n_iter_no_change': 15, 'activation': 'tanh'}\n",
      "Saved model -> mlp_opt_20251012T223220Z.joblib; metadata -> mlp_opt_20251012T223220Z.json\n",
      "Saved model -> mlp_opt_20251012T223220Z.joblib; metadata -> mlp_opt_20251012T223220Z.json\n"
     ]
    }
   ],
   "source": [
    "# Torch MLP Regressor with sklearn API\n",
    "\n",
    "def _ensure_dense_np(X):\n",
    "    return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "class TorchMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"PyTorch MLP for regression with sklearn API and CUDA support.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=(128, 64), activation='relu', \n",
    "                 learning_rate_init=1e-3, alpha=0.0, batch_size=128, max_iter=100,\n",
    "                 early_stopping=True, validation_fraction=0.1, n_iter_no_change=10,\n",
    "                 random_state=42, verbose=False, device=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "    def _build_network(self, in_features):\n",
    "        layers = []\n",
    "        act_fn = nn.ReLU if self.activation == 'relu' else nn.Tanh\n",
    "        prev = in_features\n",
    "        for h in self.hidden_layer_sizes:\n",
    "            layers.extend([nn.Linear(prev, h), act_fn()])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Setup\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        \n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        y_np = (y.values if hasattr(y, 'values') else y).astype('float32').reshape(-1, 1)\n",
    "        \n",
    "        # Train/val split\n",
    "        if self.early_stopping and 0 < self.validation_fraction < 0.5:\n",
    "            val_size = max(1, int(len(X_np) * self.validation_fraction))\n",
    "            idx = np.random.RandomState(self.random_state).permutation(len(X_np))\n",
    "            X_train, X_val = X_np[idx[val_size:]], X_np[idx[:val_size]]\n",
    "            y_train, y_val = y_np[idx[val_size:]], y_np[idx[:val_size]]\n",
    "        else:\n",
    "            X_train, y_train, X_val, y_val = X_np, y_np, None, None\n",
    "        \n",
    "        # Device and model\n",
    "        device = self.device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._device_ = device\n",
    "        self.model_ = self._build_network(X_np.shape[1]).to(device)\n",
    "        self.optimizer_ = torch.optim.Adam(self.model_.parameters(), \n",
    "                                          lr=self.learning_rate_init, \n",
    "                                          weight_decay=self.alpha)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        best_val, best_state, patience = math.inf, None, 0\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            self.model_.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                self.optimizer_.zero_grad()\n",
    "                criterion(self.model_(xb), yb).backward()\n",
    "                self.optimizer_.step()\n",
    "            \n",
    "            # Validation\n",
    "            if X_val is not None:\n",
    "                self.model_.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = criterion(\n",
    "                        self.model_(torch.from_numpy(X_val).to(device)),\n",
    "                        torch.from_numpy(y_val).to(device)\n",
    "                    ).item()\n",
    "                \n",
    "                if val_loss < best_val - 1e-9:\n",
    "                    best_val, patience = val_loss, 0\n",
    "                    best_state = {k: v.cpu().clone() for k, v in self.model_.state_dict().items()}\n",
    "                else:\n",
    "                    patience += 1\n",
    "                \n",
    "                if patience >= self.n_iter_no_change:\n",
    "                    break\n",
    "        \n",
    "        if best_state:\n",
    "            self.model_.load_state_dict(best_state)\n",
    "        self.n_features_in_ = X_np.shape[1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, ['model_'])\n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(torch.from_numpy(X_np).to(self._device_))\n",
    "        return preds.cpu().numpy().ravel()\n",
    "\n",
    "# Helper functions\n",
    "def build_mlp_estimator(params):\n",
    "    params = params.copy()\n",
    "    params.pop('hl1', None)\n",
    "    params.pop('hl2', None)\n",
    "    return TorchMLPRegressor(**params)\n",
    "\n",
    "def mlp_param_space(trial):\n",
    "    return {\n",
    "        'hidden_layer_sizes': tuple(sorted([\n",
    "            trial.suggest_int('hl1', 64, 256, step=32),\n",
    "            trial.suggest_int('hl2', 32, 192, step=32)\n",
    "        ], reverse=True)),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        'max_iter': trial.suggest_int('max_iter', 150, 600, step=75),\n",
    "        'n_iter_no_change': trial.suggest_int('n_iter_no_change', 5, 25, step=5),\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.15,\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'random_state': 42,\n",
    "        'verbose': False,\n",
    "    }\n",
    "\n",
    "# Training\n",
    "if mlp_model is None or FORCE_RETRAIN:\n",
    "    print(f\"[MLP] Starting optimization on {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    mlp_study, mlp_best_params = optimize_model_with_optuna(\n",
    "        model_name='TorchMLPRegressor',\n",
    "        estimator_builder=build_mlp_estimator,\n",
    "        param_space_fn=mlp_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1,\n",
    "    )\n",
    "    print(f'[MLP] Best params: {mlp_best_params}')\n",
    "    mlp_model = build_mlp_estimator(mlp_best_params)\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt', {'best_params': mlp_best_params, 'framework': 'torch'})\n",
    "else:\n",
    "    print(\"[MLP] Refitting preloaded model\")\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt_refit', {'refit': True, 'framework': 'torch'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7801",
   "metadata": {},
   "source": [
    "## TABNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3270f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:32:20,075] A new study created in memory with name: TabNetRegressor_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet] Training new model with Optuna optimization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fb8fb56f7b44ad91c283c544ef9c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:32:58,886] Trial 0 finished with value: 17.90373767061614 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 3, 'gamma': 1.692940916619948, 'lambda_sparse': 6.358358856676247e-05, 'lr': 0.00834110643236209}. Best is trial 0 with value: 17.90373767061614.\n",
      "[I 2025-10-12 18:34:24,286] Trial 1 finished with value: 17.941336017480243 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 4, 'gamma': 1.4198051453057903, 'lambda_sparse': 1.9762189340280066e-05, 'lr': 0.002392752876558064}. Best is trial 0 with value: 17.90373767061614.\n",
      "[I 2025-10-12 18:34:24,286] Trial 1 finished with value: 17.941336017480243 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 4, 'gamma': 1.4198051453057903, 'lambda_sparse': 1.9762189340280066e-05, 'lr': 0.002392752876558064}. Best is trial 0 with value: 17.90373767061614.\n",
      "[I 2025-10-12 18:35:34,386] Trial 2 finished with value: 24.2295931036714 and parameters: {'n_d': 8, 'n_a': 24, 'n_steps': 3, 'gamma': 1.4113875507308893, 'lambda_sparse': 5.987474910461405e-05, 'lr': 0.001149299930022141}. Best is trial 0 with value: 17.90373767061614.\n",
      "[I 2025-10-12 18:35:34,386] Trial 2 finished with value: 24.2295931036714 and parameters: {'n_d': 8, 'n_a': 24, 'n_steps': 3, 'gamma': 1.4113875507308893, 'lambda_sparse': 5.987474910461405e-05, 'lr': 0.001149299930022141}. Best is trial 0 with value: 17.90373767061614.\n",
      "[I 2025-10-12 18:37:00,734] Trial 3 finished with value: 17.710129571202412 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.078137691205107, 'lambda_sparse': 0.00011290133559092664, 'lr': 0.0037381058681917956}. Best is trial 3 with value: 17.710129571202412.\n",
      "[I 2025-10-12 18:37:00,734] Trial 3 finished with value: 17.710129571202412 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.078137691205107, 'lambda_sparse': 0.00011290133559092664, 'lr': 0.0037381058681917956}. Best is trial 3 with value: 17.710129571202412.\n",
      "[I 2025-10-12 18:38:19,324] Trial 4 finished with value: 18.158084391546044 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 4, 'gamma': 1.4160544169422487, 'lambda_sparse': 4.366473592979636e-05, 'lr': 0.0017398074711291722}. Best is trial 3 with value: 17.710129571202412.\n",
      "[I 2025-10-12 18:38:19,324] Trial 4 finished with value: 18.158084391546044 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 4, 'gamma': 1.4160544169422487, 'lambda_sparse': 4.366473592979636e-05, 'lr': 0.0017398074711291722}. Best is trial 3 with value: 17.710129571202412.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2657.33423| val_0_rmse: 51.80996|  0:00:01s\n",
      "epoch 1  | loss: 2592.91721| val_0_rmse: 51.28325|  0:00:02s\n",
      "epoch 1  | loss: 2592.91721| val_0_rmse: 51.28325|  0:00:02s\n",
      "epoch 2  | loss: 2534.17283| val_0_rmse: 50.71539|  0:00:03s\n",
      "epoch 2  | loss: 2534.17283| val_0_rmse: 50.71539|  0:00:03s\n",
      "epoch 3  | loss: 2475.51138| val_0_rmse: 50.08335|  0:00:04s\n",
      "epoch 3  | loss: 2475.51138| val_0_rmse: 50.08335|  0:00:04s\n",
      "epoch 4  | loss: 2415.30674| val_0_rmse: 49.35671|  0:00:05s\n",
      "epoch 4  | loss: 2415.30674| val_0_rmse: 49.35671|  0:00:05s\n",
      "epoch 5  | loss: 2335.52185| val_0_rmse: 48.48561|  0:00:06s\n",
      "epoch 5  | loss: 2335.52185| val_0_rmse: 48.48561|  0:00:06s\n",
      "epoch 6  | loss: 2239.76829| val_0_rmse: 47.28928|  0:00:07s\n",
      "epoch 6  | loss: 2239.76829| val_0_rmse: 47.28928|  0:00:07s\n",
      "epoch 7  | loss: 2138.7392| val_0_rmse: 46.05031|  0:00:08s\n",
      "epoch 7  | loss: 2138.7392| val_0_rmse: 46.05031|  0:00:08s\n",
      "epoch 8  | loss: 2013.08971| val_0_rmse: 44.5799 |  0:00:10s\n",
      "epoch 8  | loss: 2013.08971| val_0_rmse: 44.5799 |  0:00:10s\n",
      "epoch 9  | loss: 1891.52769| val_0_rmse: 42.91088|  0:00:11s\n",
      "epoch 9  | loss: 1891.52769| val_0_rmse: 42.91088|  0:00:11s\n",
      "epoch 10 | loss: 1752.52275| val_0_rmse: 41.21901|  0:00:12s\n",
      "epoch 10 | loss: 1752.52275| val_0_rmse: 41.21901|  0:00:12s\n",
      "epoch 11 | loss: 1617.02031| val_0_rmse: 39.33984|  0:00:13s\n",
      "epoch 11 | loss: 1617.02031| val_0_rmse: 39.33984|  0:00:13s\n",
      "epoch 12 | loss: 1477.56021| val_0_rmse: 37.21009|  0:00:14s\n",
      "epoch 12 | loss: 1477.56021| val_0_rmse: 37.21009|  0:00:14s\n",
      "epoch 13 | loss: 1326.72227| val_0_rmse: 34.99816|  0:00:15s\n",
      "epoch 13 | loss: 1326.72227| val_0_rmse: 34.99816|  0:00:15s\n",
      "epoch 14 | loss: 1187.91538| val_0_rmse: 33.27443|  0:00:16s\n",
      "epoch 14 | loss: 1187.91538| val_0_rmse: 33.27443|  0:00:16s\n",
      "epoch 15 | loss: 1040.41159| val_0_rmse: 30.94183|  0:00:17s\n",
      "epoch 15 | loss: 1040.41159| val_0_rmse: 30.94183|  0:00:17s\n",
      "epoch 16 | loss: 916.53849| val_0_rmse: 28.62366|  0:00:18s\n",
      "epoch 16 | loss: 916.53849| val_0_rmse: 28.62366|  0:00:18s\n",
      "epoch 17 | loss: 793.59174| val_0_rmse: 26.72539|  0:00:20s\n",
      "epoch 17 | loss: 793.59174| val_0_rmse: 26.72539|  0:00:20s\n",
      "epoch 18 | loss: 685.03822| val_0_rmse: 24.65454|  0:00:21s\n",
      "epoch 18 | loss: 685.03822| val_0_rmse: 24.65454|  0:00:21s\n",
      "epoch 19 | loss: 580.58308| val_0_rmse: 22.88182|  0:00:22s\n",
      "epoch 19 | loss: 580.58308| val_0_rmse: 22.88182|  0:00:22s\n",
      "epoch 20 | loss: 503.83371| val_0_rmse: 21.28425|  0:00:23s\n",
      "epoch 20 | loss: 503.83371| val_0_rmse: 21.28425|  0:00:23s\n",
      "epoch 21 | loss: 443.3019| val_0_rmse: 20.1442 |  0:00:24s\n",
      "epoch 21 | loss: 443.3019| val_0_rmse: 20.1442 |  0:00:24s\n",
      "epoch 22 | loss: 401.51572| val_0_rmse: 19.28058|  0:00:25s\n",
      "epoch 22 | loss: 401.51572| val_0_rmse: 19.28058|  0:00:25s\n",
      "epoch 23 | loss: 375.79466| val_0_rmse: 18.96473|  0:00:26s\n",
      "epoch 23 | loss: 375.79466| val_0_rmse: 18.96473|  0:00:26s\n",
      "epoch 24 | loss: 362.16681| val_0_rmse: 18.83395|  0:00:27s\n",
      "epoch 24 | loss: 362.16681| val_0_rmse: 18.83395|  0:00:27s\n",
      "epoch 25 | loss: 357.40857| val_0_rmse: 18.72362|  0:00:28s\n",
      "epoch 25 | loss: 357.40857| val_0_rmse: 18.72362|  0:00:28s\n",
      "epoch 26 | loss: 348.84167| val_0_rmse: 18.59288|  0:00:29s\n",
      "epoch 26 | loss: 348.84167| val_0_rmse: 18.59288|  0:00:29s\n",
      "epoch 27 | loss: 344.47919| val_0_rmse: 18.47289|  0:00:31s\n",
      "epoch 27 | loss: 344.47919| val_0_rmse: 18.47289|  0:00:31s\n",
      "epoch 28 | loss: 339.42965| val_0_rmse: 18.08122|  0:00:32s\n",
      "epoch 28 | loss: 339.42965| val_0_rmse: 18.08122|  0:00:32s\n",
      "epoch 29 | loss: 338.84245| val_0_rmse: 18.16214|  0:00:33s\n",
      "epoch 29 | loss: 338.84245| val_0_rmse: 18.16214|  0:00:33s\n",
      "epoch 30 | loss: 338.29456| val_0_rmse: 17.92056|  0:00:34s\n",
      "epoch 30 | loss: 338.29456| val_0_rmse: 17.92056|  0:00:34s\n",
      "epoch 31 | loss: 330.55938| val_0_rmse: 17.79114|  0:00:35s\n",
      "epoch 31 | loss: 330.55938| val_0_rmse: 17.79114|  0:00:35s\n",
      "epoch 32 | loss: 331.20724| val_0_rmse: 17.69611|  0:00:36s\n",
      "epoch 32 | loss: 331.20724| val_0_rmse: 17.69611|  0:00:36s\n",
      "epoch 33 | loss: 331.12915| val_0_rmse: 17.8649 |  0:00:37s\n",
      "epoch 33 | loss: 331.12915| val_0_rmse: 17.8649 |  0:00:37s\n",
      "epoch 34 | loss: 329.6421| val_0_rmse: 17.71808|  0:00:38s\n",
      "epoch 34 | loss: 329.6421| val_0_rmse: 17.71808|  0:00:38s\n",
      "epoch 35 | loss: 324.64593| val_0_rmse: 17.71347|  0:00:39s\n",
      "epoch 35 | loss: 324.64593| val_0_rmse: 17.71347|  0:00:39s\n",
      "epoch 36 | loss: 326.81667| val_0_rmse: 17.73099|  0:00:40s\n",
      "epoch 36 | loss: 326.81667| val_0_rmse: 17.73099|  0:00:40s\n",
      "epoch 37 | loss: 325.25594| val_0_rmse: 17.79525|  0:00:41s\n",
      "epoch 37 | loss: 325.25594| val_0_rmse: 17.79525|  0:00:41s\n",
      "epoch 38 | loss: 323.16835| val_0_rmse: 17.68805|  0:00:42s\n",
      "epoch 38 | loss: 323.16835| val_0_rmse: 17.68805|  0:00:42s\n",
      "epoch 39 | loss: 326.23722| val_0_rmse: 17.57903|  0:00:43s\n",
      "epoch 39 | loss: 326.23722| val_0_rmse: 17.57903|  0:00:43s\n",
      "epoch 40 | loss: 324.31311| val_0_rmse: 17.58285|  0:00:44s\n",
      "epoch 40 | loss: 324.31311| val_0_rmse: 17.58285|  0:00:44s\n",
      "epoch 41 | loss: 322.19089| val_0_rmse: 17.60769|  0:00:46s\n",
      "epoch 41 | loss: 322.19089| val_0_rmse: 17.60769|  0:00:46s\n",
      "epoch 42 | loss: 321.68297| val_0_rmse: 17.57861|  0:00:47s\n",
      "epoch 42 | loss: 321.68297| val_0_rmse: 17.57861|  0:00:47s\n",
      "epoch 43 | loss: 318.96445| val_0_rmse: 17.44603|  0:00:48s\n",
      "epoch 43 | loss: 318.96445| val_0_rmse: 17.44603|  0:00:48s\n",
      "epoch 44 | loss: 316.33002| val_0_rmse: 17.55874|  0:00:49s\n",
      "epoch 44 | loss: 316.33002| val_0_rmse: 17.55874|  0:00:49s\n",
      "epoch 45 | loss: 315.83806| val_0_rmse: 17.52211|  0:00:50s\n",
      "epoch 45 | loss: 315.83806| val_0_rmse: 17.52211|  0:00:50s\n",
      "epoch 46 | loss: 313.68499| val_0_rmse: 17.49851|  0:00:51s\n",
      "epoch 46 | loss: 313.68499| val_0_rmse: 17.49851|  0:00:51s\n",
      "epoch 47 | loss: 315.0899| val_0_rmse: 17.5216 |  0:00:52s\n",
      "epoch 47 | loss: 315.0899| val_0_rmse: 17.5216 |  0:00:52s\n",
      "epoch 48 | loss: 313.97276| val_0_rmse: 17.49219|  0:00:53s\n",
      "epoch 48 | loss: 313.97276| val_0_rmse: 17.49219|  0:00:53s\n",
      "epoch 49 | loss: 315.50003| val_0_rmse: 17.52693|  0:00:54s\n",
      "epoch 49 | loss: 315.50003| val_0_rmse: 17.52693|  0:00:54s\n",
      "epoch 50 | loss: 312.72342| val_0_rmse: 17.54757|  0:00:55s\n",
      "epoch 50 | loss: 312.72342| val_0_rmse: 17.54757|  0:00:55s\n",
      "epoch 51 | loss: 315.02096| val_0_rmse: 17.46435|  0:00:56s\n",
      "epoch 51 | loss: 315.02096| val_0_rmse: 17.46435|  0:00:56s\n",
      "epoch 52 | loss: 314.56346| val_0_rmse: 17.52114|  1:26:13s\n",
      "epoch 52 | loss: 314.56346| val_0_rmse: 17.52114|  1:26:13s\n",
      "epoch 53 | loss: 312.20156| val_0_rmse: 17.49122|  1:26:15s\n",
      "epoch 53 | loss: 312.20156| val_0_rmse: 17.49122|  1:26:15s\n",
      "epoch 54 | loss: 313.07559| val_0_rmse: 17.42541|  1:26:17s\n",
      "epoch 54 | loss: 313.07559| val_0_rmse: 17.42541|  1:26:17s\n",
      "epoch 55 | loss: 310.42073| val_0_rmse: 17.38298|  1:26:19s\n",
      "epoch 55 | loss: 310.42073| val_0_rmse: 17.38298|  1:26:19s\n",
      "epoch 56 | loss: 311.45979| val_0_rmse: 17.3607 |  1:26:22s\n",
      "epoch 56 | loss: 311.45979| val_0_rmse: 17.3607 |  1:26:22s\n",
      "epoch 57 | loss: 309.60683| val_0_rmse: 17.46156|  1:26:24s\n",
      "epoch 57 | loss: 309.60683| val_0_rmse: 17.46156|  1:26:24s\n",
      "epoch 58 | loss: 309.02338| val_0_rmse: 17.41244|  1:26:26s\n",
      "epoch 58 | loss: 309.02338| val_0_rmse: 17.41244|  1:26:26s\n",
      "epoch 59 | loss: 304.964 | val_0_rmse: 17.33978|  1:26:28s\n",
      "epoch 59 | loss: 304.964 | val_0_rmse: 17.33978|  1:26:28s\n",
      "epoch 60 | loss: 308.40396| val_0_rmse: 17.37358|  1:26:30s\n",
      "epoch 60 | loss: 308.40396| val_0_rmse: 17.37358|  1:26:30s\n",
      "epoch 61 | loss: 306.6697| val_0_rmse: 17.38806|  1:26:33s\n",
      "epoch 61 | loss: 306.6697| val_0_rmse: 17.38806|  1:26:33s\n",
      "epoch 62 | loss: 305.3351| val_0_rmse: 17.40715|  1:26:34s\n",
      "epoch 62 | loss: 305.3351| val_0_rmse: 17.40715|  1:26:34s\n",
      "epoch 63 | loss: 304.37751| val_0_rmse: 17.4098 |  1:26:36s\n",
      "epoch 63 | loss: 304.37751| val_0_rmse: 17.4098 |  1:26:36s\n",
      "epoch 64 | loss: 306.73715| val_0_rmse: 17.43936|  1:26:37s\n",
      "epoch 64 | loss: 306.73715| val_0_rmse: 17.43936|  1:26:37s\n",
      "epoch 65 | loss: 305.36127| val_0_rmse: 17.409  |  1:26:38s\n",
      "epoch 65 | loss: 305.36127| val_0_rmse: 17.409  |  1:26:38s\n",
      "epoch 66 | loss: 302.92823| val_0_rmse: 17.37473|  1:26:40s\n",
      "epoch 66 | loss: 302.92823| val_0_rmse: 17.37473|  1:26:40s\n",
      "epoch 67 | loss: 301.75186| val_0_rmse: 17.41999|  1:26:41s\n",
      "epoch 67 | loss: 301.75186| val_0_rmse: 17.41999|  1:26:41s\n",
      "epoch 68 | loss: 303.53636| val_0_rmse: 17.34458|  1:26:42s\n",
      "epoch 68 | loss: 303.53636| val_0_rmse: 17.34458|  1:26:42s\n",
      "epoch 69 | loss: 303.89106| val_0_rmse: 17.30306|  1:26:44s\n",
      "epoch 69 | loss: 303.89106| val_0_rmse: 17.30306|  1:26:44s\n",
      "epoch 70 | loss: 300.33104| val_0_rmse: 17.2868 |  1:26:45s\n",
      "epoch 70 | loss: 300.33104| val_0_rmse: 17.2868 |  1:26:45s\n",
      "epoch 71 | loss: 303.64534| val_0_rmse: 17.31378|  1:26:46s\n",
      "epoch 71 | loss: 303.64534| val_0_rmse: 17.31378|  1:26:46s\n",
      "epoch 72 | loss: 304.01674| val_0_rmse: 17.36367|  1:26:47s\n",
      "epoch 72 | loss: 304.01674| val_0_rmse: 17.36367|  1:26:47s\n",
      "epoch 73 | loss: 301.03182| val_0_rmse: 17.47725|  1:26:48s\n",
      "epoch 73 | loss: 301.03182| val_0_rmse: 17.47725|  1:26:48s\n",
      "epoch 74 | loss: 302.64343| val_0_rmse: 17.27058|  1:26:50s\n",
      "epoch 74 | loss: 302.64343| val_0_rmse: 17.27058|  1:26:50s\n",
      "epoch 75 | loss: 300.93557| val_0_rmse: 17.35048|  1:26:51s\n",
      "epoch 75 | loss: 300.93557| val_0_rmse: 17.35048|  1:26:51s\n",
      "epoch 76 | loss: 298.63701| val_0_rmse: 17.31279|  1:26:52s\n",
      "epoch 76 | loss: 298.63701| val_0_rmse: 17.31279|  1:26:52s\n",
      "epoch 77 | loss: 299.41367| val_0_rmse: 17.33667|  1:26:53s\n",
      "epoch 77 | loss: 299.41367| val_0_rmse: 17.33667|  1:26:53s\n",
      "epoch 78 | loss: 301.34526| val_0_rmse: 17.36688|  1:26:54s\n",
      "epoch 78 | loss: 301.34526| val_0_rmse: 17.36688|  1:26:54s\n",
      "epoch 79 | loss: 300.87958| val_0_rmse: 17.38064|  1:26:56s\n",
      "epoch 79 | loss: 300.87958| val_0_rmse: 17.38064|  1:26:56s\n",
      "epoch 80 | loss: 300.30761| val_0_rmse: 17.31435|  1:26:57s\n",
      "epoch 80 | loss: 300.30761| val_0_rmse: 17.31435|  1:26:57s\n",
      "epoch 81 | loss: 300.09799| val_0_rmse: 17.24475|  1:26:58s\n",
      "epoch 81 | loss: 300.09799| val_0_rmse: 17.24475|  1:26:58s\n",
      "epoch 82 | loss: 297.93201| val_0_rmse: 17.31907|  1:26:59s\n",
      "epoch 82 | loss: 297.93201| val_0_rmse: 17.31907|  1:26:59s\n",
      "epoch 83 | loss: 295.07803| val_0_rmse: 17.38153|  1:27:00s\n",
      "epoch 83 | loss: 295.07803| val_0_rmse: 17.38153|  1:27:00s\n",
      "epoch 84 | loss: 295.4886| val_0_rmse: 17.32817|  1:27:01s\n",
      "epoch 84 | loss: 295.4886| val_0_rmse: 17.32817|  1:27:01s\n",
      "epoch 85 | loss: 298.22061| val_0_rmse: 17.29434|  1:27:02s\n",
      "epoch 85 | loss: 298.22061| val_0_rmse: 17.29434|  1:27:02s\n",
      "epoch 86 | loss: 296.35681| val_0_rmse: 17.33593|  1:27:03s\n",
      "epoch 86 | loss: 296.35681| val_0_rmse: 17.33593|  1:27:03s\n",
      "epoch 87 | loss: 293.80657| val_0_rmse: 17.23873|  1:27:05s\n",
      "epoch 87 | loss: 293.80657| val_0_rmse: 17.23873|  1:27:05s\n",
      "epoch 88 | loss: 295.77282| val_0_rmse: 17.20867|  1:27:06s\n",
      "epoch 88 | loss: 295.77282| val_0_rmse: 17.20867|  1:27:06s\n",
      "epoch 89 | loss: 295.50965| val_0_rmse: 17.31817|  1:27:07s\n",
      "epoch 89 | loss: 295.50965| val_0_rmse: 17.31817|  1:27:07s\n",
      "epoch 90 | loss: 294.94351| val_0_rmse: 17.30531|  1:27:08s\n",
      "epoch 90 | loss: 294.94351| val_0_rmse: 17.30531|  1:27:08s\n",
      "epoch 91 | loss: 296.94968| val_0_rmse: 17.27825|  1:27:09s\n",
      "epoch 91 | loss: 296.94968| val_0_rmse: 17.27825|  1:27:09s\n",
      "epoch 92 | loss: 293.68462| val_0_rmse: 17.22138|  1:27:10s\n",
      "epoch 92 | loss: 293.68462| val_0_rmse: 17.22138|  1:27:10s\n",
      "epoch 93 | loss: 292.12932| val_0_rmse: 17.20963|  1:27:11s\n",
      "epoch 93 | loss: 292.12932| val_0_rmse: 17.20963|  1:27:11s\n",
      "epoch 94 | loss: 296.45871| val_0_rmse: 17.22661|  1:27:12s\n",
      "epoch 94 | loss: 296.45871| val_0_rmse: 17.22661|  1:27:12s\n",
      "epoch 95 | loss: 296.04199| val_0_rmse: 17.11405|  1:27:14s\n",
      "epoch 95 | loss: 296.04199| val_0_rmse: 17.11405|  1:27:14s\n",
      "epoch 96 | loss: 292.79924| val_0_rmse: 17.11884|  1:27:15s\n",
      "epoch 96 | loss: 292.79924| val_0_rmse: 17.11884|  1:27:15s\n",
      "epoch 97 | loss: 296.29686| val_0_rmse: 17.26173|  1:27:16s\n",
      "epoch 97 | loss: 296.29686| val_0_rmse: 17.26173|  1:27:16s\n",
      "epoch 98 | loss: 292.08246| val_0_rmse: 17.27552|  1:27:17s\n",
      "epoch 98 | loss: 292.08246| val_0_rmse: 17.27552|  1:27:17s\n",
      "epoch 99 | loss: 296.24407| val_0_rmse: 17.19777|  1:27:18s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_rmse = 17.11405\n",
      "epoch 99 | loss: 296.24407| val_0_rmse: 17.19777|  1:27:18s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_rmse = 17.11405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model -> tabnet_opt_20251013T000538Z.joblib; metadata -> tabnet_opt_20251013T000538Z.json\n",
      "[TabNet] Best parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.078137691205107, 'lambda_sparse': 0.00011290133559092664, 'lr': 0.0037381058681917956}\n"
     ]
    }
   ],
   "source": [
    "# Simplified TabNet Implementation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "def prepare_data_for_tabnet(X, y=None):\n",
    "    \"\"\"Convert data to TabNet-compatible format.\"\"\"\n",
    "    X_dense = X.toarray().astype(np.float32) if hasattr(X, 'toarray') else np.asarray(X, dtype=np.float32)\n",
    "    if y is not None:\n",
    "        y_reshaped = np.asarray(y).reshape(-1, 1) if np.asarray(y).ndim == 1 else np.asarray(y)\n",
    "        return X_dense, y_reshaped\n",
    "    return X_dense\n",
    "\n",
    "class SimpleTabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Simplified TabNet wrapper with sensible defaults.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_d=16, n_a=16, n_steps=5, gamma=1.3, lambda_sparse=1e-4, \n",
    "                 lr=0.02, max_epochs=100, patience=20, batch_size=1024, seed=42):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_prep, y_prep = prepare_data_for_tabnet(X, y)\n",
    "        \n",
    "        # Create internal validation split (15% for early stopping)\n",
    "        n_samples = X_prep.shape[0]\n",
    "        val_size = max(1, int(n_samples * 0.15))\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        indices = rng.permutation(n_samples)\n",
    "        \n",
    "        train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "        X_train, y_train = X_prep[train_idx], y_prep[train_idx]\n",
    "        X_val, y_val = X_prep[val_idx], y_prep[val_idx]\n",
    "        \n",
    "        self.model_ = TabNetRegressor(\n",
    "            n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps,\n",
    "            gamma=self.gamma, lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params={'lr': self.lr}, seed=self.seed\n",
    "        )\n",
    "        \n",
    "        self.model_.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=self.max_epochs,\n",
    "            patience=self.patience,\n",
    "            batch_size=self.batch_size,\n",
    "            virtual_batch_size=128\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'model_')\n",
    "        X_prep = prepare_data_for_tabnet(X)\n",
    "        return self.model_.predict(X_prep).ravel()\n",
    "\n",
    "def get_tabnet_param_space(trial):\n",
    "    \"\"\"Simplified parameter space for TabNet optimization.\"\"\"\n",
    "    return {\n",
    "        'n_d': trial.suggest_categorical('n_d', [8, 16, 24]),\n",
    "        'n_a': trial.suggest_categorical('n_a', [8, 16, 24]),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 6),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 1.8),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),\n",
    "        'lr': trial.suggest_float('lr', 1e-3, 2e-2, log=True),\n",
    "        'max_epochs': 100,\n",
    "        'patience': 20\n",
    "    }\n",
    "\n",
    "def load_or_train_tabnet():\n",
    "    \"\"\"Load existing TabNet model or train new one.\"\"\"\n",
    "    tabnet_files = sorted(MODEL_DIR.glob('tabnet_opt_*.joblib'))\n",
    "    \n",
    "    if tabnet_files and not FORCE_RETRAIN:\n",
    "        try:\n",
    "            model = joblib.load(tabnet_files[-1])\n",
    "            print(f\"[TabNet] Loaded existing model: {tabnet_files[-1].name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"[TabNet] Failed to load model: {e}\")\n",
    "    \n",
    "    # Train new model\n",
    "    print(\"[TabNet] Training new model with Optuna optimization\")\n",
    "    study, best_params = optimize_model_with_optuna(\n",
    "        model_name='TabNetRegressor',\n",
    "        estimator_builder=lambda params: SimpleTabNetWrapper(**params),\n",
    "        param_space_fn=get_tabnet_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model = SimpleTabNetWrapper(**best_params)\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Save model and metadata\n",
    "    save_model(model, 'tabnet_opt', {'best_params': best_params})\n",
    "    print(f\"[TabNet] Best parameters: {best_params}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Execute TabNet training/loading\n",
    "tabnet_model = load_or_train_tabnet()\n",
    "\n",
    "# Generate predictions\n",
    "tabnet_valid_pred = tabnet_model.predict(X_valid_enc)\n",
    "tabnet_test_pred = tabnet_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2adcc8",
   "metadata": {},
   "source": [
    "# 5. Tree-based ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7c82e",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f9c81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 20:05:39,185] A new study created in memory with name: XGBoost_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73742b03c7684ce592b2364e54fb1c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 20:05:54,055] Trial 0 finished with value: 17.677079001125602 and parameters: {'n_estimators': 350, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.779597545259111, 'colsample_bytree': 0.6312037280884872, 'reg_alpha': 1.23583827723069e-07, 'reg_lambda': 3.3323645788192616e-08}. Best is trial 0 with value: 17.677079001125602.\n",
      "[I 2025-10-12 20:06:01,858] Trial 1 finished with value: 17.804445533726852 and parameters: {'n_estimators': 547, 'max_depth': 7, 'learning_rate': 0.11114989443094977, 'subsample': 0.6061753482887408, 'colsample_bytree': 0.7939819704323989, 'reg_alpha': 0.006715811311069936, 'reg_lambda': 8.148018307012941e-07}. Best is trial 0 with value: 17.677079001125602.\n",
      "[I 2025-10-12 20:06:01,858] Trial 1 finished with value: 17.804445533726852 and parameters: {'n_estimators': 547, 'max_depth': 7, 'learning_rate': 0.11114989443094977, 'subsample': 0.6061753482887408, 'colsample_bytree': 0.7939819704323989, 'reg_alpha': 0.006715811311069936, 'reg_lambda': 8.148018307012941e-07}. Best is trial 0 with value: 17.677079001125602.\n",
      "[I 2025-10-12 20:06:04,492] Trial 2 finished with value: 16.813587197977455 and parameters: {'n_estimators': 272, 'max_depth': 4, 'learning_rate': 0.028145092716060652, 'subsample': 0.7574269294896714, 'colsample_bytree': 0.6863890037284232, 'reg_alpha': 1.092959278721938e-06, 'reg_lambda': 0.0032112643094417484}. Best is trial 2 with value: 16.813587197977455.\n",
      "[I 2025-10-12 20:06:04,492] Trial 2 finished with value: 16.813587197977455 and parameters: {'n_estimators': 272, 'max_depth': 4, 'learning_rate': 0.028145092716060652, 'subsample': 0.7574269294896714, 'colsample_bytree': 0.6863890037284232, 'reg_alpha': 1.092959278721938e-06, 'reg_lambda': 0.0032112643094417484}. Best is trial 2 with value: 16.813587197977455.\n",
      "[I 2025-10-12 20:06:06,908] Trial 3 finished with value: 16.758000881962577 and parameters: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'subsample': 0.7368209952651108, 'colsample_bytree': 0.7570351922786027, 'reg_alpha': 2.498713568466942e-07, 'reg_lambda': 0.00042472707398058225}. Best is trial 3 with value: 16.758000881962577.\n",
      "[I 2025-10-12 20:06:06,908] Trial 3 finished with value: 16.758000881962577 and parameters: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'subsample': 0.7368209952651108, 'colsample_bytree': 0.7570351922786027, 'reg_alpha': 2.498713568466942e-07, 'reg_lambda': 0.00042472707398058225}. Best is trial 3 with value: 16.758000881962577.\n",
      "[I 2025-10-12 20:06:09,792] Trial 4 finished with value: 16.829427479934907 and parameters: {'n_estimators': 437, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'subsample': 0.6511572371061874, 'colsample_bytree': 0.6130103185970559, 'reg_alpha': 0.04387314432435398, 'reg_lambda': 4.905556676028774}. Best is trial 3 with value: 16.758000881962577.\n",
      "Best XGBoost params: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'subsample': 0.7368209952651108, 'colsample_bytree': 0.7570351922786027, 'reg_alpha': 2.498713568466942e-07, 'reg_lambda': 0.00042472707398058225}\n",
      "[I 2025-10-12 20:06:09,792] Trial 4 finished with value: 16.829427479934907 and parameters: {'n_estimators': 437, 'max_depth': 3, 'learning_rate': 0.07896186801026692, 'subsample': 0.6511572371061874, 'colsample_bytree': 0.6130103185970559, 'reg_alpha': 0.04387314432435398, 'reg_lambda': 4.905556676028774}. Best is trial 3 with value: 16.758000881962577.\n",
      "Best XGBoost params: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'subsample': 0.7368209952651108, 'colsample_bytree': 0.7570351922786027, 'reg_alpha': 2.498713568466942e-07, 'reg_lambda': 0.00042472707398058225}\n",
      "Saved model -> xgboost_opt_20251013T000610Z.joblib; metadata -> xgboost_opt_20251013T000610Z.json\n",
      "Saved model -> xgboost_opt_20251013T000610Z.joblib; metadata -> xgboost_opt_20251013T000610Z.json\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization and training for ensemble models\n",
    "def build_xgb_estimator(params: Dict) -> XGBRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return XGBRegressor(**base_params)\n",
    "\n",
    "\n",
    "def xgb_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1e-1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True)\n",
    "    }\n",
    "\n",
    "if xgb_model is None or FORCE_RETRAIN:\n",
    "    print(\"[XGBoost] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    xgb_study, xgb_best_params = optimize_model_with_optuna(\n",
    "        model_name='XGBoost',\n",
    "        estimator_builder=build_xgb_estimator,\n",
    "        param_space_fn=xgb_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print('Best XGBoost params:', xgb_best_params)\n",
    "    # Fit model with optimized hyperparameters\n",
    "    xgb_model = build_xgb_estimator(xgb_best_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    save_model(xgb_model, 'xgboost_opt', {'best_params': xgb_best_params})\n",
    "else:\n",
    "    # Refit even when preloaded to ensure alignment with current data & preprocessing\n",
    "    print('[XGBoost] Preloaded model found; refitting on current data.')\n",
    "    # Try to pull previously stored best params from metadata if available\n",
    "    reuse_params = None\n",
    "    try:\n",
    "        if 'xgb_model_meta' in globals() and isinstance(xgb_model_meta, dict):\n",
    "            reuse_params = xgb_model_meta.get('best_params')\n",
    "    except Exception:\n",
    "        reuse_params = None\n",
    "    if reuse_params is None:\n",
    "        # Fall back to current model's parameters (filter to search space + core)\n",
    "        try:\n",
    "            current = xgb_model.get_params()\n",
    "            reuse_keys = {'n_estimators','max_depth','learning_rate','subsample','colsample_bytree','reg_alpha','reg_lambda'}\n",
    "            reuse_params = {k: v for k, v in current.items() if k in reuse_keys}\n",
    "        except Exception:\n",
    "            reuse_params = {}\n",
    "    # Rebuild a fresh estimator to avoid any internal state carry-over\n",
    "    xgb_model = build_xgb_estimator(reuse_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    # Save refit artifact\n",
    "    save_model(xgb_model, 'xgboost_opt_refit', {'refit': True, 'best_params': reuse_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229be5e",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6950519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 20:06:10,510] A new study created in memory with name: RandomForest_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b52ffad81c4ab799a7e5d678c36846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 20:06:17,405] Trial 0 finished with value: 16.85200958243298 and parameters: {'n_estimators': 425, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.4780093202212183}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:20,530] Trial 1 finished with value: 17.118219387331564 and parameters: {'n_estimators': 293, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.7540362888980228}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:20,530] Trial 1 finished with value: 17.118219387331564 and parameters: {'n_estimators': 293, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.7540362888980228}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:23,940] Trial 2 finished with value: 16.869692123066528 and parameters: {'n_estimators': 212, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 0.49091248360355033}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:23,940] Trial 2 finished with value: 16.869692123066528 and parameters: {'n_estimators': 212, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 0.49091248360355033}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:27,329] Trial 3 finished with value: 16.937161768697717 and parameters: {'n_estimators': 310, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.5456145700990209}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:27,329] Trial 3 finished with value: 16.937161768697717 and parameters: {'n_estimators': 310, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.5456145700990209}. Best is trial 0 with value: 16.85200958243298.\n",
      "[I 2025-10-12 20:06:33,371] Trial 4 finished with value: 17.053835299150183 and parameters: {'n_estimators': 567, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.628034992108518}. Best is trial 0 with value: 16.85200958243298.\n",
      "Best Random Forest params: {'n_estimators': 425, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.4780093202212183}\n",
      "[I 2025-10-12 20:06:33,371] Trial 4 finished with value: 17.053835299150183 and parameters: {'n_estimators': 567, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.628034992108518}. Best is trial 0 with value: 16.85200958243298.\n",
      "Best Random Forest params: {'n_estimators': 425, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.4780093202212183}\n",
      "Saved model -> random_forest_opt_20251013T000637Z.joblib; metadata -> random_forest_opt_20251013T000637Z.json\n",
      "Saved model -> random_forest_opt_20251013T000637Z.joblib; metadata -> random_forest_opt_20251013T000637Z.json\n"
     ]
    }
   ],
   "source": [
    "def build_rf_estimator(params: Dict) -> RandomForestRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return RandomForestRegressor(**base_params)\n",
    "\n",
    "\n",
    "def rf_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_float('max_features', 0.4, 0.9)\n",
    "    }\n",
    "\n",
    "if rf_model is None or FORCE_RETRAIN:\n",
    "    print(\"[RandomForest] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    rf_study, rf_best_params = optimize_model_with_optuna(\n",
    "        model_name='RandomForest',\n",
    "        estimator_builder=build_rf_estimator,\n",
    "        param_space_fn=rf_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Random Forest params:', rf_best_params)\n",
    "    rf_model = build_rf_estimator(rf_best_params)\n",
    "    rf_model.fit(X_train_enc, y_train)\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n",
    "    save_model(rf_model, 'random_forest_opt', {'best_params': rf_best_params})\n",
    "else:\n",
    "    print(\"[RandomForest] Using preloaded optimized model. Skipping training.\")\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
