{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731d278a",
   "metadata": {},
   "source": [
    "# Advanced Regression Pipeline\n",
    "\n",
    "\n",
    "This notebook builds on the LightGBM pipeline to compare three regression algorithms using data from the local database. It includes data loading, preprocessing, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55439ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: D:\\docs\\MADS\\696-Milestone 2\\supervised\n",
      "LightGBM version: 4.6.0\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "## Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Core Scientific Stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Database\n",
    "import psycopg2\n",
    "\n",
    "## Machine Learning / Preprocessing (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, Pipeline as SkPipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "## Gradient Boosting Libraries\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "## Deep Learning / Tabular\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "## Optimization & Persistence\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Setup model directory (handle notebook environment where __file__ is undefined)\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback: assume notebook is inside src; go up one directory if so\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'supervised.ipynb').exists() or (cwd / 'unsupervised.ipynb').exists():\n",
    "        PROJECT_ROOT = cwd\n",
    "    else:\n",
    "        for parent in cwd.parents:\n",
    "            if (parent / 'requirements.txt').exists() or (parent / 'README.md').exists():\n",
    "                PROJECT_ROOT = parent / 'src'\n",
    "                break\n",
    "        else:\n",
    "            PROJECT_ROOT = cwd  # final fallback\n",
    "\n",
    "MODEL_DIR = (PROJECT_ROOT / '..' / 'supervised').resolve()\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODEL_DIR}\")\n",
    "\n",
    "def save_model(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"Utility to persist models and optional metadata alongside them.\n",
    "    Saves model as joblib plus a companion JSON with metadata/hyperparams.\"\"\"\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    base_name = f\"{name}_{timestamp}\"\n",
    "    model_path = MODEL_DIR / f\"{base_name}.joblib\"\n",
    "    meta_path = MODEL_DIR / f\"{base_name}.json\"\n",
    "    joblib.dump(model, model_path)\n",
    "    meta = {'model_name': name, 'saved_utc': timestamp}\n",
    "    if extra:\n",
    "        meta.update(extra)\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Saved model -> {model_path.name}; metadata -> {meta_path.name}\")\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": os.getenv(\"LOCAL_HOST\"),\n",
    "    \"user\": os.getenv(\"LOCAL_USER\"),\n",
    "    \"password\": os.getenv(\"LOCAL_PW\"),\n",
    "    \"port\": os.getenv(\"LOCAL_PORT\"),\n",
    "    \"dbname\": os.getenv(\"LOCAL_DB\")\n",
    "}\n",
    "\n",
    "# Display versions\n",
    "print('LightGBM version:', lgb.__version__)\n",
    "print('XGBoost version:', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e688798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available - PyTorch will use CPU only\")\n",
    "    print(\"To enable GPU training, install PyTorch with CUDA support:\")\n",
    "    print(\"  conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\")\n",
    "    print(\"  or\")\n",
    "    print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcae04",
   "metadata": {},
   "source": [
    "### Auto Load / Conditional Training\n",
    "If a previously saved optimized model exists in `src/supervised`, the notebook will load the most recent artifact (by timestamp in filename) and skip retraining unless `FORCE_RETRAIN=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe517ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-load status:\n",
      "  xgb_model: loaded xgboost_opt_refit_20251008T110355Z.joblib\n",
      "  rf_model: loaded random_forest_opt_20251011T193747Z.joblib\n",
      "  mlp_model: failed: Can't get attribute 'TorchMLPRegressor' on <module '__main__'>\n",
      "  svr_model: loaded svr_opt_20251011T185830Z.joblib\n",
      "  lr_model: loaded linear_regression_opt_20251011T185830Z.joblib\n",
      "  poly_model: loaded poly_reg_opt_20251011T191654Z.joblib\n",
      "  tabnet_model: deferred (class not yet defined)\n",
      "FORCE_RETRAIN= True\n"
     ]
    }
   ],
   "source": [
    "# Auto-load previously saved optimized models (XGBoost / RandomForest / SVR / LinearRegression / Polynomial / MLP / TabNet)\n",
    "from pathlib import Path as _Path\n",
    "import json as _json\n",
    "\n",
    "# Initialize placeholders if not already present\n",
    "globals().setdefault('FORCE_RETRAIN', True)\n",
    "\n",
    "# Only set to None if not defined to avoid clobbering models loaded earlier in session\n",
    "if 'xgb_model' not in globals():\n",
    "    xgb_model = None\n",
    "if 'rf_model' not in globals():\n",
    "    rf_model = None\n",
    "if 'mlp_model' not in globals():\n",
    "    mlp_model = None\n",
    "if 'tabnet_model' not in globals():\n",
    "    tabnet_model = None  # Will delay loading until wrapper class defined\n",
    "if 'svr_model' not in globals():\n",
    "    svr_model = None\n",
    "if 'lr_model' not in globals():\n",
    "    lr_model = None\n",
    "if 'poly_model' not in globals():\n",
    "    poly_model = None\n",
    "\n",
    "MODEL_GLOB_PATTERNS = {\n",
    "    'xgb_model': 'xgboost_opt_*.joblib',\n",
    "    'rf_model': 'random_forest_opt_*.joblib', \n",
    "    'mlp_model': 'mlp_opt_*.joblib', \n",
    "    # 'tabnet_model': DEFERRED - skip here, load after wrapper class defined\n",
    "    'svr_model': 'svr_opt_*.joblib',\n",
    "    'lr_model': 'linear_regression_opt_*.joblib',\n",
    "    'poly_model': 'poly_reg_opt_*.joblib'\n",
    "}\n",
    "\n",
    "loaded_flags = {}\n",
    "for var, pattern in MODEL_GLOB_PATTERNS.items():\n",
    "    if globals().get(var) is not None:\n",
    "        loaded_flags[var] = 'pre-existing'\n",
    "        continue\n",
    "    matches = sorted(MODEL_DIR.glob(pattern))\n",
    "    if not matches:\n",
    "        loaded_flags[var] = 'not found'\n",
    "        continue\n",
    "    latest = matches[-1]\n",
    "    try:\n",
    "        globals()[var] = joblib.load(latest)\n",
    "        meta_file = latest.with_suffix('.json')\n",
    "        if meta_file.exists():\n",
    "            with open(meta_file) as f:\n",
    "                globals()[f\"{var}_meta\"] = _json.load(f)\n",
    "        loaded_flags[var] = f\"loaded {latest.name}\"\n",
    "    except Exception as e:\n",
    "        loaded_flags[var] = f\"failed: {e}\";\n",
    "        globals()[var] = None\n",
    "\n",
    "loaded_flags['tabnet_model'] = 'deferred (class not yet defined)'\n",
    "\n",
    "print(\"Auto-load status:\")\n",
    "for k,v in loaded_flags.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"FORCE_RETRAIN=\", FORCE_RETRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6177eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_31388\\312996059.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden data loaded into DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23038 entries, 0 to 23037\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   school_name             23038 non-null  object \n",
      " 1   school_type             23038 non-null  object \n",
      " 2   teachers_fte            22550 non-null  float64\n",
      " 3   enrollment              22863 non-null  float64\n",
      " 4   grade_eight_enrollment  21613 non-null  float64\n",
      " 5   math_counts             22507 non-null  float64\n",
      " 6   math_high_pct           22507 non-null  float64\n",
      " 7   math_low_pct            19960 non-null  float64\n",
      " 8   read_counts             22386 non-null  float64\n",
      " 9   read_high_pct           22386 non-null  float64\n",
      " 10  read_low_pct            19907 non-null  float64\n",
      " 11  pct_hhi_150k_200k       23038 non-null  float64\n",
      " 12  pct_hhi_220k_plus       23038 non-null  float64\n",
      " 13  avg_natwalkind          23038 non-null  float64\n",
      " 14  total_10_14             23038 non-null  int64  \n",
      " 15  pct_10_14               23038 non-null  int64  \n",
      " 16  pct_female_10_14        22937 non-null  float64\n",
      " 17  total_pop               23038 non-null  int64  \n",
      " 18  hhi_150k_200k           23038 non-null  int64  \n",
      " 19  hhi_220k_plus           23038 non-null  int64  \n",
      " 20  schools_in_zip          23038 non-null  int64  \n",
      " 21  dup_rank                23038 non-null  int64  \n",
      "dtypes: float64(13), int64(7), object(2)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load data\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    print(\"Database connection successful\")\n",
    "    sql_query = \"SELECT * FROM dev.base_data;\"\n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    conn.close()\n",
    "    print(\"Golden data loaded into DataFrame:\")\n",
    "    print(df.info())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6bcca",
   "metadata": {},
   "source": [
    "## 3. Data Splitting: Train, Validation, Test\n",
    "Split the dataset into train, validation, and test sets, ensuring proper handling of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1bb52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_name', 'school_type', 'teachers_fte', 'enrollment',\n",
       "       'grade_eight_enrollment', 'math_counts', 'math_high_pct',\n",
       "       'math_low_pct', 'read_counts', 'read_high_pct', 'read_low_pct',\n",
       "       'pct_hhi_150k_200k', 'pct_hhi_220k_plus', 'avg_natwalkind',\n",
       "       'total_10_14', 'pct_10_14', 'pct_female_10_14', 'total_pop',\n",
       "       'hhi_150k_200k', 'hhi_220k_plus', 'schools_in_zip', 'dup_rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13df7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (11367, 18), Validation shape: (3789, 18), Test shape: (3789, 18)\n"
     ]
    }
   ],
   "source": [
    "# Define target and drop missing\n",
    "TARGET = 'math_high_pct' if 'math_high_pct' in df.columns else 'target'\n",
    "data = df.dropna().reset_index(drop=True)\n",
    "data = data.set_index('school_name')\n",
    "\n",
    "# Split features and target\n",
    "feature_cols = [c for c in data.columns if c != TARGET and c != 'dup_rank' and c != 'math_low_pct']\n",
    "X = data[feature_cols]\n",
    "y = data[TARGET]\n",
    "\n",
    "# Train/validation/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "print(f'Train shape: {X_train.shape}, Validation shape: {X_valid.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52296",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Preprocessing Pipeline\n",
    "Identify numeric and categorical features, set up StandardScaler and OneHotEncoder, and build a ColumnTransformer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c071ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['teachers_fte', 'enrollment', 'grade_eight_enrollment', 'math_counts', 'read_counts', 'read_high_pct', 'read_low_pct', 'pct_hhi_150k_200k', 'pct_hhi_220k_plus', 'avg_natwalkind', 'total_10_14', 'pct_10_14', 'pct_female_10_14', 'total_pop', 'hhi_150k_200k', 'hhi_220k_plus', 'schools_in_zip']\n",
      "Categorical features: ['school_type']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_valid_enc = preprocessor.transform(X_valid)\n",
    "X_test_enc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6187ca5",
   "metadata": {},
   "source": [
    "## Model Optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bc1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Optuna-based optimizer for cross-validated hyperparameter tuning\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "def optimize_model_with_optuna(\n",
    "                                model_name: str,\n",
    "                                estimator_builder: Callable[[Dict], object],\n",
    "                                param_space_fn: Callable[[optuna.trial.Trial], Dict],\n",
    "                                X,\n",
    "                                y,\n",
    "                                scoring: str = 'neg_root_mean_squared_error',\n",
    "                                cv: int = 3,\n",
    "                                n_trials: int = 5,\n",
    "                                direction: str = 'minimize',\n",
    "                                random_state: int = 42,\n",
    "                                n_jobs: int = -1,\n",
    "                            ) -> Tuple[optuna.study.Study, Dict]:\n",
    "    \"\"\"Optimize a model's hyperparameters using Optuna and cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name used to label the Optuna study\n",
    "        estimator_builder: Callable that receives a params dict and returns an unfitted estimator\n",
    "        param_space_fn: Callable that maps an Optuna trial to a hyperparameter dictionary\n",
    "        X, y: Training features and targets used for cross-validation\n",
    "        scoring: scikit-learn scoring string guiding optimization\n",
    "        cv: Number of cross-validation folds\n",
    "        n_trials: Number of Optuna trials to run\n",
    "        direction: 'minimize' or 'maximize' depending on the objective\n",
    "        random_state: Seed for the Optuna sampler\n",
    "        n_jobs: Parallelism for cross_val_score\n",
    "\n",
    "    Returns:\n",
    "        The completed Optuna study and the best hyperparameters discovered.\n",
    "    \"\"\"\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(study_name=f\"{model_name}_opt\", direction=direction, sampler=sampler)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        params = param_space_fn(trial)\n",
    "        estimator = estimator_builder(params)\n",
    "        scores = cross_val_score(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "        mean_score = np.mean(scores)\n",
    "        normalized_score = -mean_score if scoring.startswith('neg') else mean_score\n",
    "        return normalized_score if direction == 'minimize' else -normalized_score\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    return study, study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6bcf7",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533b035",
   "metadata": {},
   "source": [
    "## LINEAR MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:21,066] A new study created in memory with name: SVR_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVR] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bac00f4aee4780907b59354fb06a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:28,609] Trial 0 finished with value: 21.65850568247251 and parameters: {'kernel': 'poly', 'C': 3.0049873591901566, 'epsilon': 0.08900466011060913, 'degree': 2, 'gamma': 0.1143098387631322}. Best is trial 0 with value: 21.65850568247251.\n",
      "[I 2025-10-11 16:02:36,048] Trial 1 finished with value: 20.70295950569489 and parameters: {'kernel': 'rbf', 'C': 0.5318033256270142, 'epsilon': 0.2924774630404986, 'gamma': 0.3818145165896869}. Best is trial 1 with value: 20.70295950569489.\n",
      "[I 2025-10-11 16:02:36,048] Trial 1 finished with value: 20.70295950569489 and parameters: {'kernel': 'rbf', 'C': 0.5318033256270142, 'epsilon': 0.2924774630404986, 'gamma': 0.3818145165896869}. Best is trial 1 with value: 20.70295950569489.\n",
      "[I 2025-10-11 16:02:41,615] Trial 2 finished with value: 19.044401625690607 and parameters: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}. Best is trial 2 with value: 19.044401625690607.\n",
      "Best SVR params: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}\n",
      "[I 2025-10-11 16:02:41,615] Trial 2 finished with value: 19.044401625690607 and parameters: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}. Best is trial 2 with value: 19.044401625690607.\n",
      "Best SVR params: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:53,313] A new study created in memory with name: PolynomialRegression_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model -> svr_opt_20251011T200253Z.joblib; metadata -> svr_opt_20251011T200253Z.json\n",
      "Saved model -> linear_regression_opt_20251011T200253Z.joblib; metadata -> linear_regression_opt_20251011T200253Z.json\n",
      "[PolynomialRegression] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606a34d560a41b9b63b2ee40dc49940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:03:03,403] Trial 0 finished with value: 7434869348.622883 and parameters: {'degree': 3, 'interaction_only': False}. Best is trial 0 with value: 7434869348.622883.\n"
     ]
    }
   ],
   "source": [
    "# Linear & Kernel-based Models: SVR + Polynomial Regression (with Optuna tuning + CV)\n",
    "def build_svr_estimator(params: Dict) -> SVR:\n",
    "    base = {'kernel': params.get('kernel', 'rbf')}\n",
    "    # Map params safely\n",
    "    for k in ['C','epsilon','gamma','degree']:\n",
    "        if k in params:\n",
    "            base[k] = params[k]\n",
    "    return SVR(**base)\n",
    "\n",
    "def svr_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf','poly','sigmoid'])\n",
    "    params = {\n",
    "        'kernel': kernel,\n",
    "        'C': trial.suggest_float('C', 0.5, 10, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.05, 0.3),\n",
    "    }\n",
    "    if kernel in ['rbf','sigmoid']:\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 0.5, log=True)\n",
    "    if kernel == 'poly':\n",
    "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 1, log=True)\n",
    "    return params\n",
    "\n",
    "def build_poly_estimator(params: Dict):\n",
    "    degree = params.get('degree', 2)\n",
    "    include_bias = params.get('include_bias', False)\n",
    "    interaction_only = params.get('interaction_only', False)\n",
    "    return SkPipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias, interaction_only=interaction_only)),\n",
    "        ('lr', LinearRegression())\n",
    "    ])\n",
    "\n",
    "def poly_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'degree': trial.suggest_int('degree', 2, 5),\n",
    "        'include_bias': False,\n",
    "        'interaction_only': trial.suggest_categorical('interaction_only', [False, True])\n",
    "    }\n",
    "\n",
    "# Run / reuse SVR optimization\n",
    "if svr_model is None or FORCE_RETRAIN:\n",
    "    print('[SVR] Starting Optuna optimization...')\n",
    "    svr_study, svr_best_params = optimize_model_with_optuna(\n",
    "        model_name='SVR',\n",
    "        estimator_builder=build_svr_estimator,\n",
    "        param_space_fn=svr_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best SVR params:', svr_best_params)\n",
    "    svr_model = build_svr_estimator(svr_best_params)\n",
    "    svr_model.fit(X_train_enc, y_train)\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "    save_model(svr_model, 'svr_opt', {'best_params': svr_best_params})\n",
    "else:\n",
    "    print('[SVR] Using preloaded optimized model; generating predictions.')\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "\n",
    "# Baseline Linear Regression (also optionally re-optimized via polynomial)\n",
    "if lr_model is None or FORCE_RETRAIN:\n",
    "    # Keep a simple baseline linear regression for reference\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_enc, y_train)\n",
    "    save_model(lr_model, 'linear_regression_opt', {'params': lr_model.get_params(), 'baseline': True})\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "elif lr_model is not None:\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "\n",
    "# Polynomial Regression optimization\n",
    "if poly_model is None or FORCE_RETRAIN:\n",
    "    print('[PolynomialRegression] Starting Optuna optimization...')\n",
    "    poly_study, poly_best_params = optimize_model_with_optuna(\n",
    "        model_name='PolynomialRegression',\n",
    "        estimator_builder=build_poly_estimator,\n",
    "        param_space_fn=poly_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Polynomial Regression params:', poly_best_params)\n",
    "    poly_model = build_poly_estimator(poly_best_params)\n",
    "    poly_model.fit(X_train_enc, y_train)\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)\n",
    "    save_model(poly_model, 'poly_reg_opt', {'best_params': poly_best_params})\n",
    "else:\n",
    "    print('[PolynomialRegression] Using preloaded optimized model; generating predictions.')\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf134e",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58546626",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745b1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Refitting preloaded model\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.TorchMLPRegressor'>: it's not the same object as __main__.TorchMLPRegressor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m mlp_valid_pred \u001b[38;5;241m=\u001b[39m mlp_model\u001b[38;5;241m.\u001b[39mpredict(X_valid_enc)\n\u001b[0;32m    158\u001b[0m mlp_test_pred \u001b[38;5;241m=\u001b[39m mlp_model\u001b[38;5;241m.\u001b[39mpredict(X_test_enc)\n\u001b[1;32m--> 159\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmlp_opt_refit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrefit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 83\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, name, extra)\u001b[0m\n\u001b[0;32m     81\u001b[0m model_path \u001b[38;5;241m=\u001b[39m MODEL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m meta_path \u001b[38;5;241m=\u001b[39m MODEL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m meta \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_utc\u001b[39m\u001b[38;5;124m'\u001b[39m: timestamp}\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\joblib\\numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 553\u001b[0m         \u001b[43mNumpyPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:687\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs[0] from __newobj__ args has the wrong class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    686\u001b[0m args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 687\u001b[0m \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m save(args)\n\u001b[0;32m    689\u001b[0m write(NEWOBJ)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:1129\u001b[0m, in \u001b[0;36m_Pickler.save_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m):\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(\u001b[38;5;28mtype\u001b[39m, (\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,), obj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m-> 1129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\miniconda3\\envs\\milestone2\\lib\\pickle.py:1076\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n\u001b[1;32m-> 1076\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[0;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not the same object as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1078\u001b[0m             (obj, module_name, name))\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1081\u001b[0m     code \u001b[38;5;241m=\u001b[39m _extension_registry\u001b[38;5;241m.\u001b[39mget((module_name, name))\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class '__main__.TorchMLPRegressor'>: it's not the same object as __main__.TorchMLPRegressor"
     ]
    }
   ],
   "source": [
    "# Torch MLP Regressor with sklearn API\n",
    "\n",
    "def _ensure_dense_np(X):\n",
    "    return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "class TorchMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"PyTorch MLP for regression with sklearn API and CUDA support.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=(128, 64), activation='relu', \n",
    "                 learning_rate_init=1e-3, alpha=0.0, batch_size=128, max_iter=100,\n",
    "                 early_stopping=True, validation_fraction=0.1, n_iter_no_change=10,\n",
    "                 random_state=42, verbose=False, device=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "    def _build_network(self, in_features):\n",
    "        layers = []\n",
    "        act_fn = nn.ReLU if self.activation == 'relu' else nn.Tanh\n",
    "        prev = in_features\n",
    "        for h in self.hidden_layer_sizes:\n",
    "            layers.extend([nn.Linear(prev, h), act_fn()])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Setup\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        \n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        y_np = (y.values if hasattr(y, 'values') else y).astype('float32').reshape(-1, 1)\n",
    "        \n",
    "        # Train/val split\n",
    "        if self.early_stopping and 0 < self.validation_fraction < 0.5:\n",
    "            val_size = max(1, int(len(X_np) * self.validation_fraction))\n",
    "            idx = np.random.RandomState(self.random_state).permutation(len(X_np))\n",
    "            X_train, X_val = X_np[idx[val_size:]], X_np[idx[:val_size]]\n",
    "            y_train, y_val = y_np[idx[val_size:]], y_np[idx[:val_size]]\n",
    "        else:\n",
    "            X_train, y_train, X_val, y_val = X_np, y_np, None, None\n",
    "        \n",
    "        # Device and model\n",
    "        device = self.device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._device_ = device\n",
    "        self.model_ = self._build_network(X_np.shape[1]).to(device)\n",
    "        self.optimizer_ = torch.optim.Adam(self.model_.parameters(), \n",
    "                                          lr=self.learning_rate_init, \n",
    "                                          weight_decay=self.alpha)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        best_val, best_state, patience = math.inf, None, 0\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            self.model_.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                self.optimizer_.zero_grad()\n",
    "                criterion(self.model_(xb), yb).backward()\n",
    "                self.optimizer_.step()\n",
    "            \n",
    "            # Validation\n",
    "            if X_val is not None:\n",
    "                self.model_.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = criterion(\n",
    "                        self.model_(torch.from_numpy(X_val).to(device)),\n",
    "                        torch.from_numpy(y_val).to(device)\n",
    "                    ).item()\n",
    "                \n",
    "                if val_loss < best_val - 1e-9:\n",
    "                    best_val, patience = val_loss, 0\n",
    "                    best_state = {k: v.cpu().clone() for k, v in self.model_.state_dict().items()}\n",
    "                else:\n",
    "                    patience += 1\n",
    "                \n",
    "                if patience >= self.n_iter_no_change:\n",
    "                    break\n",
    "        \n",
    "        if best_state:\n",
    "            self.model_.load_state_dict(best_state)\n",
    "        self.n_features_in_ = X_np.shape[1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, ['model_'])\n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(torch.from_numpy(X_np).to(self._device_))\n",
    "        return preds.cpu().numpy().ravel()\n",
    "\n",
    "# Helper functions\n",
    "def build_mlp_estimator(params):\n",
    "    params = params.copy()\n",
    "    params.pop('hl1', None)\n",
    "    params.pop('hl2', None)\n",
    "    return TorchMLPRegressor(**params)\n",
    "\n",
    "def mlp_param_space(trial):\n",
    "    return {\n",
    "        'hidden_layer_sizes': tuple(sorted([\n",
    "            trial.suggest_int('hl1', 64, 256, step=32),\n",
    "            trial.suggest_int('hl2', 32, 192, step=32)\n",
    "        ], reverse=True)),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        'max_iter': trial.suggest_int('max_iter', 150, 600, step=75),\n",
    "        'n_iter_no_change': trial.suggest_int('n_iter_no_change', 5, 25, step=5),\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.15,\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'random_state': 42,\n",
    "        'verbose': False,\n",
    "    }\n",
    "\n",
    "# Training\n",
    "if mlp_model is None or FORCE_RETRAIN:\n",
    "    print(f\"[MLP] Starting optimization on {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    mlp_study, mlp_best_params = optimize_model_with_optuna(\n",
    "        model_name='TorchMLPRegressor',\n",
    "        estimator_builder=build_mlp_estimator,\n",
    "        param_space_fn=mlp_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1,\n",
    "    )\n",
    "    print(f'[MLP] Best params: {mlp_best_params}')\n",
    "    mlp_model = build_mlp_estimator(mlp_best_params)\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt', {'best_params': mlp_best_params, 'framework': 'torch'})\n",
    "else:\n",
    "    print(\"[MLP] Refitting preloaded model\")\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt_refit', {'refit': True, 'framework': 'torch'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7801",
   "metadata": {},
   "source": [
    "## TABNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3270f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified TabNet Implementation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "def prepare_data_for_tabnet(X, y=None):\n",
    "    \"\"\"Convert data to TabNet-compatible format.\"\"\"\n",
    "    X_dense = X.toarray().astype(np.float32) if hasattr(X, 'toarray') else np.asarray(X, dtype=np.float32)\n",
    "    if y is not None:\n",
    "        y_reshaped = np.asarray(y).reshape(-1, 1) if np.asarray(y).ndim == 1 else np.asarray(y)\n",
    "        return X_dense, y_reshaped\n",
    "    return X_dense\n",
    "\n",
    "class SimpleTabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Simplified TabNet wrapper with sensible defaults.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_d=16, n_a=16, n_steps=5, gamma=1.3, lambda_sparse=1e-4, \n",
    "                 lr=0.02, max_epochs=100, patience=20, batch_size=1024, seed=42):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_prep, y_prep = prepare_data_for_tabnet(X, y)\n",
    "        \n",
    "        # Create internal validation split (15% for early stopping)\n",
    "        n_samples = X_prep.shape[0]\n",
    "        val_size = max(1, int(n_samples * 0.15))\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        indices = rng.permutation(n_samples)\n",
    "        \n",
    "        train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "        X_train, y_train = X_prep[train_idx], y_prep[train_idx]\n",
    "        X_val, y_val = X_prep[val_idx], y_prep[val_idx]\n",
    "        \n",
    "        self.model_ = TabNetRegressor(\n",
    "            n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps,\n",
    "            gamma=self.gamma, lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params={'lr': self.lr}, seed=self.seed\n",
    "        )\n",
    "        \n",
    "        self.model_.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=self.max_epochs,\n",
    "            patience=self.patience,\n",
    "            batch_size=self.batch_size,\n",
    "            virtual_batch_size=128\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'model_')\n",
    "        X_prep = prepare_data_for_tabnet(X)\n",
    "        return self.model_.predict(X_prep).ravel()\n",
    "\n",
    "def get_tabnet_param_space(trial):\n",
    "    \"\"\"Simplified parameter space for TabNet optimization.\"\"\"\n",
    "    return {\n",
    "        'n_d': trial.suggest_categorical('n_d', [8, 16, 24]),\n",
    "        'n_a': trial.suggest_categorical('n_a', [8, 16, 24]),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 6),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 1.8),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),\n",
    "        'lr': trial.suggest_float('lr', 1e-3, 2e-2, log=True),\n",
    "        'max_epochs': 100,\n",
    "        'patience': 20\n",
    "    }\n",
    "\n",
    "def load_or_train_tabnet():\n",
    "    \"\"\"Load existing TabNet model or train new one.\"\"\"\n",
    "    tabnet_files = sorted(MODEL_DIR.glob('tabnet_opt_*.joblib'))\n",
    "    \n",
    "    if tabnet_files and not FORCE_RETRAIN:\n",
    "        try:\n",
    "            model = joblib.load(tabnet_files[-1])\n",
    "            print(f\"[TabNet] Loaded existing model: {tabnet_files[-1].name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"[TabNet] Failed to load model: {e}\")\n",
    "    \n",
    "    # Train new model\n",
    "    print(\"[TabNet] Training new model with Optuna optimization\")\n",
    "    study, best_params = optimize_model_with_optuna(\n",
    "        model_name='TabNetRegressor',\n",
    "        estimator_builder=lambda params: SimpleTabNetWrapper(**params),\n",
    "        param_space_fn=get_tabnet_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model = SimpleTabNetWrapper(**best_params)\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Save model and metadata\n",
    "    save_model(model, 'tabnet_opt', {'best_params': best_params})\n",
    "    print(f\"[TabNet] Best parameters: {best_params}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Execute TabNet training/loading\n",
    "tabnet_model = load_or_train_tabnet()\n",
    "\n",
    "# Generate predictions\n",
    "tabnet_valid_pred = tabnet_model.predict(X_valid_enc)\n",
    "tabnet_test_pred = tabnet_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2adcc8",
   "metadata": {},
   "source": [
    "# 5. Tree-based ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7c82e",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization and training for ensemble models\n",
    "def build_xgb_estimator(params: Dict) -> XGBRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return XGBRegressor(**base_params)\n",
    "\n",
    "\n",
    "def xgb_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1e-1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True)\n",
    "    }\n",
    "\n",
    "if xgb_model is None or FORCE_RETRAIN:\n",
    "    print(\"[XGBoost] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    xgb_study, xgb_best_params = optimize_model_with_optuna(\n",
    "        model_name='XGBoost',\n",
    "        estimator_builder=build_xgb_estimator,\n",
    "        param_space_fn=xgb_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print('Best XGBoost params:', xgb_best_params)\n",
    "    # Fit model with optimized hyperparameters\n",
    "    xgb_model = build_xgb_estimator(xgb_best_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    save_model(xgb_model, 'xgboost_opt', {'best_params': xgb_best_params})\n",
    "else:\n",
    "    # Refit even when preloaded to ensure alignment with current data & preprocessing\n",
    "    print('[XGBoost] Preloaded model found; refitting on current data.')\n",
    "    # Try to pull previously stored best params from metadata if available\n",
    "    reuse_params = None\n",
    "    try:\n",
    "        if 'xgb_model_meta' in globals() and isinstance(xgb_model_meta, dict):\n",
    "            reuse_params = xgb_model_meta.get('best_params')\n",
    "    except Exception:\n",
    "        reuse_params = None\n",
    "    if reuse_params is None:\n",
    "        # Fall back to current model's parameters (filter to search space + core)\n",
    "        try:\n",
    "            current = xgb_model.get_params()\n",
    "            reuse_keys = {'n_estimators','max_depth','learning_rate','subsample','colsample_bytree','reg_alpha','reg_lambda'}\n",
    "            reuse_params = {k: v for k, v in current.items() if k in reuse_keys}\n",
    "        except Exception:\n",
    "            reuse_params = {}\n",
    "    # Rebuild a fresh estimator to avoid any internal state carry-over\n",
    "    xgb_model = build_xgb_estimator(reuse_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    # Save refit artifact\n",
    "    save_model(xgb_model, 'xgboost_opt_refit', {'refit': True, 'best_params': reuse_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229be5e",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6950519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rf_estimator(params: Dict) -> RandomForestRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return RandomForestRegressor(**base_params)\n",
    "\n",
    "\n",
    "def rf_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_float('max_features', 0.4, 0.9)\n",
    "    }\n",
    "\n",
    "if rf_model is None or FORCE_RETRAIN:\n",
    "    print(\"[RandomForest] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    rf_study, rf_best_params = optimize_model_with_optuna(\n",
    "        model_name='RandomForest',\n",
    "        estimator_builder=build_rf_estimator,\n",
    "        param_space_fn=rf_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Random Forest params:', rf_best_params)\n",
    "    rf_model = build_rf_estimator(rf_best_params)\n",
    "    rf_model.fit(X_train_enc, y_train)\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n",
    "    save_model(rf_model, 'random_forest_opt', {'best_params': rf_best_params})\n",
    "else:\n",
    "    print(\"[RandomForest] Using preloaded optimized model. Skipping training.\")\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
