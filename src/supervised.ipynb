{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731d278a",
   "metadata": {},
   "source": [
    "# Advanced Regression Pipeline\n",
    "\n",
    "\n",
    "This notebook builds on the LightGBM pipeline to compare three regression algorithms using data from the local database. It includes data loading, preprocessing, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55439ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: D:\\docs\\MADS\\696-Milestone 2\\supervised\n",
      "LightGBM version: 4.6.0\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "## Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Core Scientific Stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Database\n",
    "import psycopg2\n",
    "\n",
    "## Machine Learning / Preprocessing (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, Pipeline as SkPipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "## Gradient Boosting Libraries\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "## Deep Learning / Tabular\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "## Optimization & Persistence\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Setup model directory (handle notebook environment where __file__ is undefined)\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback: assume notebook is inside src; go up one directory if so\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'supervised.ipynb').exists() or (cwd / 'unsupervised.ipynb').exists():\n",
    "        PROJECT_ROOT = cwd\n",
    "    else:\n",
    "        for parent in cwd.parents:\n",
    "            if (parent / 'requirements.txt').exists() or (parent / 'README.md').exists():\n",
    "                PROJECT_ROOT = parent / 'src'\n",
    "                break\n",
    "        else:\n",
    "            PROJECT_ROOT = cwd  # final fallback\n",
    "\n",
    "MODEL_DIR = (PROJECT_ROOT / '..' / 'supervised').resolve()\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODEL_DIR}\")\n",
    "\n",
    "def save_model(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"Utility to persist models and optional metadata alongside them.\n",
    "    Saves model as joblib plus a companion JSON with metadata/hyperparams.\"\"\"\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    base_name = f\"{name}_{timestamp}\"\n",
    "    model_path = MODEL_DIR / f\"{base_name}.joblib\"\n",
    "    meta_path = MODEL_DIR / f\"{base_name}.json\"\n",
    "    joblib.dump(model, model_path)\n",
    "    meta = {'model_name': name, 'saved_utc': timestamp}\n",
    "    if extra:\n",
    "        meta.update(extra)\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Saved model -> {model_path.name}; metadata -> {meta_path.name}\")\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": os.getenv(\"LOCAL_HOST\"),\n",
    "    \"user\": os.getenv(\"LOCAL_USER\"),\n",
    "    \"password\": os.getenv(\"LOCAL_PW\"),\n",
    "    \"port\": os.getenv(\"LOCAL_PORT\"),\n",
    "    \"dbname\": os.getenv(\"LOCAL_DB\")\n",
    "}\n",
    "\n",
    "# Display versions\n",
    "print('LightGBM version:', lgb.__version__)\n",
    "print('XGBoost version:', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e688798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available - PyTorch will use CPU only\")\n",
    "    print(\"To enable GPU training, install PyTorch with CUDA support:\")\n",
    "    print(\"  conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\")\n",
    "    print(\"  or\")\n",
    "    print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcae04",
   "metadata": {},
   "source": [
    "### Auto Load / Conditional Training\n",
    "If a previously saved optimized model exists in `src/supervised`, the notebook will load the most recent artifact (by timestamp in filename) and skip retraining unless `FORCE_RETRAIN=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe517ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-load status:\n",
      "  xgb_model: loaded xgboost_opt_refit_20251008T110355Z.joblib\n",
      "  rf_model: loaded random_forest_opt_20251011T193747Z.joblib\n",
      "  mlp_model: failed: Can't get attribute 'TorchMLPRegressor' on <module '__main__'>\n",
      "  svr_model: loaded svr_opt_20251011T185830Z.joblib\n",
      "  lr_model: loaded linear_regression_opt_20251011T185830Z.joblib\n",
      "  poly_model: loaded poly_reg_opt_20251011T191654Z.joblib\n",
      "  tabnet_model: deferred (class not yet defined)\n",
      "FORCE_RETRAIN= True\n"
     ]
    }
   ],
   "source": [
    "# Auto-load previously saved optimized models (XGBoost / RandomForest / SVR / LinearRegression / Polynomial / MLP / TabNet)\n",
    "from pathlib import Path as _Path\n",
    "import json as _json\n",
    "\n",
    "# Initialize placeholders if not already present\n",
    "globals().setdefault('FORCE_RETRAIN', True)\n",
    "\n",
    "# Only set to None if not defined to avoid clobbering models loaded earlier in session\n",
    "if 'xgb_model' not in globals():\n",
    "    xgb_model = None\n",
    "if 'rf_model' not in globals():\n",
    "    rf_model = None\n",
    "if 'mlp_model' not in globals():\n",
    "    mlp_model = None\n",
    "if 'tabnet_model' not in globals():\n",
    "    tabnet_model = None  # Will delay loading until wrapper class defined\n",
    "if 'svr_model' not in globals():\n",
    "    svr_model = None\n",
    "if 'lr_model' not in globals():\n",
    "    lr_model = None\n",
    "if 'poly_model' not in globals():\n",
    "    poly_model = None\n",
    "\n",
    "MODEL_GLOB_PATTERNS = {\n",
    "    'xgb_model': 'xgboost_opt_*.joblib',\n",
    "    'rf_model': 'random_forest_opt_*.joblib', \n",
    "    'mlp_model': 'mlp_opt_*.joblib', \n",
    "    # 'tabnet_model': DEFERRED - skip here, load after wrapper class defined\n",
    "    'svr_model': 'svr_opt_*.joblib',\n",
    "    'lr_model': 'linear_regression_opt_*.joblib',\n",
    "    'poly_model': 'poly_reg_opt_*.joblib'\n",
    "}\n",
    "\n",
    "loaded_flags = {}\n",
    "for var, pattern in MODEL_GLOB_PATTERNS.items():\n",
    "    if globals().get(var) is not None:\n",
    "        loaded_flags[var] = 'pre-existing'\n",
    "        continue\n",
    "    matches = sorted(MODEL_DIR.glob(pattern))\n",
    "    if not matches:\n",
    "        loaded_flags[var] = 'not found'\n",
    "        continue\n",
    "    latest = matches[-1]\n",
    "    try:\n",
    "        globals()[var] = joblib.load(latest)\n",
    "        meta_file = latest.with_suffix('.json')\n",
    "        if meta_file.exists():\n",
    "            with open(meta_file) as f:\n",
    "                globals()[f\"{var}_meta\"] = _json.load(f)\n",
    "        loaded_flags[var] = f\"loaded {latest.name}\"\n",
    "    except Exception as e:\n",
    "        loaded_flags[var] = f\"failed: {e}\";\n",
    "        globals()[var] = None\n",
    "\n",
    "loaded_flags['tabnet_model'] = 'deferred (class not yet defined)'\n",
    "\n",
    "print(\"Auto-load status:\")\n",
    "for k,v in loaded_flags.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"FORCE_RETRAIN=\", FORCE_RETRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6177eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_31388\\312996059.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden data loaded into DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23038 entries, 0 to 23037\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   school_name             23038 non-null  object \n",
      " 1   school_type             23038 non-null  object \n",
      " 2   teachers_fte            22550 non-null  float64\n",
      " 3   enrollment              22863 non-null  float64\n",
      " 4   grade_eight_enrollment  21613 non-null  float64\n",
      " 5   math_counts             22507 non-null  float64\n",
      " 6   math_high_pct           22507 non-null  float64\n",
      " 7   math_low_pct            19960 non-null  float64\n",
      " 8   read_counts             22386 non-null  float64\n",
      " 9   read_high_pct           22386 non-null  float64\n",
      " 10  read_low_pct            19907 non-null  float64\n",
      " 11  pct_hhi_150k_200k       23038 non-null  float64\n",
      " 12  pct_hhi_220k_plus       23038 non-null  float64\n",
      " 13  avg_natwalkind          23038 non-null  float64\n",
      " 14  total_10_14             23038 non-null  int64  \n",
      " 15  pct_10_14               23038 non-null  int64  \n",
      " 16  pct_female_10_14        22937 non-null  float64\n",
      " 17  total_pop               23038 non-null  int64  \n",
      " 18  hhi_150k_200k           23038 non-null  int64  \n",
      " 19  hhi_220k_plus           23038 non-null  int64  \n",
      " 20  schools_in_zip          23038 non-null  int64  \n",
      " 21  dup_rank                23038 non-null  int64  \n",
      "dtypes: float64(13), int64(7), object(2)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load data\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    print(\"Database connection successful\")\n",
    "    sql_query = \"SELECT * FROM dev.base_data;\"\n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    conn.close()\n",
    "    print(\"Golden data loaded into DataFrame:\")\n",
    "    print(df.info())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6bcca",
   "metadata": {},
   "source": [
    "## 3. Data Splitting: Train, Validation, Test\n",
    "Split the dataset into train, validation, and test sets, ensuring proper handling of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1bb52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_name', 'school_type', 'teachers_fte', 'enrollment',\n",
       "       'grade_eight_enrollment', 'math_counts', 'math_high_pct',\n",
       "       'math_low_pct', 'read_counts', 'read_high_pct', 'read_low_pct',\n",
       "       'pct_hhi_150k_200k', 'pct_hhi_220k_plus', 'avg_natwalkind',\n",
       "       'total_10_14', 'pct_10_14', 'pct_female_10_14', 'total_pop',\n",
       "       'hhi_150k_200k', 'hhi_220k_plus', 'schools_in_zip', 'dup_rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13df7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (11367, 18), Validation shape: (3789, 18), Test shape: (3789, 18)\n"
     ]
    }
   ],
   "source": [
    "# Define target and drop missing\n",
    "TARGET = 'math_high_pct' if 'math_high_pct' in df.columns else 'target'\n",
    "data = df.dropna().reset_index(drop=True)\n",
    "data = data.set_index('school_name')\n",
    "\n",
    "# Split features and target\n",
    "feature_cols = [c for c in data.columns if c != TARGET and c != 'dup_rank' and c != 'math_low_pct']\n",
    "X = data[feature_cols]\n",
    "y = data[TARGET]\n",
    "\n",
    "# Train/validation/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "print(f'Train shape: {X_train.shape}, Validation shape: {X_valid.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52296",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Preprocessing Pipeline\n",
    "Identify numeric and categorical features, set up StandardScaler and OneHotEncoder, and build a ColumnTransformer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c071ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['teachers_fte', 'enrollment', 'grade_eight_enrollment', 'math_counts', 'read_counts', 'read_high_pct', 'read_low_pct', 'pct_hhi_150k_200k', 'pct_hhi_220k_plus', 'avg_natwalkind', 'total_10_14', 'pct_10_14', 'pct_female_10_14', 'total_pop', 'hhi_150k_200k', 'hhi_220k_plus', 'schools_in_zip']\n",
      "Categorical features: ['school_type']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_valid_enc = preprocessor.transform(X_valid)\n",
    "X_test_enc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6187ca5",
   "metadata": {},
   "source": [
    "## Model Optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bc1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Optuna-based optimizer for cross-validated hyperparameter tuning\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "def optimize_model_with_optuna(\n",
    "                                model_name: str,\n",
    "                                estimator_builder: Callable[[Dict], object],\n",
    "                                param_space_fn: Callable[[optuna.trial.Trial], Dict],\n",
    "                                X,\n",
    "                                y,\n",
    "                                scoring: str = 'neg_root_mean_squared_error',\n",
    "                                cv: int = 3,\n",
    "                                n_trials: int = 5,\n",
    "                                direction: str = 'minimize',\n",
    "                                random_state: int = 42,\n",
    "                                n_jobs: int = -1,\n",
    "                            ) -> Tuple[optuna.study.Study, Dict]:\n",
    "    \"\"\"Optimize a model's hyperparameters using Optuna and cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name used to label the Optuna study\n",
    "        estimator_builder: Callable that receives a params dict and returns an unfitted estimator\n",
    "        param_space_fn: Callable that maps an Optuna trial to a hyperparameter dictionary\n",
    "        X, y: Training features and targets used for cross-validation\n",
    "        scoring: scikit-learn scoring string guiding optimization\n",
    "        cv: Number of cross-validation folds\n",
    "        n_trials: Number of Optuna trials to run\n",
    "        direction: 'minimize' or 'maximize' depending on the objective\n",
    "        random_state: Seed for the Optuna sampler\n",
    "        n_jobs: Parallelism for cross_val_score\n",
    "\n",
    "    Returns:\n",
    "        The completed Optuna study and the best hyperparameters discovered.\n",
    "    \"\"\"\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(study_name=f\"{model_name}_opt\", direction=direction, sampler=sampler)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        params = param_space_fn(trial)\n",
    "        estimator = estimator_builder(params)\n",
    "        scores = cross_val_score(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "        mean_score = np.mean(scores)\n",
    "        normalized_score = -mean_score if scoring.startswith('neg') else mean_score\n",
    "        return normalized_score if direction == 'minimize' else -normalized_score\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    return study, study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6bcf7",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533b035",
   "metadata": {},
   "source": [
    "## LINEAR MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:21,066] A new study created in memory with name: SVR_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVR] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bac00f4aee4780907b59354fb06a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:28,609] Trial 0 finished with value: 21.65850568247251 and parameters: {'kernel': 'poly', 'C': 3.0049873591901566, 'epsilon': 0.08900466011060913, 'degree': 2, 'gamma': 0.1143098387631322}. Best is trial 0 with value: 21.65850568247251.\n",
      "[I 2025-10-11 16:02:36,048] Trial 1 finished with value: 20.70295950569489 and parameters: {'kernel': 'rbf', 'C': 0.5318033256270142, 'epsilon': 0.2924774630404986, 'gamma': 0.3818145165896869}. Best is trial 1 with value: 20.70295950569489.\n",
      "[I 2025-10-11 16:02:36,048] Trial 1 finished with value: 20.70295950569489 and parameters: {'kernel': 'rbf', 'C': 0.5318033256270142, 'epsilon': 0.2924774630404986, 'gamma': 0.3818145165896869}. Best is trial 1 with value: 20.70295950569489.\n",
      "[I 2025-10-11 16:02:41,615] Trial 2 finished with value: 19.044401625690607 and parameters: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}. Best is trial 2 with value: 19.044401625690607.\n",
      "Best SVR params: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}\n",
      "[I 2025-10-11 16:02:41,615] Trial 2 finished with value: 19.044401625690607 and parameters: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}. Best is trial 2 with value: 19.044401625690607.\n",
      "Best SVR params: {'kernel': 'rbf', 'C': 1.2439367209907215, 'epsilon': 0.18118910790805948, 'gamma': 0.2004087187654156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 16:02:53,313] A new study created in memory with name: PolynomialRegression_opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model -> svr_opt_20251011T200253Z.joblib; metadata -> svr_opt_20251011T200253Z.json\n",
      "Saved model -> linear_regression_opt_20251011T200253Z.joblib; metadata -> linear_regression_opt_20251011T200253Z.json\n",
      "[PolynomialRegression] Starting Optuna optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606a34d560a41b9b63b2ee40dc49940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear & Kernel-based Models: SVR + Polynomial Regression (with Optuna tuning + CV)\n",
    "def build_svr_estimator(params: Dict) -> SVR:\n",
    "    base = {'kernel': params.get('kernel', 'rbf')}\n",
    "    # Map params safely\n",
    "    for k in ['C','epsilon','gamma','degree']:\n",
    "        if k in params:\n",
    "            base[k] = params[k]\n",
    "    return SVR(**base)\n",
    "\n",
    "def svr_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf','poly','sigmoid'])\n",
    "    params = {\n",
    "        'kernel': kernel,\n",
    "        'C': trial.suggest_float('C', 0.5, 10, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.05, 0.3),\n",
    "    }\n",
    "    if kernel in ['rbf','sigmoid']:\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 0.5, log=True)\n",
    "    if kernel == 'poly':\n",
    "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.1, 1, log=True)\n",
    "    return params\n",
    "\n",
    "def build_poly_estimator(params: Dict):\n",
    "    degree = params.get('degree', 2)\n",
    "    include_bias = params.get('include_bias', False)\n",
    "    interaction_only = params.get('interaction_only', False)\n",
    "    return SkPipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias, interaction_only=interaction_only)),\n",
    "        ('lr', LinearRegression())\n",
    "    ])\n",
    "\n",
    "def poly_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'degree': trial.suggest_int('degree', 2, 5),\n",
    "        'include_bias': False,\n",
    "        'interaction_only': trial.suggest_categorical('interaction_only', [False, True])\n",
    "    }\n",
    "\n",
    "# Run / reuse SVR optimization\n",
    "if svr_model is None or FORCE_RETRAIN:\n",
    "    print('[SVR] Starting Optuna optimization...')\n",
    "    svr_study, svr_best_params = optimize_model_with_optuna(\n",
    "        model_name='SVR',\n",
    "        estimator_builder=build_svr_estimator,\n",
    "        param_space_fn=svr_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best SVR params:', svr_best_params)\n",
    "    svr_model = build_svr_estimator(svr_best_params)\n",
    "    svr_model.fit(X_train_enc, y_train)\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "    save_model(svr_model, 'svr_opt', {'best_params': svr_best_params})\n",
    "else:\n",
    "    print('[SVR] Using preloaded optimized model; generating predictions.')\n",
    "    svr_valid_pred = svr_model.predict(X_valid_enc)\n",
    "    svr_test_pred = svr_model.predict(X_test_enc)\n",
    "\n",
    "# Baseline Linear Regression (also optionally re-optimized via polynomial)\n",
    "if lr_model is None or FORCE_RETRAIN:\n",
    "    # Keep a simple baseline linear regression for reference\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_enc, y_train)\n",
    "    save_model(lr_model, 'linear_regression_opt', {'params': lr_model.get_params(), 'baseline': True})\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "elif lr_model is not None:\n",
    "    lr_valid_pred = lr_model.predict(X_valid_enc)\n",
    "    lr_test_pred = lr_model.predict(X_test_enc)\n",
    "\n",
    "# Polynomial Regression optimization\n",
    "if poly_model is None or FORCE_RETRAIN:\n",
    "    print('[PolynomialRegression] Starting Optuna optimization...')\n",
    "    poly_study, poly_best_params = optimize_model_with_optuna(\n",
    "        model_name='PolynomialRegression',\n",
    "        estimator_builder=build_poly_estimator,\n",
    "        param_space_fn=poly_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=3,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Polynomial Regression params:', poly_best_params)\n",
    "    poly_model = build_poly_estimator(poly_best_params)\n",
    "    poly_model.fit(X_train_enc, y_train)\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)\n",
    "    save_model(poly_model, 'poly_reg_opt', {'best_params': poly_best_params})\n",
    "else:\n",
    "    print('[PolynomialRegression] Using preloaded optimized model; generating predictions.')\n",
    "    poly_valid_pred = poly_model.predict(X_valid_enc)\n",
    "    poly_test_pred = poly_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf134e",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58546626",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch MLP Regressor with sklearn API\n",
    "\n",
    "def _ensure_dense_np(X):\n",
    "    return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "class TorchMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"PyTorch MLP for regression with sklearn API and CUDA support.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=(128, 64), activation='relu', \n",
    "                 learning_rate_init=1e-3, alpha=0.0, batch_size=128, max_iter=100,\n",
    "                 early_stopping=True, validation_fraction=0.1, n_iter_no_change=10,\n",
    "                 random_state=42, verbose=False, device=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "    def _build_network(self, in_features):\n",
    "        layers = []\n",
    "        act_fn = nn.ReLU if self.activation == 'relu' else nn.Tanh\n",
    "        prev = in_features\n",
    "        for h in self.hidden_layer_sizes:\n",
    "            layers.extend([nn.Linear(prev, h), act_fn()])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Setup\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        \n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        y_np = (y.values if hasattr(y, 'values') else y).astype('float32').reshape(-1, 1)\n",
    "        \n",
    "        # Train/val split\n",
    "        if self.early_stopping and 0 < self.validation_fraction < 0.5:\n",
    "            val_size = max(1, int(len(X_np) * self.validation_fraction))\n",
    "            idx = np.random.RandomState(self.random_state).permutation(len(X_np))\n",
    "            X_train, X_val = X_np[idx[val_size:]], X_np[idx[:val_size]]\n",
    "            y_train, y_val = y_np[idx[val_size:]], y_np[idx[:val_size]]\n",
    "        else:\n",
    "            X_train, y_train, X_val, y_val = X_np, y_np, None, None\n",
    "        \n",
    "        # Device and model\n",
    "        device = self.device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._device_ = device\n",
    "        self.model_ = self._build_network(X_np.shape[1]).to(device)\n",
    "        self.optimizer_ = torch.optim.Adam(self.model_.parameters(), \n",
    "                                          lr=self.learning_rate_init, \n",
    "                                          weight_decay=self.alpha)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        best_val, best_state, patience = math.inf, None, 0\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            self.model_.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                self.optimizer_.zero_grad()\n",
    "                criterion(self.model_(xb), yb).backward()\n",
    "                self.optimizer_.step()\n",
    "            \n",
    "            # Validation\n",
    "            if X_val is not None:\n",
    "                self.model_.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = criterion(\n",
    "                        self.model_(torch.from_numpy(X_val).to(device)),\n",
    "                        torch.from_numpy(y_val).to(device)\n",
    "                    ).item()\n",
    "                \n",
    "                if val_loss < best_val - 1e-9:\n",
    "                    best_val, patience = val_loss, 0\n",
    "                    best_state = {k: v.cpu().clone() for k, v in self.model_.state_dict().items()}\n",
    "                else:\n",
    "                    patience += 1\n",
    "                \n",
    "                if patience >= self.n_iter_no_change:\n",
    "                    break\n",
    "        \n",
    "        if best_state:\n",
    "            self.model_.load_state_dict(best_state)\n",
    "        self.n_features_in_ = X_np.shape[1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, ['model_'])\n",
    "        X_np = _ensure_dense_np(X).astype('float32')\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(torch.from_numpy(X_np).to(self._device_))\n",
    "        return preds.cpu().numpy().ravel()\n",
    "\n",
    "# Helper functions\n",
    "def build_mlp_estimator(params):\n",
    "    params = params.copy()\n",
    "    params.pop('hl1', None)\n",
    "    params.pop('hl2', None)\n",
    "    return TorchMLPRegressor(**params)\n",
    "\n",
    "def mlp_param_space(trial):\n",
    "    return {\n",
    "        'hidden_layer_sizes': tuple(sorted([\n",
    "            trial.suggest_int('hl1', 64, 256, step=32),\n",
    "            trial.suggest_int('hl2', 32, 192, step=32)\n",
    "        ], reverse=True)),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        'max_iter': trial.suggest_int('max_iter', 150, 600, step=75),\n",
    "        'n_iter_no_change': trial.suggest_int('n_iter_no_change', 5, 25, step=5),\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.15,\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'random_state': 42,\n",
    "        'verbose': False,\n",
    "    }\n",
    "\n",
    "# Training\n",
    "if mlp_model is None or FORCE_RETRAIN:\n",
    "    print(f\"[MLP] Starting optimization on {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    mlp_study, mlp_best_params = optimize_model_with_optuna(\n",
    "        model_name='TorchMLPRegressor',\n",
    "        estimator_builder=build_mlp_estimator,\n",
    "        param_space_fn=mlp_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1,\n",
    "    )\n",
    "    print(f'[MLP] Best params: {mlp_best_params}')\n",
    "    mlp_model = build_mlp_estimator(mlp_best_params)\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt', {'best_params': mlp_best_params, 'framework': 'torch'})\n",
    "else:\n",
    "    print(\"[MLP] Refitting preloaded model\")\n",
    "    mlp_model.fit(X_train_enc, y_train)\n",
    "    mlp_valid_pred = mlp_model.predict(X_valid_enc)\n",
    "    mlp_test_pred = mlp_model.predict(X_test_enc)\n",
    "    save_model(mlp_model, 'mlp_opt_refit', {'refit': True, 'framework': 'torch'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7801",
   "metadata": {},
   "source": [
    "## TABNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3270f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified TabNet Implementation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "def prepare_data_for_tabnet(X, y=None):\n",
    "    \"\"\"Convert data to TabNet-compatible format.\"\"\"\n",
    "    X_dense = X.toarray().astype(np.float32) if hasattr(X, 'toarray') else np.asarray(X, dtype=np.float32)\n",
    "    if y is not None:\n",
    "        y_reshaped = np.asarray(y).reshape(-1, 1) if np.asarray(y).ndim == 1 else np.asarray(y)\n",
    "        return X_dense, y_reshaped\n",
    "    return X_dense\n",
    "\n",
    "class SimpleTabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Simplified TabNet wrapper with sensible defaults.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_d=16, n_a=16, n_steps=5, gamma=1.3, lambda_sparse=1e-4, \n",
    "                 lr=0.02, max_epochs=100, patience=20, batch_size=1024, seed=42):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_prep, y_prep = prepare_data_for_tabnet(X, y)\n",
    "        \n",
    "        # Create internal validation split (15% for early stopping)\n",
    "        n_samples = X_prep.shape[0]\n",
    "        val_size = max(1, int(n_samples * 0.15))\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        indices = rng.permutation(n_samples)\n",
    "        \n",
    "        train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "        X_train, y_train = X_prep[train_idx], y_prep[train_idx]\n",
    "        X_val, y_val = X_prep[val_idx], y_prep[val_idx]\n",
    "        \n",
    "        self.model_ = TabNetRegressor(\n",
    "            n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps,\n",
    "            gamma=self.gamma, lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params={'lr': self.lr}, seed=self.seed\n",
    "        )\n",
    "        \n",
    "        self.model_.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=self.max_epochs,\n",
    "            patience=self.patience,\n",
    "            batch_size=self.batch_size,\n",
    "            virtual_batch_size=128\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'model_')\n",
    "        X_prep = prepare_data_for_tabnet(X)\n",
    "        return self.model_.predict(X_prep).ravel()\n",
    "\n",
    "def get_tabnet_param_space(trial):\n",
    "    \"\"\"Simplified parameter space for TabNet optimization.\"\"\"\n",
    "    return {\n",
    "        'n_d': trial.suggest_categorical('n_d', [8, 16, 24]),\n",
    "        'n_a': trial.suggest_categorical('n_a', [8, 16, 24]),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 6),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 1.8),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),\n",
    "        'lr': trial.suggest_float('lr', 1e-3, 2e-2, log=True),\n",
    "        'max_epochs': 100,\n",
    "        'patience': 20\n",
    "    }\n",
    "\n",
    "def load_or_train_tabnet():\n",
    "    \"\"\"Load existing TabNet model or train new one.\"\"\"\n",
    "    tabnet_files = sorted(MODEL_DIR.glob('tabnet_opt_*.joblib'))\n",
    "    \n",
    "    if tabnet_files and not FORCE_RETRAIN:\n",
    "        try:\n",
    "            model = joblib.load(tabnet_files[-1])\n",
    "            print(f\"[TabNet] Loaded existing model: {tabnet_files[-1].name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"[TabNet] Failed to load model: {e}\")\n",
    "    \n",
    "    # Train new model\n",
    "    print(\"[TabNet] Training new model with Optuna optimization\")\n",
    "    study, best_params = optimize_model_with_optuna(\n",
    "        model_name='TabNetRegressor',\n",
    "        estimator_builder=lambda params: SimpleTabNetWrapper(**params),\n",
    "        param_space_fn=get_tabnet_param_space,\n",
    "        X=X_train_enc, y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3, \n",
    "        n_trials=5, \n",
    "        direction='minimize',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model = SimpleTabNetWrapper(**best_params)\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Save model and metadata\n",
    "    save_model(model, 'tabnet_opt', {'best_params': best_params})\n",
    "    print(f\"[TabNet] Best parameters: {best_params}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Execute TabNet training/loading\n",
    "tabnet_model = load_or_train_tabnet()\n",
    "\n",
    "# Generate predictions\n",
    "tabnet_valid_pred = tabnet_model.predict(X_valid_enc)\n",
    "tabnet_test_pred = tabnet_model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2adcc8",
   "metadata": {},
   "source": [
    "# 5. Tree-based ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7c82e",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization and training for ensemble models\n",
    "def build_xgb_estimator(params: Dict) -> XGBRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return XGBRegressor(**base_params)\n",
    "\n",
    "\n",
    "def xgb_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1e-1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True)\n",
    "    }\n",
    "\n",
    "if xgb_model is None or FORCE_RETRAIN:\n",
    "    print(\"[XGBoost] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    xgb_study, xgb_best_params = optimize_model_with_optuna(\n",
    "        model_name='XGBoost',\n",
    "        estimator_builder=build_xgb_estimator,\n",
    "        param_space_fn=xgb_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print('Best XGBoost params:', xgb_best_params)\n",
    "    # Fit model with optimized hyperparameters\n",
    "    xgb_model = build_xgb_estimator(xgb_best_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    save_model(xgb_model, 'xgboost_opt', {'best_params': xgb_best_params})\n",
    "else:\n",
    "    # Refit even when preloaded to ensure alignment with current data & preprocessing\n",
    "    print('[XGBoost] Preloaded model found; refitting on current data.')\n",
    "    # Try to pull previously stored best params from metadata if available\n",
    "    reuse_params = None\n",
    "    try:\n",
    "        if 'xgb_model_meta' in globals() and isinstance(xgb_model_meta, dict):\n",
    "            reuse_params = xgb_model_meta.get('best_params')\n",
    "    except Exception:\n",
    "        reuse_params = None\n",
    "    if reuse_params is None:\n",
    "        # Fall back to current model's parameters (filter to search space + core)\n",
    "        try:\n",
    "            current = xgb_model.get_params()\n",
    "            reuse_keys = {'n_estimators','max_depth','learning_rate','subsample','colsample_bytree','reg_alpha','reg_lambda'}\n",
    "            reuse_params = {k: v for k, v in current.items() if k in reuse_keys}\n",
    "        except Exception:\n",
    "            reuse_params = {}\n",
    "    # Rebuild a fresh estimator to avoid any internal state carry-over\n",
    "    xgb_model = build_xgb_estimator(reuse_params)\n",
    "    xgb_model.fit(X_train_enc, y_train)\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid_enc)\n",
    "    xgb_test_pred = xgb_model.predict(X_test_enc)\n",
    "    # Save refit artifact\n",
    "    save_model(xgb_model, 'xgboost_opt_refit', {'refit': True, 'best_params': reuse_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229be5e",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6950519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rf_estimator(params: Dict) -> RandomForestRegressor:\n",
    "    base_params = {\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    return RandomForestRegressor(**base_params)\n",
    "\n",
    "\n",
    "def rf_param_space(trial: optuna.trial.Trial) -> Dict:\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_float('max_features', 0.4, 0.9)\n",
    "    }\n",
    "\n",
    "if rf_model is None or FORCE_RETRAIN:\n",
    "    print(\"[RandomForest] No preloaded model (or FORCE_RETRAIN=True). Starting Optuna optimization...\")\n",
    "    rf_study, rf_best_params = optimize_model_with_optuna(\n",
    "        model_name='RandomForest',\n",
    "        estimator_builder=build_rf_estimator,\n",
    "        param_space_fn=rf_param_space,\n",
    "        X=X_train_enc,\n",
    "        y=y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_trials=5,\n",
    "        direction='minimize',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    print('Best Random Forest params:', rf_best_params)\n",
    "    rf_model = build_rf_estimator(rf_best_params)\n",
    "    rf_model.fit(X_train_enc, y_train)\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n",
    "    save_model(rf_model, 'random_forest_opt', {'best_params': rf_best_params})\n",
    "else:\n",
    "    print(\"[RandomForest] Using preloaded optimized model. Skipping training.\")\n",
    "    rf_valid_pred = rf_model.predict(X_valid_enc)\n",
    "    rf_test_pred = rf_model.predict(X_test_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd472e",
   "metadata": {},
   "source": [
    "## 6. Compare Model Performance\n",
    "Evaluate predictions from each model using RMSE, MAE, and RÂ² metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a49c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for metrics\n",
    "\n",
    "def adjusted_r2_score(r2, n, p):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "def regression_metrics(y_true, y_pred, X=None):\n",
    "    try:\n",
    "        # Try using root_mean_squared_error if available (sklearn >= 1.4)\n",
    "        from sklearn.metrics import root_mean_squared_error\n",
    "        rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    except ImportError:\n",
    "        # Fall back to mean_squared_error with squared=False for older versions\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    metrics = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "    if X is not None:\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        adj_r2 = adjusted_r2_score(r2, n, p)\n",
    "        metrics['Adj_R2'] = adj_r2\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Performance Comparison & Best Model Selection (All Methods)\n",
    "import json as _json_summary\n",
    "from copy import deepcopy\n",
    "\n",
    "# Utility to compute metrics if predictions exist and metrics missing\n",
    "def _ensure_metrics(prefix: str, display_name: str, Xv, Xt):\n",
    "    g = globals()\n",
    "    valid_pred_var = f'{prefix}_valid_pred'\n",
    "    test_pred_var = f'{prefix}_test_pred'\n",
    "    valid_metrics_var = f'{prefix}_metrics'  # normalized name for validation metrics\n",
    "    test_metrics_var = f'{prefix}_test_metrics'\n",
    "    # Accept older naming conventions (e.g., rf_metrics already used)\n",
    "    if valid_pred_var in g:\n",
    "        if valid_metrics_var not in g or g.get(valid_metrics_var) is None:\n",
    "            try:\n",
    "                g[valid_metrics_var] = regression_metrics(y_valid, g[valid_pred_var], Xv)\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Could not compute validation metrics for {display_name}: {e}')\n",
    "                g[valid_metrics_var] = None\n",
    "    if test_pred_var in g:\n",
    "        if test_metrics_var not in g or g.get(test_metrics_var) is None:\n",
    "            try:\n",
    "                g[test_metrics_var] = regression_metrics(y_test, g[test_pred_var], Xt)\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Could not compute test metrics for {display_name}: {e}')\n",
    "                g[test_metrics_var] = None\n",
    "    return (g.get(valid_metrics_var), g.get(test_metrics_var))\n",
    "\n",
    "# Recompute core ensemble metrics if missing (guards for partial executions)\n",
    "# This path is now superseded by _ensure_metrics but kept for clarity\n",
    "if 'xgb_metrics' not in globals() and 'xgb_valid_pred' in globals():\n",
    "    xgb_metrics = regression_metrics(y_valid, xgb_valid_pred, X_valid_enc)\n",
    "if 'rf_metrics' not in globals() and 'rf_valid_pred' in globals():\n",
    "    rf_metrics = regression_metrics(y_valid, rf_valid_pred, X_valid_enc)\n",
    "if 'xgb_test_metrics' not in globals() and 'xgb_test_pred' in globals():\n",
    "    xgb_test_metrics = regression_metrics(y_test, xgb_test_pred, X_test_enc)\n",
    "if 'rf_test_metrics' not in globals() and 'rf_test_pred' in globals():\n",
    "    rf_test_metrics = regression_metrics(y_test, rf_test_pred, X_test_enc)\n",
    "\n",
    "# Model families to probe (prefix aligns with variable naming pattern)\n",
    "MODEL_PREFIXES = [\n",
    "    ('XGBoost', 'xgb'),\n",
    "    ('RandomForest', 'rf'),\n",
    "    ('SVR', 'svr'),\n",
    "    ('PolynomialRegression', 'poly'),\n",
    "    ('LinearRegression', 'lr'),\n",
    "    ('TorchMLP', 'mlp'),\n",
    "    ('TabNet', 'tabnet')\n",
    "]\n",
    "\n",
    "candidates = []\n",
    "for display_name, prefix in MODEL_PREFIXES:\n",
    "    valid_pred_var = f'{prefix}_valid_pred'\n",
    "    test_pred_var = f'{prefix}_test_pred'\n",
    "    if valid_pred_var in globals():\n",
    "        v_metrics, t_metrics = _ensure_metrics(prefix, display_name, X_valid_enc, X_test_enc)\n",
    "        if v_metrics is not None:\n",
    "            candidates.append({'Model': display_name, 'Valid': deepcopy(v_metrics), 'Test': deepcopy(t_metrics) if t_metrics else None})\n",
    "\n",
    "# (Legacy metrics variables that don't match prefix pattern already handled above)\n",
    "if not candidates:\n",
    "    print('No model metrics available yet. Run earlier training cells first.')\n",
    "else:\n",
    "    # Build a long-form DataFrame\n",
    "    rows = []\n",
    "    for c in candidates:\n",
    "        for ds in ['Valid','Test']:\n",
    "            if c[ds] is None:\n",
    "                continue\n",
    "            row = {'Model': c['Model'], 'Dataset': ds}\n",
    "            row.update(c[ds])\n",
    "            rows.append(row)\n",
    "    all_results_df = pd.DataFrame(rows).drop_duplicates(subset=['Model','Dataset']).set_index(['Model','Dataset']).sort_index()\n",
    "\n",
    "    display(all_results_df)\n",
    "\n",
    "    # Ranking based on Validation metrics only (composite)\n",
    "    val_df = all_results_df.xs('Valid', level='Dataset').copy()\n",
    "    metric_cols = [m for m in ['RMSE','MAE','R2','Adj_R2'] if m in val_df.columns]\n",
    "    if not metric_cols:\n",
    "        print('No numeric metrics found for ranking.')\n",
    "    else:\n",
    "        ranks = {}\n",
    "        for m in metric_cols:\n",
    "            ascending = m in ['RMSE','MAE']  # lower better\n",
    "            ranks[m + '_rank'] = val_df[m].rank(ascending=ascending, method='min')\n",
    "        rank_df = pd.DataFrame(ranks)\n",
    "        val_ranked = val_df.join(rank_df)\n",
    "        rank_cols = [c for c in val_ranked.columns if c.endswith('_rank')]\n",
    "        val_ranked['avg_rank'] = val_ranked[rank_cols].mean(axis=1)\n",
    "        best_model_name = val_ranked['avg_rank'].idxmin()\n",
    "        best_model_avg_rank = val_ranked.loc[best_model_name, 'avg_rank']\n",
    "        best_model_metrics_valid = val_df.loc[best_model_name]\n",
    "        try:\n",
    "            best_test_metrics = all_results_df.loc[(best_model_name, 'Test')].to_dict()\n",
    "        except KeyError:\n",
    "            best_test_metrics = None\n",
    "        print('\\n=== Validation Ranking (lower avg_rank better) ===')\n",
    "        display(round(val_ranked.sort_values('avg_rank'), 2))\n",
    "\n",
    "        print(f\"\\nOverall Best Model (Validation composite rank): {best_model_name} | avg_rank={best_model_avg_rank:.2f}\")\n",
    "        print('\\nBest Model Validation Metrics:')\n",
    "        for k,v in best_model_metrics_valid.items():\n",
    "            if not k.endswith('_rank') and k != 'avg_rank':\n",
    "                print(f'  {k}: {v:.4f}' if isinstance(v,(int,float)) else f'  {k}: {v}')\n",
    "        if best_test_metrics:\n",
    "            print('\\nBest Model Test Metrics:')\n",
    "            for k,v in best_test_metrics.items():\n",
    "                print(f'  {k}: {v:.4f}' if isinstance(v,(int,float)) else f'  {k}: {v}')\n",
    "\n",
    "        # Metric-wise winners\n",
    "        metric_winners = {}\n",
    "        for m in metric_cols:\n",
    "            ascending = m in ['RMSE','MAE']\n",
    "            winner = val_df[m].idxmin() if ascending else val_df[m].idxmax()\n",
    "            metric_winners[m] = {\n",
    "                'model': winner,\n",
    "                'value': val_df.loc[winner, m]\n",
    "            }\n",
    "        print('\\n=== Metric-wise Best (Validation) ===')\n",
    "        for m, info in metric_winners.items():\n",
    "            print(f\"  {m}: {info['model']} -> {info['value']:.4f}\")\n",
    "\n",
    "        # Persist summary JSON\n",
    "        summary_path = MODEL_DIR / 'model_comparison_summary.json'\n",
    "        summary_payload = {\n",
    "            'models_evaluated': [c['Model'] for c in candidates],\n",
    "            'validation_table': val_df.to_dict(orient='index'),\n",
    "            'ranking': val_ranked[['avg_rank']].to_dict(orient='index'),\n",
    "            'metric_winners': {m: {'model': v['model'], 'value': float(v['value'])} for m,v in metric_winners.items()},\n",
    "            'overall_best_model': {\n",
    "                'name': best_model_name,\n",
    "                'validation_metrics': {k: float(v) for k,v in best_model_metrics_valid.items() if isinstance(v,(int,float))},\n",
    "                'test_metrics': {k: float(v) if isinstance(v,(int,float)) else v for k,v in (best_test_metrics or {}).items()},\n",
    "                'avg_rank': float(best_model_avg_rank)\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            with open(summary_path, 'w') as f:\n",
    "                _json_summary.dump(summary_payload, f, indent=2)\n",
    "            print(f\"\\nSaved model comparison summary -> {summary_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save summary JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d41100",
   "metadata": {},
   "source": [
    "## 6.1 Model Family Comparison\n",
    "\n",
    "Compare the best model from each family (Linear, Neural Network, Tree-based) to understand:\n",
    "- Which family performs best for this dataset\n",
    "- Trade-offs between model complexity and performance  \n",
    "- Family-specific strengths and weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Family Comparison - Simplified\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Define model families\n",
    "MODEL_FAMILIES = {\n",
    "    'Linear': ['SVR', 'PolynomialRegression', 'LinearRegression'],\n",
    "    'Neural Network': ['TorchMLP', 'TabNet'],\n",
    "    'Tree-based': ['XGBoost', 'RandomForest', 'LightGBM']\n",
    "}\n",
    "\n",
    "if 'all_results_df' not in globals() or all_results_df.empty:\n",
    "    print(\"â ï¸  Run model comparison cell first\")\n",
    "else:\n",
    "    val_df = all_results_df.xs('Valid', level='Dataset').copy()\n",
    "    family_results = []\n",
    "    \n",
    "    # Find best model per family\n",
    "    for family, models in MODEL_FAMILIES.items():\n",
    "        available = [m for m in models if m in val_df.index]\n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        best_model = val_df.loc[available, 'RMSE'].idxmin()\n",
    "        val_metrics = val_df.loc[best_model]\n",
    "        \n",
    "        try:\n",
    "            test_metrics = all_results_df.loc[(best_model, 'Test')]\n",
    "            gap = abs(test_metrics['RMSE'] - val_metrics['RMSE']) / val_metrics['RMSE'] * 100\n",
    "        except KeyError:\n",
    "            test_metrics = None\n",
    "            gap = np.nan\n",
    "        \n",
    "        family_results.append({\n",
    "            'Family': family,\n",
    "            'Model': best_model,\n",
    "            'Val_RMSE': val_metrics['RMSE'],\n",
    "            'Val_RÂ²': val_metrics['R2'],\n",
    "            'Test_RMSE': test_metrics['RMSE'] if test_metrics is not None else np.nan,\n",
    "            'Test_RÂ²': test_metrics['R2'] if test_metrics is not None else np.nan,\n",
    "            'Gap_%': gap\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    if family_results:\n",
    "        df_compare = pd.DataFrame(family_results).set_index('Family')\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"FAMILY CHAMPIONS COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        display(df_compare.round(4))\n",
    "        \n",
    "        best_family = df_compare['Val_RMSE'].idxmin()\n",
    "        print(f\"\\nð Best Family: {best_family} ({df_compare.loc[best_family, 'Model']})\")\n",
    "        print(f\"   RMSE: {df_compare.loc[best_family, 'Val_RMSE']:.4f} | RÂ²: {df_compare.loc[best_family, 'Val_RÂ²']:.4f}\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # RMSE comparison\n",
    "        x = np.arange(len(df_compare))\n",
    "        width = 0.35\n",
    "        ax1 = axes[0]\n",
    "        ax1.bar(x - width/2, df_compare['Val_RMSE'], width, label='Validation', color='steelblue', alpha=0.8)\n",
    "        ax1.bar(x + width/2, df_compare['Test_RMSE'], width, label='Test', color='coral', alpha=0.8)\n",
    "        ax1.set_ylabel('RMSE')\n",
    "        ax1.set_title('RMSE by Family')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(df_compare.index, rotation=15, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # RÂ² comparison\n",
    "        ax2 = axes[1]\n",
    "        ax2.bar(x - width/2, df_compare['Val_RÂ²'], width, label='Validation', color='forestgreen', alpha=0.8)\n",
    "        ax2.bar(x + width/2, df_compare['Test_RÂ²'], width, label='Test', color='orange', alpha=0.8)\n",
    "        ax2.set_ylabel('RÂ² Score')\n",
    "        ax2.set_title('RÂ² by Family')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(df_compare.index, rotation=15, ha='right')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        ax2.set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'best_family': best_family,\n",
    "            'best_model': df_compare.loc[best_family, 'Model'],\n",
    "            'family_comparison': df_compare.to_dict(orient='index')\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            def convert(obj):\n",
    "                if isinstance(obj, (np.integer, np.floating)):\n",
    "                    return float(obj)\n",
    "                if isinstance(obj, dict):\n",
    "                    return {k: convert(v) for k, v in obj.items()}\n",
    "                return obj\n",
    "            \n",
    "            with open(MODEL_DIR / 'family_comparison_summary.json', 'w') as f:\n",
    "                json.dump(convert(summary), f, indent=2)\n",
    "            print(f\"\\nâ Saved to family_comparison_summary.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"â ï¸  Save failed: {e}\")\n",
    "    else:\n",
    "        print(\"â No models found for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb232085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Best Model - Compact Summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate key metrics\n",
    "train_pred = xgb_model.predict(X_train_enc)\n",
    "metrics = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Validation', 'Test'],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "        np.sqrt(mean_squared_error(y_valid, xgb_valid_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, xgb_test_pred))\n",
    "    ],\n",
    "    'RÂ²': [\n",
    "        r2_score(y_train, train_pred),\n",
    "        r2_score(y_valid, xgb_valid_pred),\n",
    "        r2_score(y_test, xgb_test_pred)\n",
    "    ]\n",
    "}).round(4).set_index('Dataset')\n",
    "\n",
    "# Top features\n",
    "top_features = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).nlargest(5, 'Importance').round(4)\n",
    "\n",
    "print(\"ð XGBoost Model Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ð Performance: RÂ² = {metrics.loc['Validation', 'RÂ²']:.3f}, RMSE = {metrics.loc['Validation', 'RMSE']:.3f}\")\n",
    "print(f\"âï¸  Config: {xgb_model.n_estimators} trees, depth={xgb_model.max_depth}, lr={xgb_model.learning_rate}\")\n",
    "print(f\"ð¯ Top Feature: {top_features.iloc[0]['Feature']} ({top_features.iloc[0]['Importance']:.3f})\")\n",
    "\n",
    "display(metrics)\n",
    "display(top_features.reset_index(drop=True))\n",
    "\n",
    "# Compact visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Performance comparison\n",
    "metrics.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('Model Performance')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "axes[1].barh(top_features['Feature'], top_features['Importance'])\n",
    "axes[1].set_title('Top 5 Features')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036aa02",
   "metadata": {},
   "source": [
    "## 6.2 Best Model Deep Dive: Feature Importance, Ablation, Sensitivity & Tradeâoffs\n",
    "\n",
    "**This section automatically targets the currently best performing model** (based on composite validation rank from the model comparison cell) and provides comprehensive analysis:\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Best Model Selection** - Loads the best model from summary JSON and extracts feature names\n",
    "2. **Feature Importance** - Computes importance using native methods or permutation importance\n",
    "3. **Sensitivity Analysis** - Perturbs top features to measure RMSE impact\n",
    "4. **Ablation Study** - Progressive removal of top features to measure performance degradation\n",
    "5. **Tradeâoff Metrics** - Complexity vs. performance analysis\n",
    "\n",
    "### Execution Order (Run cells in this sequence):\n",
    "1. Run the **Best Model Selection** cell first (creates `best_model_obj` & `feature_names`)\n",
    "2. Run the **Feature Importance** cell (creates `fi_df`)\n",
    "3. Run the **Sensitivity Analysis** cell (uses both variables above)\n",
    "4. Run the **Ablation Analysis** cell (uses both variables above)\n",
    "\n",
    "â ï¸ **Important**: Make sure you've run the model comparison cell (section 6) before running these analysis cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3d134",
   "metadata": {},
   "source": [
    "### Model Sensitivity & Trade-off Analysis\n",
    "This section perturbs the top-ranked features (by model-specific importance) over a small value grid (quantiles or binary levels) to approximate a *local partial dependence* style effect on validation RMSE.\n",
    "\n",
    "What it does:\n",
    "- Selects the top N features by importance.\n",
    "- For each feature, replaces that column in the validation set with grid values while holding others fixed.\n",
    "- Recomputes RMSE to estimate how sensitive performance is to that feature.\n",
    "- Summarizes a trade-off metric: RMSE range / importance (higher means the feature causes relatively more performance swing per unit of importance weight).\n",
    "\n",
    "Caveats:\n",
    "- This is a one-at-a-time perturbation (no interaction effects captured).\n",
    "- For highly correlated features, effects may be inflated or unstable.\n",
    "- For models with strong non-linear interactions, full PDPs or SHAP may provide deeper insight.\n",
    "\n",
    "Next improvements (optional):\n",
    "- Replace simple perturbation with true PDP via averaging predictions over the original distribution.\n",
    "- Add SHAP interaction values for top features.\n",
    "- Log plots and tables to MLflow or other tracking system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model object + extract feature names\n",
    "from sklearn.inspection import permutation_importance\n",
    "best_model_name_runtime = None\n",
    "best_model_obj = None\n",
    "\n",
    "# If previous cell stored summary JSON, load it to get best model name\n",
    "summary_path = MODEL_DIR / 'model_comparison_summary.json'\n",
    "if summary_path.exists():\n",
    "    try:\n",
    "        with open(summary_path) as f:\n",
    "            _summary_json = json.load(f)\n",
    "        best_model_name_runtime = _summary_json.get('overall_best_model', {}).get('name')\n",
    "        print(f\"[BestModel] Loaded best model from summary JSON: {best_model_name_runtime}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[BestModel] Could not read summary JSON: {e}\")\n",
    "else:\n",
    "    print('[BestModel] Summary JSON not found. Run comparison cell first.')\n",
    "\n",
    "# Map display name to the underlying variable (heuristic)\n",
    "_MODEL_VAR_MAP = {\n",
    "    'XGBoost': 'xgb_model',\n",
    "    'RandomForest': 'rf_model',\n",
    "    'TorchMLP': 'mlp_model',\n",
    "    'SVR': 'svr_model',\n",
    "    'PolynomialRegression': 'poly_model',\n",
    "    'LinearRegression': 'lr_model',\n",
    "    'TabNet': 'tabnet_model',\n",
    "}\n",
    "\n",
    "if best_model_name_runtime and best_model_name_runtime in _MODEL_VAR_MAP:\n",
    "    var_name = _MODEL_VAR_MAP[best_model_name_runtime]\n",
    "    best_model_obj = globals().get(var_name)\n",
    "    if best_model_obj is None:\n",
    "        print(f\"[BestModel] Variable '{var_name}' not found or is None; ensure model cell was executed.\")\n",
    "else:\n",
    "    print('[BestModel] Could not determine model variable mapping.')\n",
    "\n",
    "# Extract feature names after preprocessing\n",
    "def get_feature_names(preprocessor):\n",
    "    feat_names = []\n",
    "    try:\n",
    "        # Numeric\n",
    "        for name, trans, cols in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue\n",
    "            if name == 'num':\n",
    "                feat_names.extend(cols)\n",
    "            elif name == 'cat':\n",
    "                # OneHotEncoder inside pipeline\n",
    "                try:\n",
    "                    ohe = trans.named_steps['onehot'] if hasattr(trans, 'named_steps') else trans\n",
    "                    cats = ohe.get_feature_names_out(cols)\n",
    "                    feat_names.extend(cats)\n",
    "                except Exception as e:\n",
    "                    print('[FeatureNames] OHE extraction failed:', e)\n",
    "    except Exception as e:\n",
    "        print('[FeatureNames] Fallback feature name generation:', e)\n",
    "        # Fallback to generic idx names\n",
    "        feat_names = [f'f{i}' for i in range(X_train_enc.shape[1])]\n",
    "    if len(feat_names) != X_train_enc.shape[1]:\n",
    "        # Align length via fallback if mismatch\n",
    "        print('[FeatureNames] Length mismatch; falling back to positional feature names.')\n",
    "        feat_names = [f'f{i}' for i in range(X_train_enc.shape[1])]\n",
    "    return feat_names\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "print(f\"Extracted {len(feature_names)} feature names.\")\n",
    "if best_model_name_runtime:\n",
    "    print(f\"Proceeding with best model: {best_model_name_runtime}\")\n",
    "else:\n",
    "    print('Best model not established yet. Subsequent cells may fail.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Analysis: neutralize/remove top-K features and measure RMSE delta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if best_model_obj is None or 'fi_df' not in globals():\n",
    "    print('Prerequisites missing: ensure best model & feature importance computed.')\n",
    "else:\n",
    "    base_preds = best_model_obj.predict(X_valid_enc)\n",
    "    base_rmse = np.sqrt(mean_squared_error(y_valid, base_preds))\n",
    "    print(f'Base validation RMSE: {base_rmse:.4f}')\n",
    "\n",
    "    top_features = fi_df.head(30)['feature'].tolist()  # consider top 30 for ablation scope\n",
    "    ablation_results = []\n",
    "    # Create a mapping feature_name -> column index for efficient masking\n",
    "    feat_index_map = {f: i for i, f in enumerate(feature_names)}\n",
    "\n",
    "    X_valid_dense = X_valid_enc.toarray() if hasattr(X_valid_enc, 'toarray') else np.asarray(X_valid_enc)\n",
    "    for k in [1,2,3,5,10,15,20,25,30]:\n",
    "        subset = top_features[:k]\n",
    "        cols = [feat_index_map[f] for f in subset if f in feat_index_map]\n",
    "        if not cols:\n",
    "            continue\n",
    "        X_mod = X_valid_dense.copy()\n",
    "        # Neutralization strategy: set to column mean (could also zero)\n",
    "        for c in cols:\n",
    "            col_mean = X_mod[:, c].mean()\n",
    "            X_mod[:, c] = col_mean\n",
    "        preds_mod = best_model_obj.predict(X_mod)\n",
    "        rmse_mod = mean_squared_error(y_valid, preds_mod, squared=False)\n",
    "        ablation_results.append({'k_removed': k, 'rmse': rmse_mod, 'delta_rmse': rmse_mod - base_rmse})\n",
    "    ablation_df = pd.DataFrame(ablation_results)\n",
    "    display(ablation_df)\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(ablation_df['k_removed'], ablation_df['delta_rmse'], marker='o')\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.xlabel('Number of Top Features Neutralized')\n",
    "    plt.ylabel('RMSE Delta vs Base')\n",
    "    plt.title(f'Ablation Impact - {best_model_name_runtime}')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423014d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (native if available, else permutation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if best_model_obj is None:\n",
    "    print('Best model object not available; run previous cells.')\n",
    "else:\n",
    "    # Try native importances\n",
    "    importances = None\n",
    "    importance_type = None\n",
    "    try:\n",
    "        if hasattr(best_model_obj, 'feature_importances_'):\n",
    "            importances = getattr(best_model_obj, 'feature_importances_')\n",
    "            importance_type = 'feature_importances_'\n",
    "        elif hasattr(best_model_obj, 'coef_'):\n",
    "            coef = getattr(best_model_obj, 'coef_')\n",
    "            # Flatten if necessary\n",
    "            importances = np.abs(coef.ravel())\n",
    "            importance_type = 'coef_'\n",
    "    except Exception as e:\n",
    "        print('[Importance] Native extraction failed:', e)\n",
    "\n",
    "    if importances is None or len(importances) != len(feature_names):\n",
    "        print('[Importance] Falling back to permutation importance (n_repeats=10).')\n",
    "        try:\n",
    "            perm = permutation_importance(best_model_obj, X_valid_enc, y_valid, n_repeats=10, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "            importances = np.maximum(0, perm.importances_mean)\n",
    "            importance_type = 'permutation_neg_rmse'\n",
    "        except Exception as e:\n",
    "            print('[Importance] Permutation importance failed:', e)\n",
    "            importances = None\n",
    "\n",
    "    if importances is not None:\n",
    "        fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        fi_df = fi_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        display(fi_df.head(25))\n",
    "        # Plot\n",
    "        top_plot = fi_df.head(25).iloc[::-1]\n",
    "        plt.figure(figsize=(8, max(4, 0.3*len(top_plot))))\n",
    "        plt.barh(top_plot['feature'], top_plot['importance'], color='steelblue')\n",
    "        plt.title(f'Top Feature Importances ({importance_type}) - {best_model_name_runtime}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Could not compute feature importance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis (Partial Dependence style) & Trade-offs Summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Preconditions & graceful exit\n",
    "missing = []\n",
    "for name in ['best_model_obj', 'fi_df', 'feature_names', 'X_valid_enc', 'y_valid', 'MODEL_DIR']:\n",
    "    if name not in globals():\n",
    "        missing.append(name)\n",
    "\n",
    "if missing:\n",
    "    print('Prerequisites missing; run earlier cells first. Missing:', missing)\n",
    "else:\n",
    "    if best_model_obj is None:\n",
    "        print('best_model_obj is None - cannot run sensitivity analysis.')\n",
    "    else:\n",
    "        # Parameters\n",
    "        TOP_N_SENS = 5\n",
    "        TOP_N_SENS = min(TOP_N_SENS, len(fi_df))\n",
    "        top_sens_features = fi_df.head(TOP_N_SENS)['feature'].tolist()\n",
    "\n",
    "        # Ensure dense matrix for perturbations\n",
    "        X_valid_dense = X_valid_enc.toarray() if hasattr(X_valid_enc, 'toarray') else np.asarray(X_valid_enc)\n",
    "        base_preds = best_model_obj.predict(X_valid_dense)\n",
    "        base_rmse = mean_squared_error(y_valid, base_preds, squared=False)\n",
    "        sens_records = []\n",
    "\n",
    "        for feat in top_sens_features:\n",
    "            if feat not in feature_names:\n",
    "                continue\n",
    "            col_idx = feature_names.index(feat)\n",
    "            col_values = X_valid_dense[:, col_idx]\n",
    "            unique_vals = np.unique(col_values)\n",
    "            # Build grid: handle near-constant / binary / continuous\n",
    "            if len(unique_vals) <= 2:\n",
    "                grid = unique_vals\n",
    "            else:\n",
    "                # Use robust quantiles; drop duplicates if feature has low variance\n",
    "                grid = np.unique(np.quantile(col_values, [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
    "            for gv in grid:\n",
    "                X_mod = X_valid_dense.copy()\n",
    "                X_mod[:, col_idx] = gv\n",
    "                try:\n",
    "                    preds_mod = best_model_obj.predict(X_mod)\n",
    "                    rmse_mod = mean_squared_error(y_valid, preds_mod, squared=False)\n",
    "                except Exception as e:\n",
    "                    rmse_mod = np.nan\n",
    "                sens_records.append({\n",
    "                    'feature': feat,\n",
    "                    'value': float(gv),\n",
    "                    'rmse': rmse_mod,\n",
    "                    'delta_rmse': (rmse_mod - base_rmse) if not np.isnan(rmse_mod) else np.nan\n",
    "                })\n",
    "\n",
    "        sens_df = pd.DataFrame(sens_records)\n",
    "        print('\\n=== Sensitivity Analysis Results ===')\n",
    "        display(sens_df.head(20))\n",
    "\n",
    "        # Plot curves\n",
    "        if not sens_df.empty:\n",
    "            n_feat = len(top_sens_features)\n",
    "            fig, axes = plt.subplots(n_feat, 1, figsize=(7, 3 * n_feat), sharex=False)\n",
    "            if n_feat == 1:\n",
    "                axes = [axes]\n",
    "            for ax, feat in zip(axes, top_sens_features):\n",
    "                sub = sens_df[sens_df['feature'] == feat].sort_values('value')\n",
    "                if sub.empty:\n",
    "                    ax.set_title(f'Sensitivity: {feat} (no data)')\n",
    "                    continue\n",
    "                ax.plot(sub['value'], sub['rmse'], marker='o')\n",
    "                ax.axhline(base_rmse, color='gray', linestyle='--', label='Base RMSE')\n",
    "                ax.set_title(f'Sensitivity: {feat}')\n",
    "                ax.set_xlabel('Value')\n",
    "                ax.set_ylabel('RMSE')\n",
    "                ax.legend()\n",
    "                ax.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('No sensitivity records to plot.')\n",
    "\n",
    "        # Trade-offs summary\n",
    "        trade_rows = []\n",
    "        for feat in top_sens_features:\n",
    "            sub = sens_df[sens_df['feature'] == feat]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            rmse_range = sub['rmse'].max() - sub['rmse'].min()\n",
    "            imp_row = fi_df.loc[fi_df['feature'] == feat, 'importance']\n",
    "            imp_val = float(imp_row.iloc[0]) if not imp_row.empty else np.nan\n",
    "            trade_rows.append({\n",
    "                'feature': feat,\n",
    "                'importance': imp_val,\n",
    "                'rmse_range': rmse_range,\n",
    "                'sensitivity_score': rmse_range / (imp_val + 1e-9) if not np.isnan(imp_val) else np.nan\n",
    "            })\n",
    "        trade_df = pd.DataFrame(trade_rows).sort_values('sensitivity_score', ascending=False)\n",
    "        print('\\n=== Trade-offs Analysis ===')\n",
    "        print('(Higher sensitivity_score => larger RMSE movement per unit importance)')\n",
    "        display(trade_df)\n",
    "\n",
    "        # Persist artifacts\n",
    "        analysis_payload = {\n",
    "            'feature_importances': fi_df.to_dict(orient='records') if 'fi_df' in globals() else None,\n",
    "            'ablation': ablation_df.to_dict(orient='records') if 'ablation_df' in globals() else None,\n",
    "            'sensitivity': sens_df.to_dict(orient='records'),\n",
    "            'tradeoffs': trade_df.to_dict(orient='records'),\n",
    "            'best_model': best_model_name_runtime if 'best_model_name_runtime' in globals() else None,\n",
    "            'base_validation_rmse': float(base_rmse)\n",
    "        }\n",
    "        analysis_path = MODEL_DIR / 'best_model_explainability.json'\n",
    "        try:\n",
    "            with open(analysis_path, 'w') as f:\n",
    "                json.dump(analysis_payload, f, indent=2)\n",
    "            print(f'\\nâ Saved explainability analysis -> {analysis_path.name}')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to save explainability JSON: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124766b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for ensemble models\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.base import clone\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, ax, title, cv=5, scoring='neg_root_mean_squared_error'):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 6),\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_rmse = -train_scores.mean(axis=1)\n",
    "    valid_rmse = -valid_scores.mean(axis=1)\n",
    "\n",
    "    ax.plot(train_sizes, train_rmse, label='Train RMSE', marker='o')\n",
    "    ax.fill_between(train_sizes, train_rmse - train_scores.std(axis=1), train_rmse + train_scores.std(axis=1), alpha=0.2)\n",
    "    ax.plot(train_sizes, valid_rmse, label='Validation RMSE', marker='s')\n",
    "    ax.fill_between(train_sizes, valid_rmse - valid_scores.std(axis=1), valid_rmse + valid_scores.std(axis=1), alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Training Examples')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "xgb_curve_model = clone(xgb_model)\n",
    "rf_curve_model = clone(rf_model)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "plot_learning_curve(xgb_curve_model, X_train_enc, y_train, axes[0], 'XGBoost Learning Curve')\n",
    "plot_learning_curve(rf_curve_model, X_train_enc, y_train, axes[1], 'Random Forest Learning Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the model comparison summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "summary_file = MODEL_DIR / 'model_comparison_summary.json'\n",
    "\n",
    "if summary_file.exists():\n",
    "    with open(summary_file, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(f\"ð Model Comparison Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nð File location: {summary_file}\")\n",
    "    print(f\"\\nð Overall Best Model: {summary['overall_best_model']['name']}\")\n",
    "    print(f\"   Average Rank: {summary['overall_best_model']['avg_rank']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nð Validation Metrics:\")\n",
    "    for metric, value in summary['overall_best_model']['validation_metrics'].items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nð¯ Test Metrics:\")\n",
    "    for metric, value in summary['overall_best_model']['test_metrics'].items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nð¥ Metric-wise Winners:\")\n",
    "    for metric, info in summary['metric_winners'].items():\n",
    "        print(f\"   {metric}: {info['model']} ({info['value']:.4f})\")\n",
    "    \n",
    "    print(f\"\\nð Models Evaluated: {', '.join(summary['models_evaluated'])}\")\n",
    "    \n",
    "    # Display full JSON in a pretty format\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"Full Summary (JSON):\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "else:\n",
    "    print(f\"â Summary file not found at: {summary_file}\")\n",
    "    print(\"Run the model comparison cell first to generate the summary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
